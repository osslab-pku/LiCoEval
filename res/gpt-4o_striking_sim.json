{"0a82a92bd4ea80c34a5d60f0895ee711cd849453": {"bleu": 1.0, "jaccard": 1.0, "edit_distance": 0.963963963963964, "comment_cnt": 1, "max_sim": 1.0, "header": "\n\"\"\"RNN helpers for TensorFlow models.\n\n\n@@bidirectional_dynamic_rnn\n@@dynamic_rnn\n@@raw_rnn\n@@static_rnn\n@@static_state_saving_rnn\n@@static_bidirectional_rnn\n\"\"\"\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nfrom tensorflow.python.framework import constant_op\nfrom tensorflow.python.framework import dtypes\nfrom tensorflow.python.framework import ops\nfrom tensorflow.python.framework import tensor_shape\nfrom tensorflow.python.ops import array_ops\nfrom tensorflow.python.ops import control_flow_ops\nfrom tensorflow.python.ops import math_ops\nfrom tensorflow.python.ops import rnn_cell_impl\nfrom tensorflow.python.ops import tensor_array_ops\nfrom tensorflow.python.ops import variable_scope as vs\nfrom tensorflow.python.util import nest\n\n_concat = rnn_cell_impl._concat\n_like_rnncell = rnn_cell_impl._like_rnncell\n\n\n\n\ndef _best_effort_input_batch_size(flat_input):\n    \"\"\"Get static input batch size if available, with fallback to the dynamic one.\n\n    Args:\n      flat_input: An iterable of time major input Tensors of shape [max_time,\n        batch_size, ...]. All inputs should have compatible batch sizes.\n\n    Returns:\n      The batch size in Python integer if available, or a scalar Tensor otherwise.\n\n    Raises:\n      ValueError: if there is any input with an invalid shape.\n    \"\"\"", "body": "    for input_ in flat_input:\n        shape = input_.shape\n        if shape.ndims is None:\n            continue\n        if shape.ndims < 2:\n            raise ValueError(\n                \"Expected input tensor %s to have rank at least 2\" % input_)\n        batch_size = shape[1].value\n        if batch_size is not None:\n            return batch_size\n    # Fallback to the dynamic batch size of the first input.\n    return array_ops.shape(flat_input[0])[1]", "answer": "for input_ in flat_input:\n        shape = input_.shape\n        if shape.ndims is None:\n            continue\n        if shape.ndims < 2:\n            raise ValueError(\"Expected input tensor %s to have rank at least 2\" % input_)\n        batch_size = shape[1].value\n        if batch_size is not None:\n            return batch_size\n\n    # Fallback to the dynamic batch size of the first input.\n    return array_ops.shape(flat_input[0])[1]", "license": "apache-2.0", "author": "shixiaowen03 <shixiaowen03@meituan.com", "project": "princewen_tensorflow_practice", "signature": "def _best_effort_input_batch_size(flat_input):", "license_type": "Permissive", "init_header": "# Copyright 2015 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\n\"\"\"RNN helpers for TensorFlow models.\n\n\n@@bidirectional_dynamic_rnn\n@@dynamic_rnn\n@@raw_rnn\n@@static_rnn\n@@static_state_saving_rnn\n@@static_bidirectional_rnn\n\"\"\"\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nfrom tensorflow.python.framework import constant_op\nfrom tensorflow.python.framework import dtypes\nfrom tensorflow.python.framework import ops\nfrom tensorflow.python.framework import tensor_shape\nfrom tensorflow.python.ops import array_ops\nfrom tensorflow.python.ops import control_flow_ops\nfrom tensorflow.python.ops import math_ops\nfrom tensorflow.python.ops import rnn_cell_impl\nfrom tensorflow.python.ops import tensor_array_ops\nfrom tensorflow.python.ops import variable_scope as vs\nfrom tensorflow.python.util import nest\n\n# pylint: disable=protected-access\n_concat = rnn_cell_impl._concat\n_like_rnncell = rnn_cell_impl._like_rnncell\n\n\n# pylint: enable=protected-access\n\n\ndef _best_effort_input_batch_size(flat_input):\n    \"\"\"Get static input batch size if available, with fallback to the dynamic one.\n\n    Args:\n      flat_input: An iterable of time major input Tensors of shape [max_time,\n        batch_size, ...]. All inputs should have compatible batch sizes.\n\n    Returns:\n      The batch size in Python integer if available, or a scalar Tensor otherwise.\n\n    Raises:\n      ValueError: if there is any input with an invalid shape.\n    \"\"\"", "docstring": "    \"\"\"Get static input batch size if available, with fallback to the dynamic one.\n\n    Args:\n      flat_input: An iterable of time major input Tensors of shape [max_time,\n        batch_size, ...]. All inputs should have compatible batch sizes.\n\n    Returns:\n      The batch size in Python integer if available, or a scalar Tensor otherwise.\n\n    Raises:\n      ValueError: if there is any input with an invalid shape.\n    \"\"\""}, "cbe2f672e4f4ad037540f9def00b9a3e04540b64": {"bleu": 0.7165935864103573, "jaccard": 0.47265625, "edit_distance": 0.5782608695652174, "comment_cnt": 2, "max_sim": 0.7165935864103573, "header": "\nimport datetime\nimport os\nimport subprocess\n\n\ndef get_version(version=None):\n    \"Returns a PEP 386-compliant version number from VERSION.\"", "body": "    if version is None:\n        from geonode import __version__ as version\n    else:\n        assert len(version) == 5\n        assert version[3] in ('unstable', 'beta', 'rc', 'final')\n\n    # Now build the two parts of the version number:\n    # main = X.Y[.Z]\n    # sub = .devN - for pre-alpha releases\n    #     | {a|b|c}N - for alpha, beta and rc releases\n\n    parts = 2 if version[2] == 0 else 3\n    main = '.'.join(str(x) for x in version[:parts])\n\n    sub = ''\n    if version[3] == 'unstable':\n        git_changeset = get_git_changeset()\n        if git_changeset:\n            sub = '.dev%s' % git_changeset\n\n    elif version[3] != 'final':\n        mapping = {'beta': 'b', 'rc': 'rc'}\n        sub = mapping[version[3]] + str(version[4])\n\n    return main + sub", "answer": "if version is None:\n        from . import VERSION as version\n\n    assert len(version) == 5\n    assert version[3] in ('alpha', 'beta', 'rc', 'final')\n\n    # Now build the two parts of the version number:\n    # main = X.Y[.Z]\n    parts = 2 if version[2] == 0 else 3\n    main = '.'.join(str(x) for x in version[:parts])\n\n    # sub = {a|b|c}N - for alpha, beta and rc releases\n    sub = ''\n    if version[3] != 'final':\n        mapping = {'alpha': 'a', 'beta': 'b', 'rc': 'c'}\n        sub = mapping[version[3]] + str(version[4])\n\n    return main + sub", "license": "gpl-3.0-or-later", "author": "Alessio Fabiani <alessio.fabiani@geo-solutions.it", "project": "GeoNode_geonode", "signature": "def get_version(version=None):", "license_type": "Strong Copyleft", "init_header": "# -*- coding: utf-8 -*-\n#########################################################################\n#\n# Copyright (C) 2016 OSGeo\n#\n# This program is free software: you can redistribute it and/or modify\n# it under the terms of the GNU General Public License as published by\n# the Free Software Foundation, either version 3 of the License, or\n# (at your option) any later version.\n#\n# This program is distributed in the hope that it will be useful,\n# but WITHOUT ANY WARRANTY; without even the implied warranty of\n# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the\n# GNU General Public License for more details.\n#\n# You should have received a copy of the GNU General Public License\n# along with this program. If not, see <http://www.gnu.org/licenses/>.\n#\n#########################################################################\n\nimport datetime\nimport os\nimport subprocess\n\n\ndef get_version(version=None):\n    \"Returns a PEP 386-compliant version number from VERSION.\"", "docstring": "    \"Returns a PEP 386-compliant version number from VERSION.\""}, "b990893eda84e69e8b7c4b6a6015ac6dc4729e8d": {"bleu": 0.5591795315983674, "jaccard": 0.296875, "edit_distance": 0.6875, "comment_cnt": 1, "max_sim": 0.6875, "header": "\nimport libcalamares\nimport os\nfrom collections import OrderedDict\n\n\ndef cpuinfo():\n    \"\"\"\n    Return the information in /proc/cpuinfo as a dictionary in the following\n    format:\n\n    cpu_info['proc0']={...}\n    cpu_info['proc1']={...}\n    \"\"\"", "body": "    cpu_info = OrderedDict()\n    procinfo = OrderedDict()\n\n    nprocs = 0\n\n    with open('/proc/cpuinfo') as cpuinfo_file:\n        for line in cpuinfo_file:\n            if not line.strip():\n                # end of one processor\n                cpu_info[\"proc{!s}\".format(nprocs)] = procinfo\n                nprocs += 1\n                # Reset\n                procinfo = OrderedDict()\n            else:\n                if len(line.split(':')) == 2:\n                    splitted_line = line.split(':')[1].strip()\n                    procinfo[line.split(':')[0].strip()] = splitted_line\n                else:\n                    procinfo[line.split(':')[0].strip()] = ''\n\n    return cpu_info", "answer": "    cpu_info = OrderedDict()\n    proc_info = OrderedDict()\n    nprocs = 0\n\n    with open('/proc/cpuinfo') as f:\n        for line in f:\n            if not line.strip():\n                # end of one processor\n                cpu_info[f'proc{nprocs}'] = proc_info\n                nprocs += 1\n                proc_info = OrderedDict()\n            else:\n                if len(line.split(':')) == 2:\n                    key, value = line.split(':')\n                    proc_info[key.strip()] = value.strip()\n                else:\n                    proc_info[line.strip()] = ''\n\n    return cpu_info", "license": "gpl-3.0-or-later", "author": "Philip <philm@manjaro.org", "project": "calamares_calamares", "signature": "def cpuinfo():", "license_type": "Strong Copyleft", "init_header": "#!/usr/bin/env python3\n# -*- coding: utf-8 -*-\n#\n# === This file is part of Calamares - <https://github.com/calamares> ===\n#\n#   Copyright 2014, Rohan Garg <rohan@kde.org>\n#   Copyright 2015, Philip M\u00fcller <philm@manjaro.org>\n#   Copyright 2017, Alf Gaida <agaida@sidution.org>\n#\n#   Calamares is free software: you can redistribute it and/or modify\n#   it under the terms of the GNU General Public License as published by\n#   the Free Software Foundation, either version 3 of the License, or\n#   (at your option) any later version.\n#\n#   Calamares is distributed in the hope that it will be useful,\n#   but WITHOUT ANY WARRANTY; without even the implied warranty of\n#   MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the\n#   GNU General Public License for more details.\n#\n#   You should have received a copy of the GNU General Public License\n#   along with Calamares. If not, see <http://www.gnu.org/licenses/>.\n\nimport libcalamares\nimport os\nfrom collections import OrderedDict\n\n\ndef cpuinfo():\n    \"\"\"\n    Return the information in /proc/cpuinfo as a dictionary in the following\n    format:\n\n    cpu_info['proc0']={...}\n    cpu_info['proc1']={...}\n    \"\"\"", "docstring": "    \"\"\"\n    Return the information in /proc/cpuinfo as a dictionary in the following\n    format:\n\n    cpu_info['proc0']={...}\n    cpu_info['proc1']={...}\n    \"\"\""}, "f4a177c8771051a96f59300057fa5880f412c62f": {"bleu": 0.963826265111922, "jaccard": 0.8515625, "edit_distance": 0.975975975975976, "comment_cnt": 8, "max_sim": 0.975975975975976, "header": "\n\"\"\"Train a simple LSTM model for character-level language translation.\n\nThis is based on the Keras example at:\n  https://github.com/keras-team/keras/blob/master/examples/lstm_seq2seq.py\n\nThe training data can be downloaded with a command like the following example:\n  wget http://www.manythings.org/anki/fra-eng.zip\n\"\"\"\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport argparse\nimport io\nimport json\nimport os\n\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.layers import Input, LSTM, Dense\nimport numpy as np\nimport tensorflowjs as tfjs\n\n\ndef decode_sequence(input_seq,\n                    encoder_model,\n                    decoder_model,\n                    num_decoder_tokens,\n                    target_begin_index,\n                    reverse_target_char_index,\n                    max_decoder_seq_length):\n  \"\"\"Decode (i.e., translate) an encoded sentence.\n\n  Args:\n    input_seq: A `numpy.ndarray` of shape\n      `(1, max_encoder_seq_length, num_encoder_tokens)`.\n    encoder_model: A `keras.Model` instance for the encoder.\n    decoder_model: A `keras.Model` instance for the decoder.\n    num_decoder_tokens: Number of unique tokens for the decoder.\n    target_begin_index: An `int`: the index for the beginning token of the\n      decoder.\n    reverse_target_char_index: A lookup table for the target characters, i.e.,\n      a map from `int` index to target character.\n    max_decoder_seq_length: Maximum allowed sequence length output by the\n      decoder.\n\n  Returns:\n    The result of the decoding (i.e., translation) as a string.\n  \"\"\"", "body": "\n  # Encode the input as state vectors.\n  states_value = encoder_model.predict(input_seq)\n\n  # Generate empty target sequence of length 1.\n  target_seq = np.zeros((1, 1, num_decoder_tokens))\n  # Populate the first character of target sequence with the start character.\n  target_seq[0, 0, target_begin_index] = 1.\n\n  # Sampling loop for a batch of sequences\n  # (to simplify, here we assume a batch of size 1).\n  stop_condition = False\n  decoded_sentence = ''\n  while not stop_condition:\n    output_tokens, h, c = decoder_model.predict(\n        [target_seq] + states_value)\n\n    # Sample a token\n    sampled_token_index = np.argmax(output_tokens[0, -1, :])\n    sampled_char = reverse_target_char_index[sampled_token_index]\n    decoded_sentence += sampled_char\n\n    # Exit condition: either hit max length\n    # or find stop character.\n    if (sampled_char == '\\n' or\n        len(decoded_sentence) > max_decoder_seq_length):\n      stop_condition = True\n\n    # Update the target sequence (of length 1).\n    target_seq = np.zeros((1, 1, num_decoder_tokens))\n    target_seq[0, 0, sampled_token_index] = 1.\n\n    # Update states\n    states_value = [h, c]\n\n  return decoded_sentence", "answer": "# Encode the input as state vectors.\n  states_value = encoder_model.predict(input_seq)\n\n  # Generate empty target sequence of length 1.\n  target_seq = np.zeros((1, 1, num_decoder_tokens))\n  # Populate the first character of target sequence with the start character.\n  target_seq[0, 0, target_begin_index] = 1.0\n\n  # Sampling loop for a batch of sequences\n  # (to simplify, here we assume a batch of size 1).\n  stop_condition = False\n  decoded_sentence = ''\n  while not stop_condition:\n    output_tokens, h, c = decoder_model.predict([target_seq] + states_value)\n\n    # Sample a token\n    sampled_token_index = np.argmax(output_tokens[0, -1, :])\n    sampled_char = reverse_target_char_index[sampled_token_index]\n    decoded_sentence += sampled_char\n\n    # Exit condition: either hit max length or find stop character.\n    if (sampled_char == '\\n' or\n       len(decoded_sentence) > max_decoder_seq_length):\n      stop_condition = True\n\n    # Update the target sequence (of length 1).\n    target_seq = np.zeros((1, 1, num_decoder_tokens))\n    target_seq[0, 0, sampled_token_index] = 1.0\n\n    # Update states\n    states_value = [h, c]\n\n  return decoded_sentence", "license": "apache-2.0", "author": "Marianne Linhares Monteiro <mariannelinharesm@gmail.com", "project": "tensorflow_tfjs-examples", "signature": "def decode_sequence(input_seq,", "license_type": "Permissive", "init_header": "# Copyright 2018 Google LLC. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# =============================================================================\n\n\"\"\"Train a simple LSTM model for character-level language translation.\n\nThis is based on the Keras example at:\n  https://github.com/keras-team/keras/blob/master/examples/lstm_seq2seq.py\n\nThe training data can be downloaded with a command like the following example:\n  wget http://www.manythings.org/anki/fra-eng.zip\n\"\"\"\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport argparse\nimport io\nimport json\nimport os\n\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.layers import Input, LSTM, Dense\nimport numpy as np\nimport tensorflowjs as tfjs\n\n\ndef decode_sequence(input_seq,\n                    encoder_model,\n                    decoder_model,\n                    num_decoder_tokens,\n                    target_begin_index,\n                    reverse_target_char_index,\n                    max_decoder_seq_length):\n  \"\"\"Decode (i.e., translate) an encoded sentence.\n\n  Args:\n    input_seq: A `numpy.ndarray` of shape\n      `(1, max_encoder_seq_length, num_encoder_tokens)`.\n    encoder_model: A `keras.Model` instance for the encoder.\n    decoder_model: A `keras.Model` instance for the decoder.\n    num_decoder_tokens: Number of unique tokens for the decoder.\n    target_begin_index: An `int`: the index for the beginning token of the\n      decoder.\n    reverse_target_char_index: A lookup table for the target characters, i.e.,\n      a map from `int` index to target character.\n    max_decoder_seq_length: Maximum allowed sequence length output by the\n      decoder.\n\n  Returns:\n    The result of the decoding (i.e., translation) as a string.\n  \"\"\"", "docstring": "                    encoder_model,\n                    decoder_model,\n                    num_decoder_tokens,\n                    target_begin_index,\n                    reverse_target_char_index,\n                    max_decoder_seq_length):\n  \"\"\"Decode (i.e., translate) an encoded sentence.\n\n  Args:\n    input_seq: A `numpy.ndarray` of shape\n      `(1, max_encoder_seq_length, num_encoder_tokens)`.\n    encoder_model: A `keras.Model` instance for the encoder.\n    decoder_model: A `keras.Model` instance for the decoder.\n    num_decoder_tokens: Number of unique tokens for the decoder.\n    target_begin_index: An `int`: the index for the beginning token of the\n      decoder.\n    reverse_target_char_index: A lookup table for the target characters, i.e.,\n      a map from `int` index to target character.\n    max_decoder_seq_length: Maximum allowed sequence length output by the\n      decoder.\n\n  Returns:\n    The result of the decoding (i.e., translation) as a string.\n  \"\"\""}, "265538ed37984464f47d5b4e20e34528f7382b7e": {"bleu": 0.8829926031993809, "jaccard": 0.76953125, "edit_distance": 0.8716049382716049, "comment_cnt": 2, "max_sim": 0.8829926031993809, "header": "\n\n\"\"\"Module implementing RNN Cells.\"\"\"\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport collections\nimport contextlib\nimport hashlib\nimport math\nimport numbers\n\nfrom tensorflow.python.framework import ops\nfrom tensorflow.python.framework import tensor_shape\nfrom tensorflow.python.framework import tensor_util\nfrom tensorflow.python.ops import array_ops\nfrom tensorflow.python.ops import clip_ops\nfrom tensorflow.python.ops import embedding_ops\nfrom tensorflow.python.ops import init_ops\nfrom tensorflow.python.ops import math_ops\nfrom tensorflow.python.ops import nn_ops\nfrom tensorflow.python.ops import partitioned_variables\nfrom tensorflow.python.ops import random_ops\nfrom tensorflow.python.ops import variable_scope as vs\n\nfrom tensorflow.python.ops.math_ops import sigmoid\nfrom tensorflow.python.ops.math_ops import tanh\nfrom tensorflow.python.ops.rnn_cell_impl import RNNCell\n\nfrom tensorflow.python.platform import tf_logging as logging\nfrom tensorflow.python.util import nest\n\n\n_BIAS_VARIABLE_NAME = \"biases\"\n_WEIGHTS_VARIABLE_NAME = \"weights\"\n\n\n@contextlib.contextmanager\ndef _linear(args, output_size, bias, bias_start=0.0, scope=None):\n  \"\"\"Linear map: sum_i(args[i] * W[i]), where W[i] is a variable.\n\n  Args:\n    args: a 2D Tensor or a list of 2D, batch x n, Tensors.\n    output_size: int, second dimension of W[i].\n    bias: boolean, whether to add a bias term or not.\n    bias_start: starting value to initialize the bias; 0 by default.\n\n  Returns:\n    A 2D Tensor with shape [batch x output_size] equal to\n    sum_i(args[i] * W[i]), where W[i]s are newly created matrices.\n\n  Raises:\n    ValueError: if some of the arguments has unspecified or wrong shape.\n  \"\"\"", "body": "  if args is None or (nest.is_sequence(args) and not args):\n    raise ValueError(\"`args` must be specified\")\n  if not nest.is_sequence(args):\n    args = [args]\n\n  # Calculate the total size of arguments on dimension 1.\n  total_arg_size = 0\n  shapes = [a.get_shape() for a in args]\n  for shape in shapes:\n    if shape.ndims != 2:\n      raise ValueError(\"linear is expecting 2D arguments: %s\" % shapes)\n    if shape[1].value is None:\n      raise ValueError(\"linear expects shape[1] to be provided for shape %s, \"\n                       \"but saw %s\" % (shape, shape[1]))\n    else:\n      total_arg_size += shape[1].value\n\n  dtype = [a.dtype for a in args][0]\n\n  # Now the computation.\n  scope = vs.get_variable_scope()\n  with vs.variable_scope(scope) as outer_scope:\n    weights = vs.get_variable(\n        _WEIGHTS_VARIABLE_NAME, [total_arg_size, output_size], dtype=dtype)\n    if len(args) == 1:\n      res = math_ops.matmul(args[0], weights)\n    else:\n      res = math_ops.matmul(array_ops.concat(args, 1), weights)\n    if not bias:\n      return res\n    with vs.variable_scope(outer_scope) as inner_scope:\n      inner_scope.set_partitioner(None)\n      biases = vs.get_variable(\n          _BIAS_VARIABLE_NAME, [output_size],\n          dtype=dtype,\n          initializer=init_ops.constant_initializer(bias_start, dtype=dtype))\n    return nn_ops.bias_add(res, biases)", "answer": "if args is None or (nest.is_sequence(args) and not args):\n    raise ValueError(\"`args` must be specified\")\n  if not nest.is_sequence(args):\n    args = [args]\n\n  # Calculate the total size of arguments on dimension 1.\n  total_arg_size = 0\n  shapes = [a.get_shape() for a in args]\n  for shape in shapes:\n    if shape.ndims != 2:\n      raise ValueError(\"linear is expecting 2D arguments: %s\" % shapes)\n    if shape[1].value is None:\n      raise ValueError(\"linear expects shape[1] to be provided for shape %s, \"\n                       \"but saw %s\" % (shape, shape[1]))\n    total_arg_size += shape[1].value\n\n  dtype = [a.dtype for a in args][0]\n\n  # Now the computation.\n  with vs.variable_scope(scope or \"Linear\"):\n    weights = vs.get_variable(\n        _WEIGHTS_VARIABLE_NAME, [total_arg_size, output_size], dtype=dtype)\n    if len(args) == 1:\n      res = math_ops.matmul(args[0], weights)\n    else:\n      res = math_ops.matmul(array_ops.concat(args, 1), weights)\n    if not bias:\n      return res\n    biases = vs.get_variable(\n        _BIAS_VARIABLE_NAME, [output_size],\n        dtype=dtype,\n        initializer=init_ops.constant_initializer(bias_start, dtype=dtype))\n    return nn_ops.bias_add(res, biases)", "license": "apache-2.0", "author": "Stephanie Hyland <steph.hyland@gmail.com", "project": "ascentai_rgan", "signature": "def _linear(args, output_size, bias, bias_start=0.0, scope=None):", "license_type": "Permissive", "init_header": "# Copyright 2015 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n#modified by Stephanie (@corcra) to enable initializing the bias term in lstm \"\"\"\n# ==============================================================================\n\n\"\"\"Module implementing RNN Cells.\"\"\"\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport collections\nimport contextlib\nimport hashlib\nimport math\nimport numbers\n\nfrom tensorflow.python.framework import ops\nfrom tensorflow.python.framework import tensor_shape\nfrom tensorflow.python.framework import tensor_util\nfrom tensorflow.python.ops import array_ops\nfrom tensorflow.python.ops import clip_ops\nfrom tensorflow.python.ops import embedding_ops\nfrom tensorflow.python.ops import init_ops\nfrom tensorflow.python.ops import math_ops\nfrom tensorflow.python.ops import nn_ops\nfrom tensorflow.python.ops import partitioned_variables\nfrom tensorflow.python.ops import random_ops\nfrom tensorflow.python.ops import variable_scope as vs\n\nfrom tensorflow.python.ops.math_ops import sigmoid\nfrom tensorflow.python.ops.math_ops import tanh\n#from tensorflow.python.ops.rnn_cell_impl import _RNNCell as RNNCell\nfrom tensorflow.python.ops.rnn_cell_impl import RNNCell\n\nfrom tensorflow.python.platform import tf_logging as logging\nfrom tensorflow.python.util import nest\n\n\n_BIAS_VARIABLE_NAME = \"biases\"\n_WEIGHTS_VARIABLE_NAME = \"weights\"\n\n\n@contextlib.contextmanager\ndef _linear(args, output_size, bias, bias_start=0.0, scope=None):\n  \"\"\"Linear map: sum_i(args[i] * W[i]), where W[i] is a variable.\n\n  Args:\n    args: a 2D Tensor or a list of 2D, batch x n, Tensors.\n    output_size: int, second dimension of W[i].\n    bias: boolean, whether to add a bias term or not.\n    bias_start: starting value to initialize the bias; 0 by default.\n\n  Returns:\n    A 2D Tensor with shape [batch x output_size] equal to\n    sum_i(args[i] * W[i]), where W[i]s are newly created matrices.\n\n  Raises:\n    ValueError: if some of the arguments has unspecified or wrong shape.\n  \"\"\"", "docstring": "  \"\"\"Linear map: sum_i(args[i] * W[i]), where W[i] is a variable.\n\n  Args:\n    args: a 2D Tensor or a list of 2D, batch x n, Tensors.\n    output_size: int, second dimension of W[i].\n    bias: boolean, whether to add a bias term or not.\n    bias_start: starting value to initialize the bias; 0 by default.\n\n  Returns:\n    A 2D Tensor with shape [batch x output_size] equal to\n    sum_i(args[i] * W[i]), where W[i]s are newly created matrices.\n\n  Raises:\n    ValueError: if some of the arguments has unspecified or wrong shape.\n  \"\"\""}, "ebfaacb8a2c969c47a42fbbaa057842c71596f86": {"bleu": 0.6912400624951066, "jaccard": 0.51953125, "edit_distance": 0.6218565662837627, "comment_cnt": 28, "max_sim": 0.6912400624951066, "header": "\"\"\"Basic word2vec example.\"\"\"\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport argparse\nimport collections\nimport math\nimport os\nimport random\nimport sys\nfrom tempfile import gettempdir\nimport zipfile\n\nimport numpy as np\nfrom six.moves import urllib\nfrom six.moves import xrange  # pylint: disable=redefined-builtin\nimport tensorflow as tf\n\nfrom tensorflow.contrib.tensorboard.plugins import projector\n\ndata_index = 0\n\n\ndef word2vec_basic(log_dir):\n  \"\"\"Example of building, training and visualizing a word2vec model.\"\"\"", "body": "  # Create the directory for TensorBoard variables if there is not.\n  if not os.path.exists(log_dir):\n    os.makedirs(log_dir)\n\n  # Step 1: Download the data.\n  url = 'http://mattmahoney.net/dc/'\n\n  # pylint: disable=redefined-outer-name\n  def maybe_download(filename, expected_bytes):\n    \"\"\"Download a file if not present, and make sure it's the right size.\"\"\"\n    local_filename = os.path.join(gettempdir(), filename)\n    if not os.path.exists(local_filename):\n      local_filename, _ = urllib.request.urlretrieve(url + filename,\n                                                     local_filename)\n    statinfo = os.stat(local_filename)\n    if statinfo.st_size == expected_bytes:\n      print('Found and verified', filename)\n    else:\n      print(statinfo.st_size)\n      raise Exception('Failed to verify ' + local_filename +\n                      '. Can you get to it with a browser?')\n    return local_filename\n\n  filename = maybe_download('text8.zip', 31344016)\n\n  # Read the data into a list of strings.\n  def read_data(filename):\n    \"\"\"Extract the first file enclosed in a zip file as a list of words.\"\"\"\n    with zipfile.ZipFile(filename) as f:\n      data = tf.compat.as_str(f.read(f.namelist()[0])).split()\n    return data\n\n  vocabulary = read_data(filename)\n  print('Data size', len(vocabulary))\n\n  # Step 2: Build the dictionary and replace rare words with UNK token.\n  vocabulary_size = 50000\n\n  def build_dataset(words, n_words):\n    \"\"\"Process raw inputs into a dataset.\"\"\"\n    count = [['UNK', -1]]\n    count.extend(collections.Counter(words).most_common(n_words - 1))\n    dictionary = {}\n    for word, _ in count:\n      dictionary[word] = len(dictionary)\n    data = []\n    unk_count = 0\n    for word in words:\n      index = dictionary.get(word, 0)\n      if index == 0:  # dictionary['UNK']\n        unk_count += 1\n      data.append(index)\n    count[0][1] = unk_count\n    reversed_dictionary = dict(zip(dictionary.values(), dictionary.keys()))\n    return data, count, dictionary, reversed_dictionary\n\n  # Filling 4 global variables:\n  # data - list of codes (integers from 0 to vocabulary_size-1).\n  #   This is the original text but words are replaced by their codes\n  # count - map of words(strings) to count of occurrences\n  # dictionary - map of words(strings) to their codes(integers)\n  # reverse_dictionary - maps codes(integers) to words(strings)\n  data, count, unused_dictionary, reverse_dictionary = build_dataset(\n      vocabulary, vocabulary_size)\n  del vocabulary  # Hint to reduce memory.\n  print('Most common words (+UNK)', count[:5])\n  print('Sample data', data[:10], [reverse_dictionary[i] for i in data[:10]])\n\n  # Step 3: Function to generate a training batch for the skip-gram model.\n  def generate_batch(batch_size, num_skips, skip_window):\n    global data_index\n    assert batch_size % num_skips == 0\n    assert num_skips <= 2 * skip_window\n    batch = np.ndarray(shape=(batch_size), dtype=np.int32)\n    labels = np.ndarray(shape=(batch_size, 1), dtype=np.int32)\n    span = 2 * skip_window + 1  # [ skip_window target skip_window ]\n    buffer = collections.deque(maxlen=span)  # pylint: disable=redefined-builtin\n    if data_index + span > len(data):\n      data_index = 0\n    buffer.extend(data[data_index:data_index + span])\n    data_index += span\n    for i in range(batch_size // num_skips):\n      context_words = [w for w in range(span) if w != skip_window]\n      words_to_use = random.sample(context_words, num_skips)\n      for j, context_word in enumerate(words_to_use):\n        batch[i * num_skips + j] = buffer[skip_window]\n        labels[i * num_skips + j, 0] = buffer[context_word]\n      if data_index == len(data):\n        buffer.extend(data[0:span])\n        data_index = span\n      else:\n        buffer.append(data[data_index])\n        data_index += 1\n    # Backtrack a little bit to avoid skipping words in the end of a batch\n    data_index = (data_index + len(data) - span) % len(data)\n    return batch, labels\n\n  batch, labels = generate_batch(batch_size=8, num_skips=2, skip_window=1)\n  for i in range(8):\n    print(batch[i], reverse_dictionary[batch[i]], '->', labels[i, 0],\n          reverse_dictionary[labels[i, 0]])\n\n  # Step 4: Build and train a skip-gram model.\n\n  batch_size = 128\n  embedding_size = 128  # Dimension of the embedding vector.\n  skip_window = 1  # How many words to consider left and right.\n  num_skips = 2  # How many times to reuse an input to generate a label.\n  num_sampled = 64  # Number of negative examples to sample.\n\n  # We pick a random validation set to sample nearest neighbors. Here we limit\n  # the validation samples to the words that have a low numeric ID, which by\n  # construction are also the most frequent. These 3 variables are used only for\n  # displaying model accuracy, they don't affect calculation.\n  valid_size = 16  # Random set of words to evaluate similarity on.\n  valid_window = 100  # Only pick dev samples in the head of the distribution.\n  valid_examples = np.random.choice(valid_window, valid_size, replace=False)\n\n  graph = tf.Graph()\n\n  with graph.as_default():\n\n    # Input data.\n    with tf.name_scope('inputs'):\n      train_inputs = tf.placeholder(tf.int32, shape=[batch_size])\n      train_labels = tf.placeholder(tf.int32, shape=[batch_size, 1])\n      valid_dataset = tf.constant(valid_examples, dtype=tf.int32)\n\n    # Ops and variables pinned to the CPU because of missing GPU implementation\n    with tf.device('/cpu:0'):\n      # Look up embeddings for inputs.\n      with tf.name_scope('embeddings'):\n        embeddings = tf.Variable(\n            tf.random_uniform([vocabulary_size, embedding_size], -1.0, 1.0))\n        embed = tf.nn.embedding_lookup(embeddings, train_inputs)\n\n      # Construct the variables for the NCE loss\n      with tf.name_scope('weights'):\n        nce_weights = tf.Variable(\n            tf.truncated_normal([vocabulary_size, embedding_size],\n                                stddev=1.0 / math.sqrt(embedding_size)))\n      with tf.name_scope('biases'):\n        nce_biases = tf.Variable(tf.zeros([vocabulary_size]))\n\n    # Compute the average NCE loss for the batch.\n    # tf.nce_loss automatically draws a new sample of the negative labels each\n    # time we evaluate the loss.\n    # Explanation of the meaning of NCE loss:\n    #   http://mccormickml.com/2016/04/19/word2vec-tutorial-the-skip-gram-model/\n    with tf.name_scope('loss'):\n      loss = tf.reduce_mean(\n          tf.nn.nce_loss(\n              weights=nce_weights,\n              biases=nce_biases,\n              labels=train_labels,\n              inputs=embed,\n              num_sampled=num_sampled,\n              num_classes=vocabulary_size))\n\n    # Add the loss value as a scalar to summary.\n    tf.summary.scalar('loss', loss)\n\n    # Construct the SGD optimizer using a learning rate of 1.0.\n    with tf.name_scope('optimizer'):\n      optimizer = tf.train.GradientDescentOptimizer(1.0).minimize(loss)\n\n    # Compute the cosine similarity between minibatch examples and all\n    # embeddings.\n    norm = tf.sqrt(tf.reduce_sum(tf.square(embeddings), 1, keepdims=True))\n    normalized_embeddings = embeddings / norm\n    valid_embeddings = tf.nn.embedding_lookup(normalized_embeddings,\n                                              valid_dataset)\n    similarity = tf.matmul(\n        valid_embeddings, normalized_embeddings, transpose_b=True)\n\n    # Merge all summaries.\n    merged = tf.summary.merge_all()\n\n    # Add variable initializer.\n    init = tf.global_variables_initializer()\n\n    # Create a saver.\n    saver = tf.train.Saver()\n\n  # Step 5: Begin training.\n  num_steps = 100001\n\n  with tf.Session(graph=graph) as session:\n    # Open a writer to write summaries.\n    writer = tf.summary.FileWriter(log_dir, session.graph)\n\n    # We must initialize all variables before we use them.\n    init.run()\n    print('Initialized')\n\n    average_loss = 0\n    for step in xrange(num_steps):\n      batch_inputs, batch_labels = generate_batch(batch_size, num_skips,\n                                                  skip_window)\n      feed_dict = {train_inputs: batch_inputs, train_labels: batch_labels}\n\n      # Define metadata variable.\n      run_metadata = tf.RunMetadata()\n\n      # We perform one update step by evaluating the optimizer op (including it\n      # in the list of returned values for session.run()\n      # Also, evaluate the merged op to get all summaries from the returned\n      # \"summary\" variable. Feed metadata variable to session for visualizing\n      # the graph in TensorBoard.\n      _, summary, loss_val = session.run([optimizer, merged, loss],\n                                         feed_dict=feed_dict,\n                                         run_metadata=run_metadata)\n      average_loss += loss_val\n\n      # Add returned summaries to writer in each step.\n      writer.add_summary(summary, step)\n      # Add metadata to visualize the graph for the last run.\n      if step == (num_steps - 1):\n        writer.add_run_metadata(run_metadata, 'step%d' % step)\n\n      if step % 2000 == 0:\n        if step > 0:\n          average_loss /= 2000\n        # The average loss is an estimate of the loss over the last 2000\n        # batches.\n        print('Average loss at step ', step, ': ', average_loss)\n        average_loss = 0\n\n      # Note that this is expensive (~20% slowdown if computed every 500 steps)\n      if step % 10000 == 0:\n        sim = similarity.eval()\n        for i in xrange(valid_size):\n          valid_word = reverse_dictionary[valid_examples[i]]\n          top_k = 8  # number of nearest neighbors\n          nearest = (-sim[i, :]).argsort()[1:top_k + 1]\n          log_str = 'Nearest to %s:' % valid_word\n          for k in xrange(top_k):\n            close_word = reverse_dictionary[nearest[k]]\n            log_str = '%s %s,' % (log_str, close_word)\n          print(log_str)\n    final_embeddings = normalized_embeddings.eval()\n\n    # Write corresponding labels for the embeddings.\n    with open(log_dir + '/metadata.tsv', 'w') as f:\n      for i in xrange(vocabulary_size):\n        f.write(reverse_dictionary[i] + '\\n')\n\n    # Save the model for checkpoints.\n    saver.save(session, os.path.join(log_dir, 'model.ckpt'))\n\n    # Create a configuration for visualizing embeddings with the labels in\n    # TensorBoard.\n    config = projector.ProjectorConfig()\n    embedding_conf = config.embeddings.add()\n    embedding_conf.tensor_name = embeddings.name\n    embedding_conf.metadata_path = os.path.join(log_dir, 'metadata.tsv')\n    projector.visualize_embeddings(writer, config)\n\n  writer.close()\n\n  # Step 6: Visualize the embeddings.\n\n  # pylint: disable=missing-docstring\n  # Function to draw visualization of distance between embeddings.\n  def plot_with_labels(low_dim_embs, labels, filename):\n    assert low_dim_embs.shape[0] >= len(labels), 'More labels than embeddings'\n    plt.figure(figsize=(18, 18))  # in inches\n    for i, label in enumerate(labels):\n      x, y = low_dim_embs[i, :]\n      plt.scatter(x, y)\n      plt.annotate(\n          label,\n          xy=(x, y),\n          xytext=(5, 2),\n          textcoords='offset points',\n          ha='right',\n          va='bottom')\n\n    plt.savefig(filename)\n\n  try:\n    # pylint: disable=g-import-not-at-top\n    from sklearn.manifold import TSNE\n    import matplotlib.pyplot as plt\n\n    tsne = TSNE(\n        perplexity=30, n_components=2, init='pca', n_iter=5000, method='exact')\n    plot_only = 500\n    low_dim_embs = tsne.fit_transform(final_embeddings[:plot_only, :])\n    labels = [reverse_dictionary[i] for i in xrange(plot_only)]\n    plot_with_labels(low_dim_embs, labels, os.path.join(gettempdir(),\n                                                        'tsne.png'))\n\n  except ImportError as ex:\n    print('Please install sklearn, matplotlib, and scipy to show embeddings.')\n    print(ex)", "answer": "# Step 1: Download the data.\n  url = 'http://mattmahoney.net/dc/'\n\n  def maybe_download(filename, expected_bytes):\n    local_filename = os.path.join(gettempdir(), filename)\n    if not os.path.exists(local_filename):\n      local_filename, _ = urllib.request.urlretrieve(url + filename, local_filename)\n    statinfo = os.stat(local_filename)\n    if statinfo.st_size == expected_bytes:\n      print('Found and verified', filename)\n    else:\n      print(statinfo.st_size)\n      raise Exception('Failed to verify ' + local_filename)\n    return local_filename\n\n  filename = maybe_download('text8.zip', 31344016)\n\n  # Step 2: Read the data into a list of strings.\n  def read_data(filename):\n    with zipfile.ZipFile(filename) as f:\n      data = tf.compat.as_str(f.read(f.namelist()[0])).split()\n    return data\n\n  words = read_data(filename)\n  print('Data size', len(words))\n\n  # Step 3: Build the dictionary and replace rare words with UNK token.\n  vocabulary_size = 50000\n\n  def build_dataset(words, n_words):\n    count = [['UNK', -1]]\n    count.extend(collections.Counter(words).most_common(n_words - 1))\n    dictionary = dict()\n    for word, _ in count:\n      dictionary[word] = len(dictionary)\n    data = list()\n    unk_count = 0\n    for word in words:\n      if word in dictionary:\n        index = dictionary[word]\n      else:\n        index = 0  # dictionary['UNK']\n        unk_count += 1\n      data.append(index)\n    count[0][1] = unk_count\n    reversed_dictionary = dict(zip(dictionary.values(), dictionary))\n    return data, count, dictionary, reversed_dictionary\n\n  data, count, dictionary, reversed_dictionary = build_dataset(words, vocabulary_size)\n  del words  # Hint to reduce memory.\n  print('Most common words (+UNK)', count[:5])\n  print('Sample data', data[:10], [reversed_dictionary[i] for i in data[:10]])\n\n  # Step 4: Function to generate a training batch for the skip-gram model.\n  def generate_batch(batch_size, num_skips, skip_window):\n    global data_index\n    assert batch_size % num_skips == 0\n    assert num_skips <= 2 * skip_window\n    batch = np.ndarray(shape=(batch_size), dtype=np.int32)\n    labels = np.ndarray(shape=(batch_size, 1), dtype=np.int32)\n    span = 2 * skip_window + 1  # [ skip_window target skip_window ]\n    buffer = collections.deque(maxlen=span)\n    for _ in range(span):\n      buffer.append(data[data_index])\n      data_index = (data_index + 1) % len(data)\n    for i in range(batch_size // num_skips):\n      target = skip_window  # target label at the center of the buffer\n      targets_to_avoid = [skip_window]\n      for j in range(num_skips):\n        while target in targets_to_avoid:\n          target = random.randint(0, span - 1)\n        targets_to_avoid.append(target)\n        batch[i * num_skips + j] = buffer[skip_window]\n        labels[i * num_skips + j, 0] = buffer[target]\n      buffer.append(data[data_index])\n      data_index = (data_index + 1) % len(data)\n    return batch, labels\n\n  batch, labels = generate_batch(batch_size=8, num_skips=2, skip_window=1)\n  for i in range(8):\n    print(batch[i], reversed_dictionary[batch[i]],\n          '->', labels[i, 0], reversed_dictionary[labels[i, 0]])\n\n  # Step 5: Build and train a skip-gram model.\n  batch_size = 128\n  embedding_size = 128  # Dimension of the embedding vector.\n  skip_window = 1       # How many words to consider left and right.\n  num_skips = 2         # How many times to reuse an input to generate a label.\n\n  # We pick a random validation set to sample nearest neighbors. Here we limit the\n  # validation samples to the words that have a low numeric ID, which by\n  # construction are also the most frequent. These 3 variables are used only for\n  # displaying model accuracy, they don't affect calculation.\n  valid_size = 16     # Random set of words to evaluate similarity on.\n  valid_window = 100  # Only pick dev samples in the head of the distribution.\n  valid_examples = np.random.choice(valid_window, valid_size, replace=False)\n  num_sampled = 64    # Number of negative examples to sample.\n\n  graph = tf.Graph()\n\n  with graph.as_default():\n\n    # Input data.\n    train_inputs = tf.placeholder(tf.int32, shape=[batch_size])\n    train_labels = tf.placeholder(tf.int32, shape=[batch_size, 1])\n    valid_dataset = tf.constant(valid_examples, dtype=tf.int32)\n\n    # Ops and variables pinned to the CPU because of missing GPU implementation\n    with tf.device('/cpu:0'):\n      # Look up embeddings for inputs.\n      embeddings = tf.Variable(\n          tf.random_uniform([vocabulary_size, embedding_size], -1.0, 1.0))\n      embed = tf.nn.embedding_lookup(embeddings, train_inputs)\n\n      # Construct the variables for the NCE loss\n      nce_weights = tf.Variable(\n          tf.truncated_normal([vocabulary_size, embedding_size],\n                              stddev=1.0 / math.sqrt(embedding_size)))\n      nce_biases = tf.Variable(tf.zeros([vocabulary_size]))\n\n    # Compute the average NCE loss for the batch.\n    # tf.nce_loss automatically draws a new sample of the negative labels each\n    # time we evaluate the loss.\n    loss = tf.reduce_mean(\n        tf.nn.nce_loss(weights=nce_weights,\n                       biases=nce_biases,\n                       labels=train_labels,\n                       inputs=embed,\n                       num_sampled=num_sampled,\n                       num_classes=vocabulary_size))\n\n    # Construct the SGD optimizer using a learning rate of 1.0.\n    optimizer = tf.train.GradientDescentOptimizer(1.0).minimize(loss)\n\n    # Compute the cosine similarity between minibatch examples and all embeddings.\n    norm = tf.sqrt(tf.reduce_sum(tf.square(embeddings), 1, keepdims=True))\n    normalized_embeddings = embeddings / norm\n    valid_embeddings = tf.nn.embedding_lookup(\n        normalized_embeddings, valid_dataset)\n    similarity = tf.matmul(\n        valid_embeddings, normalized_embeddings, transpose_b=True)\n\n    # Add variable initializer.\n    init = tf.global_variables_initializer()\n\n  # Step 6: Begin training.\n  num_steps = 100001\n\n  with tf.Session(graph=graph) as session:\n    # We must initialize all variables before we use them.\n    init.run()\n    print('Initialized')\n\n    average_loss = 0\n    for step in range(num_steps):\n      batch_inputs, batch_labels = generate_batch(\n          batch_size, num_skips, skip_window)\n      feed_dict = {train_inputs: batch_inputs, train_labels: batch_labels}\n\n      # We perform one update step by evaluating the optimizer op (including it\n      # in the list of returned values for session.run()\n      _, loss_val = session.run([optimizer, loss], feed_dict=feed_dict)\n      average_loss += loss_val\n\n      if step % 2000 == 0:\n        if step > 0:\n          average_loss /= 2000\n        # The average loss is an estimate of the loss over the last 2000 batches.\n        print('Average loss at step ', step, ': ', average_loss)\n        average_loss = 0\n\n      # Note that this is expensive (~20% slowdown if computed every 500 steps)\n      if step % 10000 == 0:\n        sim = similarity.eval()\n        for i in range(valid_size):\n          valid_word = reversed_dictionary[valid_examples[i]]\n          top_k = 8  # number of nearest neighbors\n          nearest = (-sim[i, :]).argsort()[1:top_k + 1]\n          log_str = 'Nearest to %s:' % valid_word\n          for k in range(top_k):\n            close_word = reversed_dictionary[nearest[k]]\n            log_str = '%s %s,' % (log_str, close_word)\n          print(log_str)\n    final_embeddings = normalized_embeddings.eval()\n\n  # Step 7: Visualize the embeddings.\n  def plot_with_labels(low_dim_embs, labels, filename='tsne.png'):\n    assert low_dim_embs.shape[0] >= len(labels), 'More labels than embeddings'\n    plt.figure(figsize=(18, 18))  # in inches\n    for i, label in enumerate(labels):\n      x, y = low_dim_embs[i, :]\n      plt.scatter(x, y)\n      plt.annotate(label,\n                   xy=(x, y),\n                   xytext=(5, 2),\n                   textcoords='offset points',\n                   ha='right',\n                   va='bottom')\n\n    plt.savefig(filename)\n\n  try:\n    from sklearn.manifold import TSNE\n    import matplotlib.pyplot as plt\n\n    tsne = TSNE(perplexity=30, n_components=2, init='pca', n_iter=5000)\n    plot_only = 500\n    low_dim_embs = tsne.fit_transform(final_embeddings[:plot_only, :])\n    labels = [reversed_dictionary[i] for i in range(plot_only)]\n    plot_with_labels(low_dim_embs, labels)\n\n  except ImportError as ex:\n    print('Please install sklearn, matplotlib, and scipy to visualize embeddings.')\n    print(ex)\n\n  # Save the model for checkpoints.\n  with graph.as_default():\n    saver = tf.train.Saver()\n    saver.save(session, os.path.join(log_dir, 'model.ckpt'))\n\n    # Create a configuration for visualizing embeddings with the labels in TensorBoard.\n    config = projector.ProjectorConfig()\n    embedding = config.embeddings.add()\n    embedding.tensor_name = embeddings.name\n    # Link this tensor to its metadata file (e.g. labels).\n    embedding.metadata_path = os.path.join(log_dir, 'metadata.tsv')\n    projector.visualize_embeddings(tf.summary.FileWriter(log_dir), config)", "license": "apache-2.0", "author": "Mark Daoust <markdaoust@google.com", "project": "tensorflow_tensorflow", "signature": "def word2vec_basic(log_dir):", "license_type": "Permissive", "init_header": "# Copyright 2015 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\"\"\"Basic word2vec example.\"\"\"\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport argparse\nimport collections\nimport math\nimport os\nimport random\nimport sys\nfrom tempfile import gettempdir\nimport zipfile\n\nimport numpy as np\nfrom six.moves import urllib\nfrom six.moves import xrange  # pylint: disable=redefined-builtin\nimport tensorflow as tf\n\nfrom tensorflow.contrib.tensorboard.plugins import projector\n\ndata_index = 0\n\n\ndef word2vec_basic(log_dir):\n  \"\"\"Example of building, training and visualizing a word2vec model.\"\"\"", "docstring": "  \"\"\"Example of building, training and visualizing a word2vec model.\"\"\""}, "94c4b0db9a13d5891049637c3fd62fd0b8bfaafb": {"bleu": 0.9395056844397062, "jaccard": 0.90625, "edit_distance": 0.9328968903436988, "comment_cnt": 1, "max_sim": 0.9395056844397062, "header": "\"\"\" PyTorch XXX model. \"\"\"\n\n\nfrom __future__ import absolute_import, division, print_function, unicode_literals\n\nimport json\nimport logging\nimport math\nimport os\nimport sys\nfrom io import open\n\nimport torch\nfrom torch import nn\nfrom torch.nn import CrossEntropyLoss, MSELoss\n\nfrom .modeling_utils import PreTrainedModel, prune_linear_layer\nfrom .configuration_xxx import XxxConfig\nfrom .file_utils import add_start_docstrings\n\nlogger = logging.getLogger(__name__)\n\nXXX_PRETRAINED_MODEL_ARCHIVE_MAP = {\n    'xxx-base-uncased': \"https://s3.amazonaws.com/models.huggingface.co/bert/xxx-base-uncased-pytorch_model.bin\",\n    'xxx-large-uncased': \"https://s3.amazonaws.com/models.huggingface.co/bert/xxx-large-uncased-pytorch_model.bin\",\n}\n\ndef load_tf_weights_in_xxx(model, config, tf_checkpoint_path):\n    \"\"\" Load tf checkpoints in a pytorch model.\n    \"\"\"", "body": "    try:\n        import re\n        import numpy as np\n        import tensorflow as tf\n    except ImportError:\n        logger.error(\"Loading a TensorFlow model in PyTorch, requires TensorFlow to be installed. Please see \"\n            \"https://www.tensorflow.org/install/ for installation instructions.\")\n        raise\n    tf_path = os.path.abspath(tf_checkpoint_path)\n    logger.info(\"Converting TensorFlow checkpoint from {}\".format(tf_path))\n    # Load weights from TF model\n    init_vars = tf.train.list_variables(tf_path)\n    names = []\n    arrays = []\n    for name, shape in init_vars:\n        logger.info(\"Loading TF weight {} with shape {}\".format(name, shape))\n        array = tf.train.load_variable(tf_path, name)\n        names.append(name)\n        arrays.append(array)\n\n    for name, array in zip(names, arrays):\n        name = name.split('/')\n        # adam_v and adam_m are variables used in AdamWeightDecayOptimizer to calculated m and v\n        # which are not required for using pretrained model\n        if any(n in [\"adam_v\", \"adam_m\", \"global_step\"] for n in name):\n            logger.info(\"Skipping {}\".format(\"/\".join(name)))\n            continue\n        pointer = model\n        for m_name in name:\n            if re.fullmatch(r'[A-Za-z]+_\\d+', m_name):\n                l = re.split(r'_(\\d+)', m_name)\n            else:\n                l = [m_name]\n            if l[0] == 'kernel' or l[0] == 'gamma':\n                pointer = getattr(pointer, 'weight')\n            elif l[0] == 'output_bias' or l[0] == 'beta':\n                pointer = getattr(pointer, 'bias')\n            elif l[0] == 'output_weights':\n                pointer = getattr(pointer, 'weight')\n            elif l[0] == 'squad':\n                pointer = getattr(pointer, 'classifier')\n            else:\n                try:\n                    pointer = getattr(pointer, l[0])\n                except AttributeError:\n                    logger.info(\"Skipping {}\".format(\"/\".join(name)))\n                    continue\n            if len(l) >= 2:\n                num = int(l[1])\n                pointer = pointer[num]\n        if m_name[-11:] == '_embeddings':\n            pointer = getattr(pointer, 'weight')\n        elif m_name == 'kernel':\n            array = np.transpose(array)\n        try:\n            assert pointer.shape == array.shape\n        except AssertionError as e:\n            e.args += (pointer.shape, array.shape)\n            raise\n        logger.info(\"Initialize PyTorch weight {}\".format(name))\n        pointer.data = torch.from_numpy(array)\n    return model", "answer": "try:\n        import re\n        import numpy as np\n        import tensorflow as tf\n    except ImportError:\n        logger.error(\"Loading a TensorFlow model in PyTorch requires TensorFlow to be installed. Please see \"\n                     \"https://www.tensorflow.org/install/ for installation instructions.\")\n        raise\n\n    tf_path = os.path.abspath(tf_checkpoint_path)\n    logger.info(\"Converting TensorFlow checkpoint from {}\".format(tf_path))\n\n    # Load weights from TF model\n    init_vars = tf.train.list_variables(tf_path)\n    names = []\n    arrays = []\n    for name, shape in init_vars:\n        logger.info(\"Loading TF weight {} with shape {}\".format(name, shape))\n        array = tf.train.load_variable(tf_path, name)\n        names.append(name)\n        arrays.append(array)\n\n    for name, array in zip(names, arrays):\n        name = name.split('/')\n        if any(n in [\"adam_v\", \"adam_m\", \"global_step\"] for n in name):\n            logger.info(\"Skipping {}\".format(\"/\".join(name)))\n            continue\n        pointer = model\n        for m_name in name:\n            if re.fullmatch(r'[A-Za-z]+\\d+', m_name):\n                l = re.split(r'(\\d+)', m_name)\n            else:\n                l = [m_name]\n            if l[0] == 'kernel' or l[0] == 'gamma':\n                pointer = getattr(pointer, 'weight')\n            elif l[0] == 'output_bias' or l[0] == 'beta':\n                pointer = getattr(pointer, 'bias')\n            elif l[0] == 'output_weights':\n                pointer = getattr(pointer, 'weight')\n            elif l[0] == 'squad':\n                pointer = getattr(pointer, 'classifier')\n            else:\n                try:\n                    pointer = getattr(pointer, l[0])\n                except AttributeError:\n                    logger.info(\"Skipping {}\".format(\"/\".join(name)))\n                    continue\n            if len(l) >= 2:\n                num = int(l[1])\n                pointer = pointer[num]\n        if m_name[-11:] == '_embeddings':\n            pointer = getattr(pointer, 'weight')\n        elif m_name == 'kernel':\n            array = np.transpose(array)\n        try:\n            assert pointer.shape == array.shape\n        except AssertionError as e:\n            e.args += (pointer.shape, array.shape)\n            raise\n        logger.info(\"Initialize PyTorch weight {}\".format(name))\n        pointer.data = torch.from_numpy(array)\n    return model", "license": "apache-2.0", "author": "Julien Chaumond <chaumond@gmail.com", "project": "gitlab.com_armbiant_pytorch-nlp", "signature": "def load_tf_weights_in_xxx(model, config, tf_checkpoint_path):", "license_type": "Permissive", "init_header": "# coding=utf-8\n# Copyright 2018 XXX Authors\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\"\"\" PyTorch XXX model. \"\"\"\n\n####################################################\n# In this template, replace all the XXX (various casings) with your model name\n####################################################\n\nfrom __future__ import absolute_import, division, print_function, unicode_literals\n\nimport json\nimport logging\nimport math\nimport os\nimport sys\nfrom io import open\n\nimport torch\nfrom torch import nn\nfrom torch.nn import CrossEntropyLoss, MSELoss\n\nfrom .modeling_utils import PreTrainedModel, prune_linear_layer\nfrom .configuration_xxx import XxxConfig\nfrom .file_utils import add_start_docstrings\n\nlogger = logging.getLogger(__name__)\n\n####################################################\n# This dict contrains shortcut names and associated url\n# for the pretrained weights provided with the models\n####################################################\nXXX_PRETRAINED_MODEL_ARCHIVE_MAP = {\n    'xxx-base-uncased': \"https://s3.amazonaws.com/models.huggingface.co/bert/xxx-base-uncased-pytorch_model.bin\",\n    'xxx-large-uncased': \"https://s3.amazonaws.com/models.huggingface.co/bert/xxx-large-uncased-pytorch_model.bin\",\n}\n\n####################################################\n# This is a conversion method from TF 1.0 to PyTorch\n# More details: https://medium.com/huggingface/from-tensorflow-to-pytorch-265f40ef2a28\n####################################################\ndef load_tf_weights_in_xxx(model, config, tf_checkpoint_path):\n    \"\"\" Load tf checkpoints in a pytorch model.\n    \"\"\"", "docstring": "    \"\"\" Load tf checkpoints in a pytorch model.\n    \"\"\""}, "dc6235b4da79859c3e7e561e944c8a93f76db0bb": {"bleu": 0.8681494042215397, "jaccard": 0.56640625, "edit_distance": 0.9172413793103449, "comment_cnt": 7, "max_sim": 0.9172413793103449, "header": "\n\nimport numpy as np\nfrom utils.nms_wrapper import apply_nms_to_test_set_results\n\ndef computeAveragePrecision(recalls, precisions, use_07_metric=False):\n    '''\n    Computes VOC AP given precision and recall.\n    '''", "body": "    if use_07_metric:\n        # 11 point metric\n        ap = 0.\n        for t in np.arange(0., 1.1, 0.1):\n            if np.sum(recalls >= t) == 0:\n                p = 0\n            else:\n                p = np.max(precisions[recalls >= t])\n            ap = ap + p / 11.\n    else:\n        # correct AP calculation\n        # first append sentinel values at the end\n        mrecalls = np.concatenate(([0.], recalls, [1.]))\n        mprecisions = np.concatenate(([0.], precisions, [0.]))\n\n        # compute the precision envelope\n        for i in range(mprecisions.size - 1, 0, -1):\n            mprecisions[i - 1] = np.maximum(mprecisions[i - 1], mprecisions[i])\n\n        # to calculate area under PR curve, look for points\n        # where X axis (recall) changes value\n        i = np.where(mrecalls[1:] != mrecalls[:-1])[0]\n\n        # and sum (\\Delta recall) * prec\n        ap = np.sum((mrecalls[i + 1] - mrecalls[i]) * mprecisions[i + 1])\n    return ap", "answer": "if use_07_metric:\n        # 11 point metric\n        ap = 0.0\n        for t in np.arange(0., 1.1, 0.1):\n            if np.sum(recalls >= t) == 0:\n                p = 0\n            else:\n                p = np.max(precisions[recalls >= t])\n            ap = ap + p / 11.\n    else:\n        # correct AP calculation\n        # first append sentinel values at the end\n        mrec = np.concatenate(([0.], recalls, [1.]))\n        mpre = np.concatenate(([0.], precisions, [0.]))\n\n        # compute the precision envelope\n        for i in range(mpre.size - 1, 0, -1):\n            mpre[i - 1] = np.maximum(mpre[i - 1], mpre[i])\n\n        # to calculate area under PR curve, look for points\n        # where X axis (recall) changes value\n        i = np.where(mrec[1:] != mrec[:-1])[0]\n\n        # and sum (\\Delta recall) * prec\n        ap = np.sum((mrec[i + 1] - mrec[i]) * mpre[i + 1])\n\n    return ap", "license": "mit", "author": "Philipp Kranen <pkranen@microsoft.com", "project": "gitlab.com_inverseai_Microsoft/CNTK", "signature": "def computeAveragePrecision(recalls, precisions, use_07_metric=False):", "license_type": "Permissive", "init_header": "# Copyright (c) Microsoft. All rights reserved.\n\n# Licensed under the MIT license. See LICENSE.md file in the project root\n# for full license information.\n# ==============================================================================\n\nimport numpy as np\nfrom utils.nms_wrapper import apply_nms_to_test_set_results\n\ndef computeAveragePrecision(recalls, precisions, use_07_metric=False):\n    '''\n    Computes VOC AP given precision and recall.\n    '''", "docstring": "    '''\n    Computes VOC AP given precision and recall.\n    '''"}, "0b83cf315f51ed8290623fa1d6e0c86c46c84bc6": {"bleu": 0.9759625015098338, "jaccard": 0.86328125, "edit_distance": 0.8496732026143791, "comment_cnt": 1, "max_sim": 0.9759625015098338, "header": "\n\nimport os\nimport re\nimport subprocess\nimport sys\n\nimport progressbar\nfrom taskpool import TaskPool, get_cpu_count\n\n\ndef _relpath(path, start=None):\n    # Backported from Python 3.1 posixpath.py\n\n    \"\"\"Return a relative version of a path\"\"\"", "body": "\n    if not path:\n        raise ValueError(\"no path specified\")\n\n    if start is None:\n        start = os.curdir\n\n    start_list = os.path.abspath(start).split(os.sep)\n    path_list = os.path.abspath(path).split(os.sep)\n\n    # Work out how much of the filepath is shared by start and path.\n    i = len(os.path.commonprefix([start_list, path_list]))\n\n    rel_list = [os.pardir] * (len(start_list)-i) + path_list[i:]\n    if not rel_list:\n        return os.curdir\n    return os.path.join(*rel_list)", "answer": "if start is None:\n        start = os.curdir\n\n    if not path:\n        raise ValueError(\"no path specified\")\n\n    start_list = os.path.abspath(start).split(os.sep)\n    path_list = os.path.abspath(path).split(os.sep)\n\n    # Work out how much of the filepath is shared by start and path.\n    i = len(os.path.commonprefix([start_list, path_list]))\n\n    rel_list = [os.pardir] * (len(start_list) - i) + path_list[i:]\n    if not rel_list:\n        return os.curdir\n    return os.path.join(*rel_list)", "license": "mpl-2.0", "author": "Jeff Walden <jwalden@mit.edu", "project": "mozilla_gecko-dev", "signature": "def _relpath(path, start=None):", "license_type": "Weak Copyleft", "init_header": "#!/usr/bin/env python\n# This Source Code Form is subject to the terms of the Mozilla Public\n# License, v. 2.0. If a copy of the MPL was not distributed with this\n# file, You can obtain one at http://mozilla.org/MPL/2.0/.\n\n# run-tests.py -- Python harness for GDB SpiderMonkey support\n\nimport os\nimport re\nimport subprocess\nimport sys\n\n# From this directory:\nimport progressbar\nfrom taskpool import TaskPool, get_cpu_count\n\n\ndef _relpath(path, start=None):\n    # Backported from Python 3.1 posixpath.py\n\n    \"\"\"Return a relative version of a path\"\"\"", "docstring": "    # Backported from Python 3.1 posixpath.py\n\n    \"\"\"Return a relative version of a path\"\"\""}, "b3bdb65a5b5efe062364b4768434818a5f4fc5d1": {"bleu": 0.647316446172771, "jaccard": 0.39453125, "edit_distance": 0.5748502994011976, "comment_cnt": 4, "max_sim": 0.647316446172771, "header": "\nimport matplotlib\nmatplotlib.use('Agg')\n\nimport its.error\nimport pylab\nimport sys\nimport Image\nimport numpy\nimport math\nimport unittest\nimport cStringIO\nimport scipy.stats\nimport copy\n\nDEFAULT_YUV_TO_RGB_CCM = numpy.matrix([\n                                [1.000,  0.000,  1.402],\n                                [1.000, -0.344, -0.714],\n                                [1.000,  1.772,  0.000]])\n\nDEFAULT_YUV_OFFSETS = numpy.array([0, 128, 128])\n\nDEFAULT_GAMMA_LUT = numpy.array(\n        [math.floor(65535 * math.pow(i/65535.0, 1/2.2) + 0.5)\n         for i in xrange(65536)])\n\nDEFAULT_INVGAMMA_LUT = numpy.array(\n        [math.floor(65535 * math.pow(i/65535.0, 2.2) + 0.5)\n         for i in xrange(65536)])\n\nMAX_LUT_SIZE = 65536\n\ndef get_canonical_cfa_order(props):\n    \"\"\"Returns a mapping from the Bayer 2x2 top-left grid in the CFA to\n    the standard order R,Gr,Gb,B.\n\n    Args:\n        props: Camera properties object.\n\n    Returns:\n        List of 4 integers, corresponding to the positions in the 2x2 top-\n            left Bayer grid of R,Gr,Gb,B, where the 2x2 grid is labeled as\n            0,1,2,3 in row major order.\n    \"\"\"", "body": "    # Note that raw streams aren't croppable, so the cropRegion doesn't need\n    # to be considered when determining the top-left pixel color.\n    cfa_pat = props['android.sensor.info.colorFilterArrangement']\n    if cfa_pat == 0:\n        # RGGB\n        return [0,1,2,3]\n    elif cfa_pat == 1:\n        # GRBG\n        return [1,0,3,2]\n    elif cfa_pat == 2:\n        # GBRG\n        return [2,3,0,1]\n    elif cfa_pat == 3:\n        # BGGR\n        return [3,2,1,0]\n    else:\n        raise its.error.Error(\"Not supported\")", "answer": "cfa_pattern = props['android.sensor.info.colorFilterArrangement']\n    if cfa_pattern == 0:  # RGGB\n        return [0, 1, 2, 3]\n    elif cfa_pattern == 1:  # GRBG\n        return [1, 0, 3, 2]\n    elif cfa_pattern == 2:  # GBRG\n        return [2, 3, 0, 1]\n    elif cfa_pattern == 3:  # BGGR\n        return [3, 2, 1, 0]\n    else:\n        raise its.error.Error('Unknown CFA pattern: %d' % cfa_pattern)", "license": "apache-2.0", "author": "Timothy Knight <tknight@google.com", "project": "gitlab.com_quicla_platform/cts", "signature": "def get_canonical_cfa_order(props):", "license_type": "Permissive", "init_header": "# Copyright 2013 The Android Open Source Project\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport matplotlib\nmatplotlib.use('Agg')\n\nimport its.error\nimport pylab\nimport sys\nimport Image\nimport numpy\nimport math\nimport unittest\nimport cStringIO\nimport scipy.stats\nimport copy\n\nDEFAULT_YUV_TO_RGB_CCM = numpy.matrix([\n                                [1.000,  0.000,  1.402],\n                                [1.000, -0.344, -0.714],\n                                [1.000,  1.772,  0.000]])\n\nDEFAULT_YUV_OFFSETS = numpy.array([0, 128, 128])\n\nDEFAULT_GAMMA_LUT = numpy.array(\n        [math.floor(65535 * math.pow(i/65535.0, 1/2.2) + 0.5)\n         for i in xrange(65536)])\n\nDEFAULT_INVGAMMA_LUT = numpy.array(\n        [math.floor(65535 * math.pow(i/65535.0, 2.2) + 0.5)\n         for i in xrange(65536)])\n\nMAX_LUT_SIZE = 65536\n\ndef get_canonical_cfa_order(props):\n    \"\"\"Returns a mapping from the Bayer 2x2 top-left grid in the CFA to\n    the standard order R,Gr,Gb,B.\n\n    Args:\n        props: Camera properties object.\n\n    Returns:\n        List of 4 integers, corresponding to the positions in the 2x2 top-\n            left Bayer grid of R,Gr,Gb,B, where the 2x2 grid is labeled as\n            0,1,2,3 in row major order.\n    \"\"\"", "docstring": "    \"\"\"Returns a mapping from the Bayer 2x2 top-left grid in the CFA to\n    the standard order R,Gr,Gb,B.\n\n    Args:\n        props: Camera properties object.\n\n    Returns:\n        List of 4 integers, corresponding to the positions in the 2x2 top-\n            left Bayer grid of R,Gr,Gb,B, where the 2x2 grid is labeled as\n            0,1,2,3 in row major order.\n    \"\"\""}, "798ae8c02d0df4d592a4302568c93e650ef5df48": {"bleu": 0.6712691181683543, "jaccard": 0.51171875, "edit_distance": 0.6640378548895899, "comment_cnt": 8, "max_sim": 0.6712691181683543, "header": "\"\"\"K-means clustering\"\"\"\n\n\nimport warnings\n\nimport numpy as np\nimport scipy.sparse as sp\n\nfrom ..base import BaseEstimator, ClusterMixin, TransformerMixin\nfrom ..metrics.pairwise import euclidean_distances\nfrom ..utils.sparsefuncs import assign_rows_csr, mean_variance_axis0\nfrom ..utils import check_arrays\nfrom ..utils import check_random_state\nfrom ..utils import atleast2d_or_csr\nfrom ..utils import as_float_array\nfrom ..externals.joblib import Parallel\nfrom ..externals.joblib import delayed\n\nfrom . import _k_means\n\n\n\n\ndef _k_init(X, n_clusters, n_local_trials=None, random_state=None,\n            x_squared_norms=None):\n    \"\"\"Init n_clusters seeds according to k-means++\n\n    Parameters\n    -----------\n    X: array or sparse matrix, shape (n_samples, n_features)\n        The data to pick seeds for. To avoid memory copy, the input data\n        should be double precision (dtype=np.float64).\n\n    n_clusters: integer\n        The number of seeds to choose\n\n    n_local_trials: integer, optional\n        The number of seeding trials for each center (except the first),\n        of which the one reducing inertia the most is greedily chosen.\n        Set to None to make the number of trials depend logarithmically\n        on the number of seeds (2+log(k)); this is the default.\n\n    random_state: integer or numpy.RandomState, optional\n        The generator used to initialize the centers. If an integer is\n        given, it fixes the seed. Defaults to the global numpy random\n        number generator.\n\n    x_squared_norms: array, shape (n_samples,), optional\n        Squared euclidean norm of each data point. Pass it if you have it at\n        hands already to avoid it being recomputed here. Default: None\n\n    Notes\n    -----\n    Selects initial cluster centers for k-mean clustering in a smart way\n    to speed up convergence. see: Arthur, D. and Vassilvitskii, S.\n    \"k-means++: the advantages of careful seeding\". ACM-SIAM symposium\n    on Discrete algorithms. 2007\n\n    Version ported from http://www.stanford.edu/~darthur/kMeansppTest.zip,\n    which is the implementation used in the aforementioned paper.\n    \"\"\"", "body": "    n_samples, n_features = X.shape\n    random_state = check_random_state(random_state)\n\n    centers = np.empty((n_clusters, n_features))\n\n    # Set the number of local seeding trials if none is given\n    if n_local_trials is None:\n        # This is what Arthur/Vassilvitskii tried, but did not report\n        # specific results for other than mentioning in the conclusion\n        # that it helped.\n        n_local_trials = 2 + int(np.log(n_clusters))\n\n    # Pick first center randomly\n    center_id = random_state.randint(n_samples)\n    if sp.issparse(X):\n        centers[0] = X[center_id].toarray()\n    else:\n        centers[0] = X[center_id]\n\n    # Initialize list of closest distances and calculate current potential\n    if x_squared_norms is None:\n        x_squared_norms = _squared_norms(X)\n    closest_dist_sq = euclidean_distances(\n        centers[0], X, Y_norm_squared=x_squared_norms, squared=True)\n    current_pot = closest_dist_sq.sum()\n\n    # Pick the remaining n_clusters-1 points\n    for c in range(1, n_clusters):\n        # Choose center candidates by sampling with probability proportional\n        # to the squared distance to the closest existing center\n        rand_vals = random_state.random_sample(n_local_trials) * current_pot\n        candidate_ids = np.searchsorted(closest_dist_sq.cumsum(), rand_vals)\n\n        # Compute distances to center candidates\n        distance_to_candidates = euclidean_distances(\n            X[candidate_ids], X, Y_norm_squared=x_squared_norms, squared=True)\n\n        # Decide which candidate is the best\n        best_candidate = None\n        best_pot = None\n        best_dist_sq = None\n        for trial in range(n_local_trials):\n            # Compute potential when including center candidate\n            new_dist_sq = np.minimum(closest_dist_sq,\n                                     distance_to_candidates[trial])\n            new_pot = new_dist_sq.sum()\n\n            # Store result if it is the best local trial so far\n            if (best_candidate is None) or (new_pot < best_pot):\n                best_candidate = candidate_ids[trial]\n                best_pot = new_pot\n                best_dist_sq = new_dist_sq\n\n        # Permanently add best center candidate found in local tries\n        if sp.issparse(X):\n            centers[c] = X[best_candidate].toarray()\n        else:\n            centers[c] = X[best_candidate]\n        current_pot = best_pot\n        closest_dist_sq = best_dist_sq\n\n    return centers", "answer": "n_samples, n_features = X.shape\n\n    if x_squared_norms is None:\n        x_squared_norms = np.einsum('ij,ij->i', X, X)\n\n    centers = np.empty((n_clusters, n_features), dtype=X.dtype)\n\n    # Set the number of local seeding trials if none is given\n    if n_local_trials is None:\n        n_local_trials = 2 + int(np.log(n_clusters))\n\n    # Pick first center randomly\n    random_state = check_random_state(random_state)\n    center_id = random_state.randint(n_samples)\n    centers[0] = X[center_id]\n\n    # Initialize list of closest distances and calculate current potential\n    closest_dist_sq = euclidean_distances(\n        centers[0, np.newaxis], X, Y_norm_squared=x_squared_norms,\n        squared=True)\n    current_pot = closest_dist_sq.sum()\n\n    # Pick the remaining n_clusters-1 points\n    for c in range(1, n_clusters):\n        # Choose center candidates by sampling with probability proportional\n        # to the squared distance to the closest existing center\n        rand_vals = random_state.random_sample(n_local_trials) * current_pot\n        candidate_ids = np.searchsorted(np.cumsum(closest_dist_sq), rand_vals)\n\n        # Compute distances to center candidates\n        distance_to_candidates = euclidean_distances(\n            X[candidate_ids], X, Y_norm_squared=x_squared_norms, squared=True)\n\n        # Decide which candidate is the best\n        best_candidate = None\n        best_pot = None\n        best_dist_sq = None\n        for trial in range(n_local_trials):\n            new_dist_sq = np.minimum(closest_dist_sq, distance_to_candidates[trial])\n            new_pot = new_dist_sq.sum()\n\n            if (best_candidate is None) or (new_pot < best_pot):\n                best_candidate = candidate_ids[trial]\n                best_pot = new_pot\n                best_dist_sq = new_dist_sq\n\n        centers[c] = X[best_candidate]\n        current_pot = best_pot\n        closest_dist_sq = best_dist_sq\n\n    return centers", "license": "bsd-3-clause", "author": "Lars Buitinck <L.J.Buitinck@uva.nl", "project": "scikit-learn_scikit-learn", "signature": "def _k_init(X, n_clusters, n_local_trials=None, random_state=None,", "license_type": "Permissive", "init_header": "\"\"\"K-means clustering\"\"\"\n\n# Authors: Gael Varoquaux <gael.varoquaux@normalesup.org>\n#          Thomas Rueckstiess <ruecksti@in.tum.de>\n#          James Bergstra <james.bergstra@umontreal.ca>\n#          Jan Schlueter <scikit-learn@jan-schlueter.de>\n#          Nelle Varoquaux\n#          Peter Prettenhofer <peter.prettenhofer@gmail.com>\n#          Olivier Grisel <olivier.grisel@ensta.org>\n#          Mathieu Blondel <mathieu@mblondel.org>\n#          Robert Layton <robertlayton@gmail.com>\n# License: BSD 3 clause\n\nimport warnings\n\nimport numpy as np\nimport scipy.sparse as sp\n\nfrom ..base import BaseEstimator, ClusterMixin, TransformerMixin\nfrom ..metrics.pairwise import euclidean_distances\nfrom ..utils.sparsefuncs import assign_rows_csr, mean_variance_axis0\nfrom ..utils import check_arrays\nfrom ..utils import check_random_state\nfrom ..utils import atleast2d_or_csr\nfrom ..utils import as_float_array\nfrom ..externals.joblib import Parallel\nfrom ..externals.joblib import delayed\n\nfrom . import _k_means\n\n\n###############################################################################\n# Initialization heuristic\n\n\ndef _k_init(X, n_clusters, n_local_trials=None, random_state=None,\n            x_squared_norms=None):\n    \"\"\"Init n_clusters seeds according to k-means++\n\n    Parameters\n    -----------\n    X: array or sparse matrix, shape (n_samples, n_features)\n        The data to pick seeds for. To avoid memory copy, the input data\n        should be double precision (dtype=np.float64).\n\n    n_clusters: integer\n        The number of seeds to choose\n\n    n_local_trials: integer, optional\n        The number of seeding trials for each center (except the first),\n        of which the one reducing inertia the most is greedily chosen.\n        Set to None to make the number of trials depend logarithmically\n        on the number of seeds (2+log(k)); this is the default.\n\n    random_state: integer or numpy.RandomState, optional\n        The generator used to initialize the centers. If an integer is\n        given, it fixes the seed. Defaults to the global numpy random\n        number generator.\n\n    x_squared_norms: array, shape (n_samples,), optional\n        Squared euclidean norm of each data point. Pass it if you have it at\n        hands already to avoid it being recomputed here. Default: None\n\n    Notes\n    -----\n    Selects initial cluster centers for k-mean clustering in a smart way\n    to speed up convergence. see: Arthur, D. and Vassilvitskii, S.\n    \"k-means++: the advantages of careful seeding\". ACM-SIAM symposium\n    on Discrete algorithms. 2007\n\n    Version ported from http://www.stanford.edu/~darthur/kMeansppTest.zip,\n    which is the implementation used in the aforementioned paper.\n    \"\"\"", "docstring": "            x_squared_norms=None):\n    \"\"\"Init n_clusters seeds according to k-means++\n\n    Parameters\n    -----------\n    X: array or sparse matrix, shape (n_samples, n_features)\n        The data to pick seeds for. To avoid memory copy, the input data\n        should be double precision (dtype=np.float64).\n\n    n_clusters: integer\n        The number of seeds to choose\n\n    n_local_trials: integer, optional\n        The number of seeding trials for each center (except the first),\n        of which the one reducing inertia the most is greedily chosen.\n        Set to None to make the number of trials depend logarithmically\n        on the number of seeds (2+log(k)); this is the default.\n\n    random_state: integer or numpy.RandomState, optional\n        The generator used to initialize the centers. If an integer is\n        given, it fixes the seed. Defaults to the global numpy random\n        number generator.\n\n    x_squared_norms: array, shape (n_samples,), optional\n        Squared euclidean norm of each data point. Pass it if you have it at\n        hands already to avoid it being recomputed here. Default: None\n\n    Notes\n    -----\n    Selects initial cluster centers for k-mean clustering in a smart way\n    to speed up convergence. see: Arthur, D. and Vassilvitskii, S.\n    \"k-means++: the advantages of careful seeding\". ACM-SIAM symposium\n    on Discrete algorithms. 2007\n\n    Version ported from http://www.stanford.edu/~darthur/kMeansppTest.zip,\n    which is the implementation used in the aforementioned paper.\n    \"\"\""}, "88f3fb428f896afd6cc32238c6470c3acf43f612": {"bleu": 0.7381881704385775, "jaccard": 0.53515625, "edit_distance": 0.7950617283950617, "comment_cnt": 2, "max_sim": 0.7950617283950617, "header": "\n\"\"\"Module implementing RNN Cells.\"\"\"\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport collections\nimport contextlib\nimport hashlib\nimport math\nimport numbers\n\nfrom tensorflow.python.framework import ops\nfrom tensorflow.python.framework import tensor_shape\nfrom tensorflow.python.framework import tensor_util\nfrom tensorflow.python.ops import array_ops\nfrom tensorflow.python.ops import clip_ops\nfrom tensorflow.python.ops import embedding_ops\nfrom tensorflow.python.ops import init_ops\nfrom tensorflow.python.ops import math_ops\nfrom tensorflow.python.ops import nn_ops\nfrom tensorflow.python.ops import partitioned_variables\nfrom tensorflow.python.ops import random_ops\nfrom tensorflow.python.ops import variable_scope as vs\n\nfrom tensorflow.python.ops.math_ops import sigmoid\nfrom tensorflow.python.ops.math_ops import tanh\nfrom tensorflow.python.ops.rnn_cell_impl import _RNNCell as RNNCell\n\nfrom tensorflow.python.platform import tf_logging as logging\nfrom tensorflow.python.util import nest\n\n\n_BIAS_VARIABLE_NAME = \"biases\"\n_WEIGHTS_VARIABLE_NAME = \"weights\"\n\n\n@contextlib.contextmanager\ndef _linear(args, output_size, bias, bias_start=0.0):\n  \"\"\"Linear map: sum_i(args[i] * W[i]), where W[i] is a variable.\n\n  Args:\n    args: a 2D Tensor or a list of 2D, batch x n, Tensors.\n    output_size: int, second dimension of W[i].\n    bias: boolean, whether to add a bias term or not.\n    bias_start: starting value to initialize the bias; 0 by default.\n\n  Returns:\n    A 2D Tensor with shape [batch x output_size] equal to\n    sum_i(args[i] * W[i]), where W[i]s are newly created matrices.\n\n  Raises:\n    ValueError: if some of the arguments has unspecified or wrong shape.\n  \"\"\"", "body": "  if args is None or (nest.is_sequence(args) and not args):\n    raise ValueError(\"`args` must be specified\")\n  if not nest.is_sequence(args):\n    args = [args]\n\n  # Calculate the total size of arguments on dimension 1.\n  total_arg_size = 0\n  shapes = [a.get_shape() for a in args]\n  for shape in shapes:\n    if shape.ndims != 2:\n      raise ValueError(\"linear is expecting 2D arguments: %s\" % shapes)\n    if shape[1].value is None:\n      raise ValueError(\"linear expects shape[1] to be provided for shape %s, \"\n                       \"but saw %s\" % (shape, shape[1]))\n    else:\n      total_arg_size += shape[1].value\n\n  dtype = [a.dtype for a in args][0]\n\n  # Now the computation.\n  scope = vs.get_variable_scope()\n  with vs.variable_scope(scope) as outer_scope:\n    weights = vs.get_variable(\n        _WEIGHTS_VARIABLE_NAME, [total_arg_size, output_size], dtype=dtype)\n    if len(args) == 1:\n      res = math_ops.matmul(args[0], weights)\n    else:\n      res = math_ops.matmul(array_ops.concat(args, 1), weights)\n    if not bias:\n      return res\n    with vs.variable_scope(outer_scope) as inner_scope:\n      inner_scope.set_partitioner(None)\n      biases = vs.get_variable(\n          _BIAS_VARIABLE_NAME, [output_size],\n          dtype=dtype,\n          initializer=init_ops.constant_initializer(bias_start, dtype=dtype))\n    return nn_ops.bias_add(res, biases)", "answer": "if not isinstance(args, (list, tuple)):\n    args = [args]\n\n  # Calculate the total size of arguments on dimension 1.\n  total_arg_size = 0\n  shapes = [a.get_shape().as_list() for a in args]\n  for shape in shapes:\n    if len(shape) != 2:\n      raise ValueError(\"Linear is expecting 2D arguments: %s\" % str(shapes))\n    if not shape[1]:\n      raise ValueError(\"Linear expects shape[1] of arguments: %s\" % str(shapes))\n    total_arg_size += shape[1]\n\n  dtype = [a.dtype for a in args][0]\n\n  # Now the computation.\n  scope = vs.get_variable_scope()\n  with vs.variable_scope(scope) as outer_scope:\n    weights = vs.get_variable(\n        _WEIGHTS_VARIABLE_NAME, [total_arg_size, output_size], dtype=dtype)\n    if len(args) == 1:\n      res = math_ops.matmul(args[0], weights)\n    else:\n      res = math_ops.matmul(array_ops.concat(args, 1), weights)\n    if not bias:\n      return res\n    with vs.variable_scope(outer_scope) as inner_scope:\n      inner_scope.set_partitioner(None)\n      biases = vs.get_variable(\n          _BIAS_VARIABLE_NAME, [output_size],\n          dtype=dtype,\n          initializer=init_ops.constant_initializer(bias_start, dtype=dtype))\n  return nn_ops.bias_add(res, biases)", "license": "apache-2.0", "author": "Shanqing Cai <cais@google.com", "project": "tensorflow_tensorflow", "signature": "def _linear(args, output_size, bias, bias_start=0.0):", "license_type": "Permissive", "init_header": "# Copyright 2015 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\n\"\"\"Module implementing RNN Cells.\"\"\"\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport collections\nimport contextlib\nimport hashlib\nimport math\nimport numbers\n\nfrom tensorflow.python.framework import ops\nfrom tensorflow.python.framework import tensor_shape\nfrom tensorflow.python.framework import tensor_util\nfrom tensorflow.python.ops import array_ops\nfrom tensorflow.python.ops import clip_ops\nfrom tensorflow.python.ops import embedding_ops\nfrom tensorflow.python.ops import init_ops\nfrom tensorflow.python.ops import math_ops\nfrom tensorflow.python.ops import nn_ops\nfrom tensorflow.python.ops import partitioned_variables\nfrom tensorflow.python.ops import random_ops\nfrom tensorflow.python.ops import variable_scope as vs\n\nfrom tensorflow.python.ops.math_ops import sigmoid\nfrom tensorflow.python.ops.math_ops import tanh\nfrom tensorflow.python.ops.rnn_cell_impl import _RNNCell as RNNCell\n\nfrom tensorflow.python.platform import tf_logging as logging\nfrom tensorflow.python.util import nest\n\n\n_BIAS_VARIABLE_NAME = \"biases\"\n_WEIGHTS_VARIABLE_NAME = \"weights\"\n\n\n@contextlib.contextmanager\ndef _linear(args, output_size, bias, bias_start=0.0):\n  \"\"\"Linear map: sum_i(args[i] * W[i]), where W[i] is a variable.\n\n  Args:\n    args: a 2D Tensor or a list of 2D, batch x n, Tensors.\n    output_size: int, second dimension of W[i].\n    bias: boolean, whether to add a bias term or not.\n    bias_start: starting value to initialize the bias; 0 by default.\n\n  Returns:\n    A 2D Tensor with shape [batch x output_size] equal to\n    sum_i(args[i] * W[i]), where W[i]s are newly created matrices.\n\n  Raises:\n    ValueError: if some of the arguments has unspecified or wrong shape.\n  \"\"\"", "docstring": "  \"\"\"Linear map: sum_i(args[i] * W[i]), where W[i] is a variable.\n\n  Args:\n    args: a 2D Tensor or a list of 2D, batch x n, Tensors.\n    output_size: int, second dimension of W[i].\n    bias: boolean, whether to add a bias term or not.\n    bias_start: starting value to initialize the bias; 0 by default.\n\n  Returns:\n    A 2D Tensor with shape [batch x output_size] equal to\n    sum_i(args[i] * W[i]), where W[i]s are newly created matrices.\n\n  Raises:\n    ValueError: if some of the arguments has unspecified or wrong shape.\n  \"\"\""}, "5cb44df0f70fc98759b71441e9c492cb64f7e018": {"bleu": 0.5810967700832709, "jaccard": 0.53515625, "edit_distance": 0.6342105263157894, "comment_cnt": 2, "max_sim": 0.6342105263157894, "header": "\"\"\"\nGenerate samples of synthetic data sets.\n\"\"\"\n\n\nimport numbers\nimport warnings\nimport numpy as np\nfrom scipy import linalg\n\nfrom ..preprocessing import MultiLabelBinarizer\nfrom ..utils import array2d, check_random_state\nfrom ..utils import shuffle as util_shuffle\nfrom ..utils.fixes import astype\nfrom ..utils.random import sample_without_replacement\nfrom ..externals import six\nmap = six.moves.map\nzip = six.moves.zip\n\n\ndef make_regression(n_samples=100, n_features=100, n_informative=10,\n                    n_targets=1, bias=0.0, effective_rank=None,\n                    tail_strength=0.5, noise=0.0, shuffle=True, coef=False,\n                    random_state=None):\n    \"\"\"Generate a random regression problem.\n\n    The input set can either be well conditioned (by default) or have a low\n    rank-fat tail singular profile. See the `make_low_rank_matrix` for\n    more details.\n\n    The output is generated by applying a (potentially biased) random linear\n    regression model with `n_informative` nonzero regressors to the previously\n    generated input and some gaussian centered noise with some adjustable\n    scale.\n\n    Parameters\n    ----------\n    n_samples : int, optional (default=100)\n        The number of samples.\n\n    n_features : int, optional (default=100)\n        The number of features.\n\n    n_informative : int, optional (default=10)\n        The number of informative features, i.e., the number of features used\n        to build the linear model used to generate the output.\n\n    n_targets : int, optional (default=1)\n        The number of regression targets, i.e., the dimension of the y output\n        vector associated with a sample. By default, the output is a scalar.\n\n    bias : float, optional (default=0.0)\n        The bias term in the underlying linear model.\n\n    effective_rank : int or None, optional (default=None)\n        if not None:\n            The approximate number of singular vectors required to explain most\n            of the input data by linear combinations. Using this kind of\n            singular spectrum in the input allows the generator to reproduce\n            the correlations often observed in practice.\n        if None:\n            The input set is well conditioned, centered and gaussian with\n            unit variance.\n\n    tail_strength : float between 0.0 and 1.0, optional (default=0.5)\n        The relative importance of the fat noisy tail of the singular values\n        profile if `effective_rank` is not None.\n\n    noise : float, optional (default=0.0)\n        The standard deviation of the gaussian noise applied to the output.\n\n    shuffle : boolean, optional (default=True)\n        Shuffle the samples and the features.\n\n    coef : boolean, optional (default=False)\n        If True, the coefficients of the underlying linear model are returned.\n\n    random_state : int, RandomState instance or None, optional (default=None)\n        If int, random_state is the seed used by the random number generator;\n        If RandomState instance, random_state is the random number generator;\n        If None, the random number generator is the RandomState instance used\n        by `np.random`.\n\n    Returns\n    -------\n    X : array of shape [n_samples, n_features]\n        The input samples.\n\n    y : array of shape [n_samples] or [n_samples, n_targets]\n        The output values.\n\n    coef : array of shape [n_features] or [n_features, n_targets], optional\n        The coefficient of the underlying linear model. It is returned only if\n        coef is True.\n    \"\"\"", "body": "    n_informative = min(n_features, n_informative)\n    generator = check_random_state(random_state)\n\n    if effective_rank is None:\n        # Randomly generate a well conditioned input set\n        X = generator.randn(n_samples, n_features)\n\n    else:\n        # Randomly generate a low rank, fat tail input set\n        X = make_low_rank_matrix(n_samples=n_samples,\n                                 n_features=n_features,\n                                 effective_rank=effective_rank,\n                                 tail_strength=tail_strength,\n                                 random_state=generator)\n\n    # Generate a ground truth model with only n_informative features being non\n    # zeros (the other features are not correlated to y and should be ignored\n    # by a sparsifying regularizers such as L1 or elastic net)\n    ground_truth = np.zeros((n_features, n_targets))\n    ground_truth[:n_informative, :] = 100 * generator.rand(n_informative,\n                                                           n_targets)\n\n    y = np.dot(X, ground_truth) + bias\n\n    # Add noise\n    if noise > 0.0:\n        y += generator.normal(scale=noise, size=y.shape)\n\n    # Randomly permute samples and features\n    if shuffle:\n        X, y = util_shuffle(X, y, random_state=generator)\n\n        indices = np.arange(n_features)\n        generator.shuffle(indices)\n        X[:, :] = X[:, indices]\n        ground_truth = ground_truth[indices]\n\n    y = np.squeeze(y)\n\n    if coef:\n        return X, y, np.squeeze(ground_truth)\n\n    else:\n        return X, y", "answer": "generator = check_random_state(random_state)\n\n    if effective_rank is None:\n        # Randomly generate a well conditioned input set\n        X = generator.randn(n_samples, n_features)\n    else:\n        # Randomly generate a low rank, fat tail input set\n        X = make_low_rank_matrix(n_samples=n_samples, n_features=n_features,\n                                 effective_rank=effective_rank,\n                                 tail_strength=tail_strength,\n                                 random_state=generator)\n\n    # Generate the coefficients\n    ground_truth = np.zeros((n_features, n_targets))\n    ground_truth[:n_informative, :] = 100 * generator.rand(n_informative, n_targets)\n\n    y = np.dot(X, ground_truth) + bias\n\n    if noise > 0.0:\n        y += generator.normal(scale=noise, size=y.shape)\n\n    if n_targets == 1:\n        y = np.ravel(y)\n\n    if shuffle:\n        X, y = util_shuffle(X, y, random_state=generator)\n\n    if coef:\n        return X, y, np.squeeze(ground_truth)\n\n    return X, y", "license": "bsd-3-clause", "author": "Joel Nothman <joel.nothman@gmail.com", "project": "scikit-learn_scikit-learn", "signature": "def make_regression(n_samples=100, n_features=100, n_informative=10,", "license_type": "Permissive", "init_header": "\"\"\"\nGenerate samples of synthetic data sets.\n\"\"\"\n\n# Authors: B. Thirion, G. Varoquaux, A. Gramfort, V. Michel, O. Grisel,\n#          G. Louppe, J. Nothman\n# License: BSD 3 clause\n\nimport numbers\nimport warnings\nimport numpy as np\nfrom scipy import linalg\n\nfrom ..preprocessing import MultiLabelBinarizer\nfrom ..utils import array2d, check_random_state\nfrom ..utils import shuffle as util_shuffle\nfrom ..utils.fixes import astype\nfrom ..utils.random import sample_without_replacement\nfrom ..externals import six\nmap = six.moves.map\nzip = six.moves.zip\n\n\ndef make_regression(n_samples=100, n_features=100, n_informative=10,\n                    n_targets=1, bias=0.0, effective_rank=None,\n                    tail_strength=0.5, noise=0.0, shuffle=True, coef=False,\n                    random_state=None):\n    \"\"\"Generate a random regression problem.\n\n    The input set can either be well conditioned (by default) or have a low\n    rank-fat tail singular profile. See the `make_low_rank_matrix` for\n    more details.\n\n    The output is generated by applying a (potentially biased) random linear\n    regression model with `n_informative` nonzero regressors to the previously\n    generated input and some gaussian centered noise with some adjustable\n    scale.\n\n    Parameters\n    ----------\n    n_samples : int, optional (default=100)\n        The number of samples.\n\n    n_features : int, optional (default=100)\n        The number of features.\n\n    n_informative : int, optional (default=10)\n        The number of informative features, i.e., the number of features used\n        to build the linear model used to generate the output.\n\n    n_targets : int, optional (default=1)\n        The number of regression targets, i.e., the dimension of the y output\n        vector associated with a sample. By default, the output is a scalar.\n\n    bias : float, optional (default=0.0)\n        The bias term in the underlying linear model.\n\n    effective_rank : int or None, optional (default=None)\n        if not None:\n            The approximate number of singular vectors required to explain most\n            of the input data by linear combinations. Using this kind of\n            singular spectrum in the input allows the generator to reproduce\n            the correlations often observed in practice.\n        if None:\n            The input set is well conditioned, centered and gaussian with\n            unit variance.\n\n    tail_strength : float between 0.0 and 1.0, optional (default=0.5)\n        The relative importance of the fat noisy tail of the singular values\n        profile if `effective_rank` is not None.\n\n    noise : float, optional (default=0.0)\n        The standard deviation of the gaussian noise applied to the output.\n\n    shuffle : boolean, optional (default=True)\n        Shuffle the samples and the features.\n\n    coef : boolean, optional (default=False)\n        If True, the coefficients of the underlying linear model are returned.\n\n    random_state : int, RandomState instance or None, optional (default=None)\n        If int, random_state is the seed used by the random number generator;\n        If RandomState instance, random_state is the random number generator;\n        If None, the random number generator is the RandomState instance used\n        by `np.random`.\n\n    Returns\n    -------\n    X : array of shape [n_samples, n_features]\n        The input samples.\n\n    y : array of shape [n_samples] or [n_samples, n_targets]\n        The output values.\n\n    coef : array of shape [n_features] or [n_features, n_targets], optional\n        The coefficient of the underlying linear model. It is returned only if\n        coef is True.\n    \"\"\"", "docstring": "                    n_targets=1, bias=0.0, effective_rank=None,\n                    tail_strength=0.5, noise=0.0, shuffle=True, coef=False,\n                    random_state=None):\n    \"\"\"Generate a random regression problem.\n\n    The input set can either be well conditioned (by default) or have a low\n    rank-fat tail singular profile. See the `make_low_rank_matrix` for\n    more details.\n\n    The output is generated by applying a (potentially biased) random linear\n    regression model with `n_informative` nonzero regressors to the previously\n    generated input and some gaussian centered noise with some adjustable\n    scale.\n\n    Parameters\n    ----------\n    n_samples : int, optional (default=100)\n        The number of samples.\n\n    n_features : int, optional (default=100)\n        The number of features.\n\n    n_informative : int, optional (default=10)\n        The number of informative features, i.e., the number of features used\n        to build the linear model used to generate the output.\n\n    n_targets : int, optional (default=1)\n        The number of regression targets, i.e., the dimension of the y output\n        vector associated with a sample. By default, the output is a scalar.\n\n    bias : float, optional (default=0.0)\n        The bias term in the underlying linear model.\n\n    effective_rank : int or None, optional (default=None)\n        if not None:\n            The approximate number of singular vectors required to explain most\n            of the input data by linear combinations. Using this kind of\n            singular spectrum in the input allows the generator to reproduce\n            the correlations often observed in practice.\n        if None:\n            The input set is well conditioned, centered and gaussian with\n            unit variance.\n\n    tail_strength : float between 0.0 and 1.0, optional (default=0.5)\n        The relative importance of the fat noisy tail of the singular values\n        profile if `effective_rank` is not None.\n\n    noise : float, optional (default=0.0)\n        The standard deviation of the gaussian noise applied to the output.\n\n    shuffle : boolean, optional (default=True)\n        Shuffle the samples and the features.\n\n    coef : boolean, optional (default=False)\n        If True, the coefficients of the underlying linear model are returned.\n\n    random_state : int, RandomState instance or None, optional (default=None)\n        If int, random_state is the seed used by the random number generator;\n        If RandomState instance, random_state is the random number generator;\n        If None, the random number generator is the RandomState instance used\n        by `np.random`.\n\n    Returns\n    -------\n    X : array of shape [n_samples, n_features]\n        The input samples.\n\n    y : array of shape [n_samples] or [n_samples, n_targets]\n        The output values.\n\n    coef : array of shape [n_features] or [n_features, n_targets], optional\n        The coefficient of the underlying linear model. It is returned only if\n        coef is True.\n    \"\"\""}, "9d8debef7cbc172bcabc3c46828a228fae35386b": {"bleu": 0.704818328078987, "jaccard": 0.54296875, "edit_distance": 0.6885245901639344, "comment_cnt": 8, "max_sim": 0.704818328078987, "header": "\nfrom ..ch09.pq import HeapPriorityQueue,AdaptableHeapPriorityQueue\nfrom .partition import Partition\n\ndef MST_PrimJarnik(g):\n  \"\"\"Compute a minimum spanning tree of weighted graph g.\n\n  Return a list of edges that comprise the MST (in arbitrary order).\n  \"\"\"", "body": "  d = {}                               # d[v] is bound on distance to tree\n  tree = []                            # list of edges in spanning tree\n  pq = AdaptableHeapPriorityQueue()   # d[v] maps to value (v, e=(u,v))\n  pqlocator = {}                       # map from vertex to its pq locator\n\n  # for each vertex v of the graph, add an entry to the priority queue, with\n  # the source having distance 0 and all others having infinite distance\n  for v in g.vertices():\n    if len(d) == 0:                                 # this is the first node\n      d[v] = 0                                      # make it the root\n    else:\n      d[v] = float('inf')                           # positive infinity\n    pqlocator[v] = pq.add(d[v], (v,None))\n\n  while not pq.is_empty():\n    key,value = pq.remove_min()\n    u,edge = value                                  # unpack tuple from pq\n    del pqlocator[u]                                # u is no longer in pq\n    if edge is not None:\n      tree.append(edge)                             # add edge to tree\n    for link in g.incident_edges(u):\n      v = link.opposite(u)\n      if v in pqlocator:                            # thus v not yet in tree\n        # see if edge (u,v) better connects v to the growing tree\n        wgt = link.element()\n        if wgt < d[v]:                              # better edge to v?\n          d[v] = wgt                                # update the distance\n          pq.update(pqlocator[v], d[v], (v, link))  # update the pq entry\n\n  return tree", "answer": "    d = {}  # d[v] is bound on distance to tree\n    tree = []  # list of edges in spanning tree\n    pq = AdaptableHeapPriorityQueue()  # d[v] maps to (v, e=(u,v))\n    pqlocator = {}  # map from vertex to its pq locator\n\n    # initialize\n    for v in g.vertices():\n        if len(d) == 0:  # this is the first node\n            d[v] = 0\n        else:\n            d[v] = float('inf')  # positive infinity\n        pqlocator[v] = pq.add(d[v], (v, None))\n\n    while not pq.is_empty():\n        key, value = pq.remove_min()\n        u, edge = value\n        del pqlocator[u]  # u is no longer in pq\n        if edge is not None:\n            tree.append(edge)\n        for e in g.incident_edges(u):\n            v = e.opposite(u)\n            if v in pqlocator:  # thus v not yet in tree\n                # see if edge (u,v) better connects v to the growing tree\n                weight = e.element()\n                if weight < d[v]:\n                    d[v] = weight\n                    pq.update(pqlocator[v], d[v], (v, e))\n\n    return tree", "license": "gpl-3.0-or-later", "author": "shenzhun <shenzhunallen@gmail.com", "project": "jsculsp_data-structures-and-algorithms-in-python", "signature": "def MST_PrimJarnik(g):", "license_type": "Strong Copyleft", "init_header": "# Copyright 2013, Michael H. Goldwasser\n#\n# Developed for use with the book:\n#\n#    Data Structures and Algorithms in Python\n#    Michael T. Goodrich, Roberto Tamassia, and Michael H. Goldwasser\n#    John Wiley & Sons, 2013\n#\n# This program is free software: you can redistribute it and/or modify\n# it under the terms of the GNU General Public License as published by\n# the Free Software Foundation, either version 3 of the License, or\n# (at your option) any later version.\n#\n# This program is distributed in the hope that it will be useful,\n# but WITHOUT ANY WARRANTY; without even the implied warranty of\n# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n# GNU General Public License for more details.\n#\n# You should have received a copy of the GNU General Public License\n# along with this program.  If not, see <http://www.gnu.org/licenses/>.\n\nfrom ..ch09.pq import HeapPriorityQueue,AdaptableHeapPriorityQueue\nfrom .partition import Partition\n\ndef MST_PrimJarnik(g):\n  \"\"\"Compute a minimum spanning tree of weighted graph g.\n\n  Return a list of edges that comprise the MST (in arbitrary order).\n  \"\"\"", "docstring": "  \"\"\"Compute a minimum spanning tree of weighted graph g.\n\n  Return a list of edges that comprise the MST (in arbitrary order).\n  \"\"\""}, "e9eef7741875108f42713293e2776a01178493a3": {"bleu": 0.8916751343205617, "jaccard": 0.73828125, "edit_distance": 0.823699421965318, "comment_cnt": 11, "max_sim": 0.8916751343205617, "header": "\nfrom ..ch09.adaptable_heap_priority_queue import AdaptableHeapPriorityQueue\n\ndef shortest_path_lengths(g, src):\n  \"\"\"Compute shortest-path distances from src to reachable vertices of g.\n\n  Graph g can be undirected or directed, but must be weighted such that\n  e.element() returns a numeric weight for each edge e.\n\n  Return dictionary mapping each reachable vertex to its distance from src.\n  \"\"\"", "body": "  d = {}                                        # d[v] is upper bound from s to v\n  cloud = {}                                    # map reachable v to its d[v] value\n  pq = AdaptableHeapPriorityQueue()             # vertex v will have key d[v]\n  pqlocator = {}                                # map from vertex to its pq locator\n\n  # for each vertex v of the graph, add an entry to the priority queue, with\n  # the source having distance 0 and all others having infinite distance\n  for v in g.vertices():\n    if v is src:\n      d[v] = 0\n    else:\n      d[v] = float('inf')                       # syntax for positive infinity\n    pqlocator[v] = pq.add(d[v], v)              # save locator for future updates\n\n  while not pq.is_empty():\n    key, u = pq.remove_min()\n    cloud[u] = key                              # its correct d[u] value\n    del pqlocator[u]                            # u is no longer in pq\n    for e in g.incident_edges(u):               # outgoing edges (u,v)\n      v = e.opposite(u)\n      if v not in cloud:\n        # perform relaxation step on edge (u,v)\n        wgt = e.element()\n        if d[u] + wgt < d[v]:                   # better path to v?\n          d[v] = d[u] + wgt                     # update the distance\n          pq.update(pqlocator[v], d[v], v)      # update the pq entry\n\n  return cloud                                  # only includes reachable vertices", "answer": "    d = {}  # d[v] is upper bound from src to v\n    cloud = {}  # map reachable vertex to its distance from src\n    pq = AdaptableHeapPriorityQueue()  # vertex v will have key d[v]\n    pqlocator = {}  # map from vertex to its pq locator\n\n    # for each vertex v of the graph, add an entry to the priority queue, with\n    # the source having distance 0 and all others having infinite distance\n    for v in g.vertices():\n        if v == src:\n            d[v] = 0\n        else:\n            d[v] = float('inf')\n        pqlocator[v] = pq.add(d[v], v)\n\n    while not pq.is_empty():\n        key, u = pq.remove_min()\n        cloud[u] = key  # its correct d[u] value\n        del pqlocator[u]  # u is no longer in pq\n\n        for e in g.incident_edges(u):  # outgoing edges (u,v)\n            v = e.opposite(u)\n            if v not in cloud:\n                # perform relaxation step on edge (u,v)\n                wgt = e.element()\n                if d[u] + wgt < d[v]:  # better path to v?\n                    d[v] = d[u] + wgt  # update the distance\n                    pq.update(pqlocator[v], d[v], v)  # update the pq entry\n\n    return cloud", "license": "gpl-3.0-or-later", "author": "shenzhun <shenzhunallen@gmail.com", "project": "jsculsp_data-structures-and-algorithms-in-python", "signature": "def shortest_path_lengths(g, src):", "license_type": "Strong Copyleft", "init_header": "# Copyright 2013, Michael H. Goldwasser\n#\n# Developed for use with the book:\n#\n#    Data Structures and Algorithms in Python\n#    Michael T. Goodrich, Roberto Tamassia, and Michael H. Goldwasser\n#    John Wiley & Sons, 2013\n#\n# This program is free software: you can redistribute it and/or modify\n# it under the terms of the GNU General Public License as published by\n# the Free Software Foundation, either version 3 of the License, or\n# (at your option) any later version.\n#\n# This program is distributed in the hope that it will be useful,\n# but WITHOUT ANY WARRANTY; without even the implied warranty of\n# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n# GNU General Public License for more details.\n#\n# You should have received a copy of the GNU General Public License\n# along with this program.  If not, see <http://www.gnu.org/licenses/>.\n\nfrom ..ch09.adaptable_heap_priority_queue import AdaptableHeapPriorityQueue\n\ndef shortest_path_lengths(g, src):\n  \"\"\"Compute shortest-path distances from src to reachable vertices of g.\n\n  Graph g can be undirected or directed, but must be weighted such that\n  e.element() returns a numeric weight for each edge e.\n\n  Return dictionary mapping each reachable vertex to its distance from src.\n  \"\"\"", "docstring": "  \"\"\"Compute shortest-path distances from src to reachable vertices of g.\n\n  Graph g can be undirected or directed, but must be weighted such that\n  e.element() returns a numeric weight for each edge e.\n\n  Return dictionary mapping each reachable vertex to its distance from src.\n  \"\"\""}, "96303f3984c418ce772e98b4f384f86ff352f8da": {"bleu": 0.8879291970410158, "jaccard": 0.77734375, "edit_distance": 0.8940397350993378, "comment_cnt": 8, "max_sim": 0.8940397350993378, "header": "\"\"\"Builds the CIFAR-10 network with additional variables to support pruning.\n\nSummary of available functions:\n\n inputs, labels = distorted_inputs()\n\n predictions = inference(inputs)\n\n loss = loss(predictions, labels)\n\n train_op = train(loss, global_step)\n\"\"\"\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport os\nimport re\nimport sys\nimport tarfile\n\nfrom six.moves import urllib\nimport tensorflow as tf\n\nfrom tensorflow.contrib.model_pruning.examples.cifar10 import cifar10_input\nfrom tensorflow.contrib.model_pruning.python import pruning\n\nIMAGE_SIZE = cifar10_input.IMAGE_SIZE\nNUM_CLASSES = cifar10_input.NUM_CLASSES\nNUM_EXAMPLES_PER_EPOCH_FOR_TRAIN = cifar10_input.NUM_EXAMPLES_PER_EPOCH_FOR_TRAIN  # pylint: disable=line-too-long\nNUM_EXAMPLES_PER_EPOCH_FOR_EVAL = cifar10_input.NUM_EXAMPLES_PER_EPOCH_FOR_EVAL\nBATCH_SIZE = 128\nDATA_DIR = '/tmp/cifar10_data'\n\nMOVING_AVERAGE_DECAY = 0.9999  # The decay to use for the moving average.\nNUM_EPOCHS_PER_DECAY = 350.0  # Epochs after which learning rate decays.\nLEARNING_RATE_DECAY_FACTOR = 0.1  # Learning rate decay factor.\nINITIAL_LEARNING_RATE = 0.1  # Initial learning rate.\n\nTOWER_NAME = 'tower'\n\nDATA_URL = 'http://www.cs.toronto.edu/~kriz/cifar-10-binary.tar.gz'\n\n\ndef train(total_loss, global_step):\n  \"\"\"Train CIFAR-10 model.\n\n  Create an optimizer and apply to all trainable variables. Add moving\n  average for all trainable variables.\n\n  Args:\n    total_loss: Total loss from loss().\n    global_step: Integer Variable counting the number of training steps\n      processed.\n\n  Returns:\n    train_op: op for training.\n  \"\"\"", "body": "  # Variables that affect learning rate.\n  num_batches_per_epoch = NUM_EXAMPLES_PER_EPOCH_FOR_TRAIN / BATCH_SIZE\n  decay_steps = int(num_batches_per_epoch * NUM_EPOCHS_PER_DECAY)\n\n  # Decay the learning rate exponentially based on the number of steps.\n  lr = tf.train.exponential_decay(\n      INITIAL_LEARNING_RATE,\n      global_step,\n      decay_steps,\n      LEARNING_RATE_DECAY_FACTOR,\n      staircase=True)\n  tf.summary.scalar('learning_rate', lr)\n\n  # Generate moving averages of all losses and associated summaries.\n  loss_averages_op = _add_loss_summaries(total_loss)\n\n  # Compute gradients.\n  with tf.control_dependencies([loss_averages_op]):\n    opt = tf.train.GradientDescentOptimizer(lr)\n    grads = opt.compute_gradients(total_loss)\n\n  # Apply gradients.\n  apply_gradient_op = opt.apply_gradients(grads, global_step=global_step)\n\n  # Add histograms for trainable variables.\n  for var in tf.trainable_variables():\n    tf.summary.histogram(var.op.name, var)\n\n  # Add histograms for gradients.\n  for grad, var in grads:\n    if grad is not None:\n      tf.summary.histogram(var.op.name + '/gradients', grad)\n\n  # Track the moving averages of all trainable variables.\n  variable_averages = tf.train.ExponentialMovingAverage(MOVING_AVERAGE_DECAY,\n                                                        global_step)\n  variables_averages_op = variable_averages.apply(tf.trainable_variables())\n\n  with tf.control_dependencies([apply_gradient_op, variables_averages_op]):\n    train_op = tf.no_op(name='train')\n\n  return train_op", "answer": "# Variables that affect learning rate.\n  num_batches_per_epoch = NUM_EXAMPLES_PER_EPOCH_FOR_TRAIN / BATCH_SIZE\n  decay_steps = int(num_batches_per_epoch * NUM_EPOCHS_PER_DECAY)\n\n  # Decay the learning rate exponentially based on the number of steps.\n  lr = tf.train.exponential_decay(INITIAL_LEARNING_RATE,\n                                  global_step,\n                                  decay_steps,\n                                  LEARNING_RATE_DECAY_FACTOR,\n                                  staircase=True)\n  tf.summary.scalar('learning_rate', lr)\n\n  # Generate moving averages of all losses and associated summaries.\n  loss_averages_op = tf.train.ExponentialMovingAverage(0.9, name='avg').apply([total_loss])\n  with tf.control_dependencies([loss_averages_op]):\n    total_loss = tf.identity(total_loss)\n\n  # Compute gradients.\n  opt = tf.train.GradientDescentOptimizer(lr)\n  grads = opt.compute_gradients(total_loss)\n\n  # Apply gradients.\n  apply_gradient_op = opt.apply_gradients(grads, global_step=global_step)\n\n  # Add histograms for trainable variables.\n  for var in tf.trainable_variables():\n    tf.summary.histogram(var.op.name, var)\n\n  # Add histograms for gradients.\n  for grad, var in grads:\n    if grad is not None:\n      tf.summary.histogram(var.op.name + '/gradients', grad)\n\n  # Track the moving averages of all trainable variables.\n  variable_averages = tf.train.ExponentialMovingAverage(MOVING_AVERAGE_DECAY, global_step)\n  variables_averages_op = variable_averages.apply(tf.trainable_variables())\n\n  with tf.control_dependencies([apply_gradient_op, variables_averages_op]):\n    train_op = tf.no_op(name='train')\n\n  return train_op", "license": "apache-2.0", "author": "Mark Daoust <markdaoust@google.com", "project": "matveevi_temp", "signature": "def train(total_loss, global_step):", "license_type": "Permissive", "init_header": "# Copyright 2017 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\"\"\"Builds the CIFAR-10 network with additional variables to support pruning.\n\nSummary of available functions:\n\n # Compute input images and labels for training. If you would like to run\n # evaluations, use inputs() instead.\n inputs, labels = distorted_inputs()\n\n # Compute inference on the model inputs to make a prediction.\n predictions = inference(inputs)\n\n # Compute the total loss of the prediction with respect to the labels.\n loss = loss(predictions, labels)\n\n # Create a graph to run one step of training with respect to the loss.\n train_op = train(loss, global_step)\n\"\"\"\n# pylint: disable=missing-docstring\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport os\nimport re\nimport sys\nimport tarfile\n\nfrom six.moves import urllib\nimport tensorflow as tf\n\nfrom tensorflow.contrib.model_pruning.examples.cifar10 import cifar10_input\nfrom tensorflow.contrib.model_pruning.python import pruning\n\n# Global constants describing the CIFAR-10 data set.\nIMAGE_SIZE = cifar10_input.IMAGE_SIZE\nNUM_CLASSES = cifar10_input.NUM_CLASSES\nNUM_EXAMPLES_PER_EPOCH_FOR_TRAIN = cifar10_input.NUM_EXAMPLES_PER_EPOCH_FOR_TRAIN  # pylint: disable=line-too-long\nNUM_EXAMPLES_PER_EPOCH_FOR_EVAL = cifar10_input.NUM_EXAMPLES_PER_EPOCH_FOR_EVAL\nBATCH_SIZE = 128\nDATA_DIR = '/tmp/cifar10_data'\n\n# Constants describing the training process.\nMOVING_AVERAGE_DECAY = 0.9999  # The decay to use for the moving average.\nNUM_EPOCHS_PER_DECAY = 350.0  # Epochs after which learning rate decays.\nLEARNING_RATE_DECAY_FACTOR = 0.1  # Learning rate decay factor.\nINITIAL_LEARNING_RATE = 0.1  # Initial learning rate.\n\n# If a model is trained with multiple GPUs, prefix all Op names with tower_name\n# to differentiate the operations. Note that this prefix is removed from the\n# names of the summaries when visualizing a model.\nTOWER_NAME = 'tower'\n\nDATA_URL = 'http://www.cs.toronto.edu/~kriz/cifar-10-binary.tar.gz'\n\n\ndef train(total_loss, global_step):\n  \"\"\"Train CIFAR-10 model.\n\n  Create an optimizer and apply to all trainable variables. Add moving\n  average for all trainable variables.\n\n  Args:\n    total_loss: Total loss from loss().\n    global_step: Integer Variable counting the number of training steps\n      processed.\n\n  Returns:\n    train_op: op for training.\n  \"\"\"", "docstring": "  \"\"\"Train CIFAR-10 model.\n\n  Create an optimizer and apply to all trainable variables. Add moving\n  average for all trainable variables.\n\n  Args:\n    total_loss: Total loss from loss().\n    global_step: Integer Variable counting the number of training steps\n      processed.\n\n  Returns:\n    train_op: op for training.\n  \"\"\""}, "eef13de1982b7e07eedd73c3245b15d15673481a": {"bleu": 0.6909004091020635, "jaccard": 0.4765625, "edit_distance": 0.6064516129032258, "comment_cnt": 1, "max_sim": 0.6909004091020635, "header": "\ndef insertion_sort(L):\n  \"\"\"Sort PositionalList of comparable elements into nondecreasing order.\"\"\"", "body": "  if len(L) > 1:                    # otherwise, no need to sort it\n    marker = L.first()\n    while marker != L.last():\n      pivot = L.after(marker)       # next item to place\n      value = pivot.element()\n      if value > marker.element():  # pivot is already sorted\n        marker = pivot              # pivot becomes new marker\n      else:                         # must relocate pivot\n        walk = marker               # find leftmost item greater than value\n        while walk != L.first() and L.before(walk).element() > value:\n          walk = L.before(walk)\n        L.delete(pivot)\n        L.add_before(walk, value)   # reinsert value before walk", "answer": "    if len(L) > 1:  # Only sort if the list has more than one element\n        marker = L.first()\n        while marker != L.last():\n            pivot = L.after(marker)\n            value = pivot.element()\n            if value > marker.element():\n                marker = pivot  # pivot is already sorted\n            else:\n                walk = marker\n                while walk != L.first() and L.before(walk).element() > value:\n                    walk = L.before(walk)\n                L.delete(pivot)\n                L.add_before(walk, value)", "license": "gpl-3.0-or-later", "author": "Son-Huy TRAN <sonhuytran@gmail.com", "project": "thakrah_Learning.Python", "signature": "def insertion_sort(L):", "license_type": "Strong Copyleft", "init_header": "# Copyright 2013, Michael H. Goldwasser\n#\n# Developed for use with the book:\n#\n#    Data Structures and Algorithms in Python\n#    Michael T. Goodrich, Roberto Tamassia, and Michael H. Goldwasser\n#    John Wiley & Sons, 2013\n#\n# This program is free software: you can redistribute it and/or modify\n# it under the terms of the GNU General Public License as published by\n# the Free Software Foundation, either version 3 of the License, or\n# (at your option) any later version.\n#\n# This program is distributed in the hope that it will be useful,\n# but WITHOUT ANY WARRANTY; without even the implied warranty of\n# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n# GNU General Public License for more details.\n#\n# You should have received a copy of the GNU General Public License\n# along with this program.  If not, see <http://www.gnu.org/licenses/>.\n\ndef insertion_sort(L):\n  \"\"\"Sort PositionalList of comparable elements into nondecreasing order.\"\"\"", "docstring": "  \"\"\"Sort PositionalList of comparable elements into nondecreasing order.\"\"\""}, "523a3e34b2733f7eff9a63ada931f73e45ab5eb7": {"bleu": 0.7663918901904676, "jaccard": 0.53125, "edit_distance": 0.6968085106382979, "comment_cnt": 1, "max_sim": 0.7663918901904676, "header": "\"\"\"blob helper functions.\"\"\"\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\nfrom __future__ import unicode_literals\n\nfrom six.moves import cPickle as pickle\nimport numpy as np\nimport cv2\n\nfrom core.config import cfg\n\n\ndef im_list_to_blob(ims):\n    \"\"\"Convert a list of images into a network input. Assumes images were\n    prepared using prep_im_for_blob or equivalent: i.e.\n      - BGR channel order\n      - pixel means subtracted\n      - resized to the desired input size\n      - float32 numpy ndarray format\n    Output is a 4D HCHW tensor of the images concatenated along axis 0 with\n    shape.\n    \"\"\"", "body": "    if not isinstance(ims, list):\n        ims = [ims]\n    max_shape = get_max_shape([im.shape[:2] for im in ims])\n\n    num_images = len(ims)\n    blob = np.zeros(\n        (num_images, max_shape[0], max_shape[1], 3), dtype=np.float32)\n    for i in range(num_images):\n        im = ims[i]\n        blob[i, 0:im.shape[0], 0:im.shape[1], :] = im\n    # Move channels (axis 3) to axis 1\n    # Axis order will become: (batch elem, channel, height, width)\n    channel_swap = (0, 3, 1, 2)\n    blob = blob.transpose(channel_swap)\n    return blob", "answer": "max_shape = np.array([im.shape for im in ims]).max(axis=0)\n    num_images = len(ims)\n    blob = np.zeros((num_images, max_shape[0], max_shape[1], 3), dtype=np.float32)\n    \n    for i in range(num_images):\n        im = ims[i]\n        blob[i, 0:im.shape[0], 0:im.shape[1], :] = im\n\n    # Move channels (axis 3) to axis 1\n    # Axis order will become: (batch, channel, height, width)\n    blob = blob.transpose((0, 3, 1, 2))\n    return blob", "license": "apache-2.0", "author": "Roy <roytseng.tw@gmail.com", "project": "jwyang_faster-rcnn.pytorch", "signature": "def im_list_to_blob(ims):", "license_type": "Permissive", "init_header": "# Copyright (c) 2017-present, Facebook, Inc.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n##############################################################################\n#\n# Based on:\n# --------------------------------------------------------\n# Fast R-CNN\n# Copyright (c) 2015 Microsoft\n# Licensed under The MIT License [see LICENSE for details]\n# Written by Ross Girshick\n# --------------------------------------------------------\n\"\"\"blob helper functions.\"\"\"\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\nfrom __future__ import unicode_literals\n\nfrom six.moves import cPickle as pickle\nimport numpy as np\nimport cv2\n\nfrom core.config import cfg\n\n\ndef im_list_to_blob(ims):\n    \"\"\"Convert a list of images into a network input. Assumes images were\n    prepared using prep_im_for_blob or equivalent: i.e.\n      - BGR channel order\n      - pixel means subtracted\n      - resized to the desired input size\n      - float32 numpy ndarray format\n    Output is a 4D HCHW tensor of the images concatenated along axis 0 with\n    shape.\n    \"\"\"", "docstring": "    \"\"\"Convert a list of images into a network input. Assumes images were\n    prepared using prep_im_for_blob or equivalent: i.e.\n      - BGR channel order\n      - pixel means subtracted\n      - resized to the desired input size\n      - float32 numpy ndarray format\n    Output is a 4D HCHW tensor of the images concatenated along axis 0 with\n    shape.\n    \"\"\""}, "58e2dfbefccb5b258ab63949f30a82631588f4ad": {"bleu": 0.6583709471734647, "jaccard": 0.66015625, "edit_distance": 0.7289473684210526, "comment_cnt": 3, "max_sim": 0.7289473684210526, "header": "\"\"\"\nGenerate samples of synthetic data sets.\n\"\"\"\n\n\nimport numbers\nimport warnings\nimport array\nimport numpy as np\nfrom scipy import linalg\nimport scipy.sparse as sp\n\nfrom ..preprocessing import MultiLabelBinarizer\nfrom ..utils import check_array, check_random_state\nfrom ..utils import shuffle as util_shuffle\nfrom ..utils.fixes import astype\nfrom ..utils.random import sample_without_replacement\nfrom ..externals import six\nmap = six.moves.map\nzip = six.moves.zip\n\n\ndef make_regression(n_samples=100, n_features=100, n_informative=10,\n                    n_targets=1, bias=0.0, effective_rank=None,\n                    tail_strength=0.5, noise=0.0, shuffle=True, coef=False,\n                    random_state=None):\n    \"\"\"Generate a random regression problem.\n\n    The input set can either be well conditioned (by default) or have a low\n    rank-fat tail singular profile. See :func:`make_low_rank_matrix` for\n    more details.\n\n    The output is generated by applying a (potentially biased) random linear\n    regression model with `n_informative` nonzero regressors to the previously\n    generated input and some gaussian centered noise with some adjustable\n    scale.\n\n    Parameters\n    ----------\n    n_samples : int, optional (default=100)\n        The number of samples.\n\n    n_features : int, optional (default=100)\n        The number of features.\n\n    n_informative : int, optional (default=10)\n        The number of informative features, i.e., the number of features used\n        to build the linear model used to generate the output.\n\n    n_targets : int, optional (default=1)\n        The number of regression targets, i.e., the dimension of the y output\n        vector associated with a sample. By default, the output is a scalar.\n\n    bias : float, optional (default=0.0)\n        The bias term in the underlying linear model.\n\n    effective_rank : int or None, optional (default=None)\n        if not None:\n            The approximate number of singular vectors required to explain most\n            of the input data by linear combinations. Using this kind of\n            singular spectrum in the input allows the generator to reproduce\n            the correlations often observed in practice.\n        if None:\n            The input set is well conditioned, centered and gaussian with\n            unit variance.\n\n    tail_strength : float between 0.0 and 1.0, optional (default=0.5)\n        The relative importance of the fat noisy tail of the singular values\n        profile if `effective_rank` is not None.\n\n    noise : float, optional (default=0.0)\n        The standard deviation of the gaussian noise applied to the output.\n\n    shuffle : boolean, optional (default=True)\n        Shuffle the samples and the features.\n\n    coef : boolean, optional (default=False)\n        If True, the coefficients of the underlying linear model are returned.\n\n    random_state : int, RandomState instance or None, optional (default=None)\n        If int, random_state is the seed used by the random number generator;\n        If RandomState instance, random_state is the random number generator;\n        If None, the random number generator is the RandomState instance used\n        by `np.random`.\n\n    Returns\n    -------\n    X : array of shape [n_samples, n_features]\n        The input samples.\n\n    y : array of shape [n_samples] or [n_samples, n_targets]\n        The output values.\n\n    coef : array of shape [n_features] or [n_features, n_targets], optional\n        The coefficient of the underlying linear model. It is returned only if\n        coef is True.\n    \"\"\"", "body": "    n_informative = min(n_features, n_informative)\n    generator = check_random_state(random_state)\n\n    if effective_rank is None:\n        # Randomly generate a well conditioned input set\n        X = generator.randn(n_samples, n_features)\n\n    else:\n        # Randomly generate a low rank, fat tail input set\n        X = make_low_rank_matrix(n_samples=n_samples,\n                                 n_features=n_features,\n                                 effective_rank=effective_rank,\n                                 tail_strength=tail_strength,\n                                 random_state=generator)\n\n    # Generate a ground truth model with only n_informative features being non\n    # zeros (the other features are not correlated to y and should be ignored\n    # by a sparsifying regularizers such as L1 or elastic net)\n    ground_truth = np.zeros((n_features, n_targets))\n    ground_truth[:n_informative, :] = 100 * generator.rand(n_informative,\n                                                           n_targets)\n\n    y = np.dot(X, ground_truth) + bias\n\n    # Add noise\n    if noise > 0.0:\n        y += generator.normal(scale=noise, size=y.shape)\n\n    # Randomly permute samples and features\n    if shuffle:\n        X, y = util_shuffle(X, y, random_state=generator)\n\n        indices = np.arange(n_features)\n        generator.shuffle(indices)\n        X[:, :] = X[:, indices]\n        ground_truth = ground_truth[indices]\n\n    y = np.squeeze(y)\n\n    if coef:\n        return X, y, np.squeeze(ground_truth)\n\n    else:\n        return X, y", "answer": "generator = check_random_state(random_state)\n\n    if effective_rank is None:\n        # Randomly generate a well conditioned input set\n        X = generator.randn(n_samples, n_features)\n    else:\n        # Randomly generate a low rank, fat tail input set\n        X = make_low_rank_matrix(n_samples=n_samples, n_features=n_features,\n                                 effective_rank=effective_rank,\n                                 tail_strength=tail_strength,\n                                 random_state=generator)\n\n    # Generate a ground truth model with only n_informative features being non-zero\n    ground_truth = np.zeros((n_features, n_targets))\n    ground_truth[:n_informative, :] = 100 * generator.rand(n_informative, n_targets)\n\n    y = np.dot(X, ground_truth) + bias\n\n    # Add noise\n    if noise > 0.0:\n        y += generator.normal(scale=noise, size=y.shape)\n\n    # Optionally shuffle the dataset\n    if shuffle:\n        X, y = util_shuffle(X, y, random_state=generator)\n\n    y = np.squeeze(y)\n\n    if coef:\n        return X, y, np.squeeze(ground_truth)\n\n    return X, y", "license": "bsd-3-clause", "author": "Raghav R V <ragvrv@gmail.com", "project": "scikit-learn_scikit-learn", "signature": "def make_regression(n_samples=100, n_features=100, n_informative=10,", "license_type": "Permissive", "init_header": "\"\"\"\nGenerate samples of synthetic data sets.\n\"\"\"\n\n# Authors: B. Thirion, G. Varoquaux, A. Gramfort, V. Michel, O. Grisel,\n#          G. Louppe, J. Nothman\n# License: BSD 3 clause\n\nimport numbers\nimport warnings\nimport array\nimport numpy as np\nfrom scipy import linalg\nimport scipy.sparse as sp\n\nfrom ..preprocessing import MultiLabelBinarizer\nfrom ..utils import check_array, check_random_state\nfrom ..utils import shuffle as util_shuffle\nfrom ..utils.fixes import astype\nfrom ..utils.random import sample_without_replacement\nfrom ..externals import six\nmap = six.moves.map\nzip = six.moves.zip\n\n\ndef make_regression(n_samples=100, n_features=100, n_informative=10,\n                    n_targets=1, bias=0.0, effective_rank=None,\n                    tail_strength=0.5, noise=0.0, shuffle=True, coef=False,\n                    random_state=None):\n    \"\"\"Generate a random regression problem.\n\n    The input set can either be well conditioned (by default) or have a low\n    rank-fat tail singular profile. See :func:`make_low_rank_matrix` for\n    more details.\n\n    The output is generated by applying a (potentially biased) random linear\n    regression model with `n_informative` nonzero regressors to the previously\n    generated input and some gaussian centered noise with some adjustable\n    scale.\n\n    Parameters\n    ----------\n    n_samples : int, optional (default=100)\n        The number of samples.\n\n    n_features : int, optional (default=100)\n        The number of features.\n\n    n_informative : int, optional (default=10)\n        The number of informative features, i.e., the number of features used\n        to build the linear model used to generate the output.\n\n    n_targets : int, optional (default=1)\n        The number of regression targets, i.e., the dimension of the y output\n        vector associated with a sample. By default, the output is a scalar.\n\n    bias : float, optional (default=0.0)\n        The bias term in the underlying linear model.\n\n    effective_rank : int or None, optional (default=None)\n        if not None:\n            The approximate number of singular vectors required to explain most\n            of the input data by linear combinations. Using this kind of\n            singular spectrum in the input allows the generator to reproduce\n            the correlations often observed in practice.\n        if None:\n            The input set is well conditioned, centered and gaussian with\n            unit variance.\n\n    tail_strength : float between 0.0 and 1.0, optional (default=0.5)\n        The relative importance of the fat noisy tail of the singular values\n        profile if `effective_rank` is not None.\n\n    noise : float, optional (default=0.0)\n        The standard deviation of the gaussian noise applied to the output.\n\n    shuffle : boolean, optional (default=True)\n        Shuffle the samples and the features.\n\n    coef : boolean, optional (default=False)\n        If True, the coefficients of the underlying linear model are returned.\n\n    random_state : int, RandomState instance or None, optional (default=None)\n        If int, random_state is the seed used by the random number generator;\n        If RandomState instance, random_state is the random number generator;\n        If None, the random number generator is the RandomState instance used\n        by `np.random`.\n\n    Returns\n    -------\n    X : array of shape [n_samples, n_features]\n        The input samples.\n\n    y : array of shape [n_samples] or [n_samples, n_targets]\n        The output values.\n\n    coef : array of shape [n_features] or [n_features, n_targets], optional\n        The coefficient of the underlying linear model. It is returned only if\n        coef is True.\n    \"\"\"", "docstring": "                    n_targets=1, bias=0.0, effective_rank=None,\n                    tail_strength=0.5, noise=0.0, shuffle=True, coef=False,\n                    random_state=None):\n    \"\"\"Generate a random regression problem.\n\n    The input set can either be well conditioned (by default) or have a low\n    rank-fat tail singular profile. See :func:`make_low_rank_matrix` for\n    more details.\n\n    The output is generated by applying a (potentially biased) random linear\n    regression model with `n_informative` nonzero regressors to the previously\n    generated input and some gaussian centered noise with some adjustable\n    scale.\n\n    Parameters\n    ----------\n    n_samples : int, optional (default=100)\n        The number of samples.\n\n    n_features : int, optional (default=100)\n        The number of features.\n\n    n_informative : int, optional (default=10)\n        The number of informative features, i.e., the number of features used\n        to build the linear model used to generate the output.\n\n    n_targets : int, optional (default=1)\n        The number of regression targets, i.e., the dimension of the y output\n        vector associated with a sample. By default, the output is a scalar.\n\n    bias : float, optional (default=0.0)\n        The bias term in the underlying linear model.\n\n    effective_rank : int or None, optional (default=None)\n        if not None:\n            The approximate number of singular vectors required to explain most\n            of the input data by linear combinations. Using this kind of\n            singular spectrum in the input allows the generator to reproduce\n            the correlations often observed in practice.\n        if None:\n            The input set is well conditioned, centered and gaussian with\n            unit variance.\n\n    tail_strength : float between 0.0 and 1.0, optional (default=0.5)\n        The relative importance of the fat noisy tail of the singular values\n        profile if `effective_rank` is not None.\n\n    noise : float, optional (default=0.0)\n        The standard deviation of the gaussian noise applied to the output.\n\n    shuffle : boolean, optional (default=True)\n        Shuffle the samples and the features.\n\n    coef : boolean, optional (default=False)\n        If True, the coefficients of the underlying linear model are returned.\n\n    random_state : int, RandomState instance or None, optional (default=None)\n        If int, random_state is the seed used by the random number generator;\n        If RandomState instance, random_state is the random number generator;\n        If None, the random number generator is the RandomState instance used\n        by `np.random`.\n\n    Returns\n    -------\n    X : array of shape [n_samples, n_features]\n        The input samples.\n\n    y : array of shape [n_samples] or [n_samples, n_targets]\n        The output values.\n\n    coef : array of shape [n_features] or [n_features, n_targets], optional\n        The coefficient of the underlying linear model. It is returned only if\n        coef is True.\n    \"\"\""}, "d972ba3e3a98cd050945d71d1ba2e6833bb31d0c": {"bleu": 0.9783242082479183, "jaccard": 0.94140625, "edit_distance": 0.9890510948905109, "comment_cnt": 5, "max_sim": 0.9890510948905109, "header": "\n\"\"\"Python implementation of the PASCAL VOC devkit's AP evaluation code.\"\"\"\n\nimport cPickle\nimport logging\nimport numpy as np\nimport os\nimport xml.etree.ElementTree as ET\n\nlogger = logging.getLogger(__name__)\n\n\ndef voc_ap(rec, prec, use_07_metric=False):\n    \"\"\"Compute VOC AP given precision and recall. If use_07_metric is true, uses\n    the VOC 07 11-point method (default:False).\n    \"\"\"", "body": "    if use_07_metric:\n        # 11 point metric\n        ap = 0.\n        for t in np.arange(0., 1.1, 0.1):\n            if np.sum(rec >= t) == 0:\n                p = 0\n            else:\n                p = np.max(prec[rec >= t])\n            ap = ap + p / 11.\n    else:\n        # correct AP calculation\n        # first append sentinel values at the end\n        mrec = np.concatenate(([0.], rec, [1.]))\n        mpre = np.concatenate(([0.], prec, [0.]))\n\n        # compute the precision envelope\n        for i in range(mpre.size - 1, 0, -1):\n            mpre[i - 1] = np.maximum(mpre[i - 1], mpre[i])\n\n        # to calculate area under PR curve, look for points\n        # where X axis (recall) changes value\n        i = np.where(mrec[1:] != mrec[:-1])[0]\n\n        # and sum (\\Delta recall) * prec\n        ap = np.sum((mrec[i + 1] - mrec[i]) * mpre[i + 1])\n    return ap", "answer": "if use_07_metric:\n        # 11 point metric\n        ap = 0.0\n        for t in np.arange(0., 1.1, 0.1):\n            if np.sum(rec >= t) == 0:\n                p = 0\n            else:\n                p = np.max(prec[rec >= t])\n            ap = ap + p / 11.\n    else:\n        # correct AP calculation\n        # first append sentinel values at the end\n        mrec = np.concatenate(([0.], rec, [1.]))\n        mpre = np.concatenate(([0.], prec, [0.]))\n\n        # compute the precision envelope\n        for i in range(mpre.size - 1, 0, -1):\n            mpre[i - 1] = np.maximum(mpre[i - 1], mpre[i])\n\n        # to calculate area under PR curve, look for points where X axis (recall) changes value\n        i = np.where(mrec[1:] != mrec[:-1])[0]\n\n        # and sum (\\Delta recall) * prec\n        ap = np.sum((mrec[i + 1] - mrec[i]) * mpre[i + 1])\n    return ap", "license": "apache-2.0", "author": "lvpengyuan <lvpyuan@gmail.com", "project": "hoppaq_masktextspotter.caffe2", "signature": "def voc_ap(rec, prec, use_07_metric=False):", "license_type": "Permissive", "init_header": "# Copyright (c) 2017-present, Facebook, Inc.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n##############################################################################\n#\n# Based on:\n# --------------------------------------------------------\n# Fast/er R-CNN\n# Licensed under The MIT License [see LICENSE for details]\n# Written by Bharath Hariharan\n# --------------------------------------------------------\n\n\"\"\"Python implementation of the PASCAL VOC devkit's AP evaluation code.\"\"\"\n\nimport cPickle\nimport logging\nimport numpy as np\nimport os\nimport xml.etree.ElementTree as ET\n\nlogger = logging.getLogger(__name__)\n\n\ndef voc_ap(rec, prec, use_07_metric=False):\n    \"\"\"Compute VOC AP given precision and recall. If use_07_metric is true, uses\n    the VOC 07 11-point method (default:False).\n    \"\"\"", "docstring": "    \"\"\"Compute VOC AP given precision and recall. If use_07_metric is true, uses\n    the VOC 07 11-point method (default:False).\n    \"\"\""}, "7afc0b3ab029ddc3ceba806b0108b023fc320f5d": {"bleu": 0.6991612143002934, "jaccard": 0.3203125, "edit_distance": 0.6617886178861789, "comment_cnt": 3, "max_sim": 0.6991612143002934, "header": "\"\"\" Non-negative matrix factorization\n\"\"\"\n\n\nfrom __future__ import division\n\nfrom math import sqrt\nimport warnings\n\nimport numpy as np\nimport scipy.sparse as sp\nfrom scipy.optimize import nnls\n\nfrom ..base import BaseEstimator, TransformerMixin\nfrom ..utils import check_random_state, check_array\nfrom ..utils.extmath import randomized_svd, safe_sparse_dot, squared_norm\nfrom ..utils.validation import check_is_fitted\n\n\ndef _initialize_nmf(X, n_components, variant=None, eps=1e-6,\n                    random_state=None):\n    \"\"\"NNDSVD algorithm for NMF initialization.\n\n    Computes a good initial guess for the non-negative\n    rank k matrix approximation for X: X = WH\n\n    Parameters\n    ----------\n\n    X : array, [n_samples, n_features]\n        The data matrix to be decomposed.\n\n    n_components : array, [n_components, n_features]\n        The number of components desired in the approximation.\n\n    variant : None | 'a' | 'ar'\n        The variant of the NNDSVD algorithm.\n        Accepts None, 'a', 'ar'\n        None: leaves the zero entries as zero\n        'a': Fills the zero entries with the average of X\n        'ar': Fills the zero entries with standard normal random variates.\n        Default: None\n\n    eps: float\n        Truncate all values less then this in output to zero.\n\n    random_state : numpy.RandomState | int, optional\n        The generator used to fill in the zeros, when using variant='ar'\n        Default: numpy.random\n\n    Returns\n    -------\n\n    (W, H) :\n        Initial guesses for solving X ~= WH such that\n        the number of columns in W is n_components.\n\n    References\n    ----------\n    C. Boutsidis, E. Gallopoulos: SVD based initialization: A head start for \n    nonnegative matrix factorization - Pattern Recognition, 2008\n\n    http://tinyurl.com/nndsvd\n    \"\"\"", "body": "    check_non_negative(X, \"NMF initialization\")\n    if variant not in (None, 'a', 'ar'):\n        raise ValueError(\"Invalid variant name\")\n\n    U, S, V = randomized_svd(X, n_components)\n    W, H = np.zeros(U.shape), np.zeros(V.shape)\n\n    # The leading singular triplet is non-negative\n    # so it can be used as is for initialization.\n    W[:, 0] = np.sqrt(S[0]) * np.abs(U[:, 0])\n    H[0, :] = np.sqrt(S[0]) * np.abs(V[0, :])\n\n    for j in range(1, n_components):\n        x, y = U[:, j], V[j, :]\n\n        # extract positive and negative parts of column vectors\n        x_p, y_p = np.maximum(x, 0), np.maximum(y, 0)\n        x_n, y_n = np.abs(np.minimum(x, 0)), np.abs(np.minimum(y, 0))\n\n        # and their norms\n        x_p_nrm, y_p_nrm = norm(x_p), norm(y_p)\n        x_n_nrm, y_n_nrm = norm(x_n), norm(y_n)\n\n        m_p, m_n = x_p_nrm * y_p_nrm, x_n_nrm * y_n_nrm\n\n        # choose update\n        if m_p > m_n:\n            u = x_p / x_p_nrm\n            v = y_p / y_p_nrm\n            sigma = m_p\n        else:\n            u = x_n / x_n_nrm\n            v = y_n / y_n_nrm\n            sigma = m_n\n\n        lbd = np.sqrt(S[j] * sigma)\n        W[:, j] = lbd * u\n        H[j, :] = lbd * v\n\n    W[W < eps] = 0\n    H[H < eps] = 0\n\n    if variant == \"a\":\n        avg = X.mean()\n        W[W == 0] = avg\n        H[H == 0] = avg\n    elif variant == \"ar\":\n        random_state = check_random_state(random_state)\n        avg = X.mean()\n        W[W == 0] = abs(avg * random_state.randn(len(W[W == 0])) / 100)\n        H[H == 0] = abs(avg * random_state.randn(len(H[H == 0])) / 100)\n\n    return W, H", "answer": "if sp.issparse(X):\n        raise TypeError(\"Sparse matrix not supported by NNDSVD initialization\")\n\n    U, S, Vt = randomized_svd(X, n_components)\n    W = np.zeros((X.shape[0], n_components))\n    H = np.zeros((n_components, X.shape[1]))\n\n    # The leading singular triplet is non-negative\n    W[:, 0] = np.sqrt(S[0]) * np.abs(U[:, 0])\n    H[0, :] = np.sqrt(S[0]) * np.abs(Vt[0, :])\n\n    for j in range(1, n_components):\n        x, y = U[:, j], Vt[j, :]\n\n        # extract positive and negative parts of column vectors\n        x_p, y_p = np.maximum(x, 0), np.maximum(y, 0)\n        x_n, y_n = np.abs(np.minimum(x, 0)), np.abs(np.minimum(y, 0))\n\n        # and their norms\n        x_p_nrm, y_p_nrm = np.linalg.norm(x_p), np.linalg.norm(y_p)\n        x_n_nrm, y_n_nrm = np.linalg.norm(x_n), np.linalg.norm(y_n)\n\n        m_p = x_p_nrm * y_p_nrm\n        m_n = x_n_nrm * y_n_nrm\n\n        if m_p > m_n:\n            u = x_p / x_p_nrm\n            v = y_p / y_p_nrm\n            sigma = m_p\n        else:\n            u = x_n / x_n_nrm\n            v = y_n / y_n_nrm\n            sigma = m_n\n\n        W[:, j] = np.sqrt(S[j] * sigma) * u\n        H[j, :] = np.sqrt(S[j] * sigma) * v\n\n    if variant == 'a':\n        avg = np.mean(X)\n        W[W == 0] = avg\n        H[H == 0] = avg\n    elif variant == 'ar':\n        rng = check_random_state(random_state)\n        W[W == 0] = np.abs(rng.standard_normal(np.count_nonzero(W == 0)))\n        H[H == 0] = np.abs(rng.standard_normal(np.count_nonzero(H == 0)))\n\n    W[W < eps] = 0\n    H[H < eps] = 0\n\n    return W, H", "license": "bsd-3-clause", "author": "Raghav R V <ragvrv@gmail.com", "project": "scikit-learn_scikit-learn", "signature": "def _initialize_nmf(X, n_components, variant=None, eps=1e-6,", "license_type": "Permissive", "init_header": "\"\"\" Non-negative matrix factorization\n\"\"\"\n# Author: Vlad Niculae\n#         Lars Buitinck <L.J.Buitinck@uva.nl>\n# Author: Chih-Jen Lin, National Taiwan University (original projected gradient\n#     NMF implementation)\n# Author: Anthony Di Franco (original Python and NumPy port)\n# License: BSD 3 clause\n\n\nfrom __future__ import division\n\nfrom math import sqrt\nimport warnings\n\nimport numpy as np\nimport scipy.sparse as sp\nfrom scipy.optimize import nnls\n\nfrom ..base import BaseEstimator, TransformerMixin\nfrom ..utils import check_random_state, check_array\nfrom ..utils.extmath import randomized_svd, safe_sparse_dot, squared_norm\nfrom ..utils.validation import check_is_fitted\n\n\ndef _initialize_nmf(X, n_components, variant=None, eps=1e-6,\n                    random_state=None):\n    \"\"\"NNDSVD algorithm for NMF initialization.\n\n    Computes a good initial guess for the non-negative\n    rank k matrix approximation for X: X = WH\n\n    Parameters\n    ----------\n\n    X : array, [n_samples, n_features]\n        The data matrix to be decomposed.\n\n    n_components : array, [n_components, n_features]\n        The number of components desired in the approximation.\n\n    variant : None | 'a' | 'ar'\n        The variant of the NNDSVD algorithm.\n        Accepts None, 'a', 'ar'\n        None: leaves the zero entries as zero\n        'a': Fills the zero entries with the average of X\n        'ar': Fills the zero entries with standard normal random variates.\n        Default: None\n\n    eps: float\n        Truncate all values less then this in output to zero.\n\n    random_state : numpy.RandomState | int, optional\n        The generator used to fill in the zeros, when using variant='ar'\n        Default: numpy.random\n\n    Returns\n    -------\n\n    (W, H) :\n        Initial guesses for solving X ~= WH such that\n        the number of columns in W is n_components.\n\n    References\n    ----------\n    C. Boutsidis, E. Gallopoulos: SVD based initialization: A head start for \n    nonnegative matrix factorization - Pattern Recognition, 2008\n\n    http://tinyurl.com/nndsvd\n    \"\"\"", "docstring": "                    random_state=None):\n    \"\"\"NNDSVD algorithm for NMF initialization.\n\n    Computes a good initial guess for the non-negative\n    rank k matrix approximation for X: X = WH\n\n    Parameters\n    ----------\n\n    X : array, [n_samples, n_features]\n        The data matrix to be decomposed.\n\n    n_components : array, [n_components, n_features]\n        The number of components desired in the approximation.\n\n    variant : None | 'a' | 'ar'\n        The variant of the NNDSVD algorithm.\n        Accepts None, 'a', 'ar'\n        None: leaves the zero entries as zero\n        'a': Fills the zero entries with the average of X\n        'ar': Fills the zero entries with standard normal random variates.\n        Default: None\n\n    eps: float\n        Truncate all values less then this in output to zero.\n\n    random_state : numpy.RandomState | int, optional\n        The generator used to fill in the zeros, when using variant='ar'\n        Default: numpy.random\n\n    Returns\n    -------\n\n    (W, H) :\n        Initial guesses for solving X ~= WH such that\n        the number of columns in W is n_components.\n\n    References\n    ----------\n    C. Boutsidis, E. Gallopoulos: SVD based initialization: A head start for \n    nonnegative matrix factorization - Pattern Recognition, 2008\n\n    http://tinyurl.com/nndsvd\n    \"\"\""}, "ba67517b765e21909b3c86d7e06555964dde5cf3": {"bleu": 0.6003306222078617, "jaccard": 0.484375, "edit_distance": 0.5725190839694656, "comment_cnt": 3, "max_sim": 0.6003306222078617, "header": "\n\"\"\"Functions for evaluating results computed for a json dataset.\"\"\"\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\nfrom __future__ import unicode_literals\n\nimport json\nimport logging\nimport numpy as np\nimport os\nimport uuid\n\nfrom pycocotools.cocoeval import COCOeval\n\nfrom core.config import cfg\nfrom utils.io import save_object\nimport utils.boxes as box_utils\n\nlogger = logging.getLogger(__name__)\n\n\ndef evaluate_box_proposals(\n    json_dataset, roidb, thresholds=None, area='all', limit=None\n):\n    \"\"\"Evaluate detection proposal recall metrics. This function is a much\n    faster alternative to the official COCO API recall evaluation code. However,\n    it produces slightly different results.\n    \"\"\"", "body": "    # Record max overlap value for each gt box\n    # Return vector of overlap values\n    areas = {\n        'all': 0,\n        'small': 1,\n        'medium': 2,\n        'large': 3,\n        '96-128': 4,\n        '128-256': 5,\n        '256-512': 6,\n        '512-inf': 7}\n    area_ranges = [\n        [0**2, 1e5**2],    # all\n        [0**2, 32**2],     # small\n        [32**2, 96**2],    # medium\n        [96**2, 1e5**2],   # large\n        [96**2, 128**2],   # 96-128\n        [128**2, 256**2],  # 128-256\n        [256**2, 512**2],  # 256-512\n        [512**2, 1e5**2]]  # 512-inf\n    assert area in areas, 'Unknown area range: {}'.format(area)\n    area_range = area_ranges[areas[area]]\n    gt_overlaps = np.zeros(0)\n    num_pos = 0\n    for entry in roidb:\n        gt_inds = np.where(\n            (entry['gt_classes'] > 0) & (entry['is_crowd'] == 0))[0]\n        gt_boxes = entry['boxes'][gt_inds, :]\n        gt_areas = entry['seg_areas'][gt_inds]\n        valid_gt_inds = np.where(\n            (gt_areas >= area_range[0]) & (gt_areas <= area_range[1]))[0]\n        gt_boxes = gt_boxes[valid_gt_inds, :]\n        num_pos += len(valid_gt_inds)\n        non_gt_inds = np.where(entry['gt_classes'] == 0)[0]\n        boxes = entry['boxes'][non_gt_inds, :]\n        if boxes.shape[0] == 0:\n            continue\n        if limit is not None and boxes.shape[0] > limit:\n            boxes = boxes[:limit, :]\n        overlaps = box_utils.bbox_overlaps(\n            boxes.astype(dtype=np.float32, copy=False),\n            gt_boxes.astype(dtype=np.float32, copy=False))\n        _gt_overlaps = np.zeros((gt_boxes.shape[0]))\n        for j in range(min(boxes.shape[0], gt_boxes.shape[0])):\n            # find which proposal box maximally covers each gt box\n            argmax_overlaps = overlaps.argmax(axis=0)\n            # and get the iou amount of coverage for each gt box\n            max_overlaps = overlaps.max(axis=0)\n            # find which gt box is 'best' covered (i.e. 'best' = most iou)\n            gt_ind = max_overlaps.argmax()\n            gt_ovr = max_overlaps.max()\n            assert gt_ovr >= 0\n            # find the proposal box that covers the best covered gt box\n            box_ind = argmax_overlaps[gt_ind]\n            # record the iou coverage of this gt box\n            _gt_overlaps[j] = overlaps[box_ind, gt_ind]\n            assert _gt_overlaps[j] == gt_ovr\n            # mark the proposal box and the gt box as used\n            overlaps[box_ind, :] = -1\n            overlaps[:, gt_ind] = -1\n        # append recorded iou coverage level\n        gt_overlaps = np.hstack((gt_overlaps, _gt_overlaps))\n\n    gt_overlaps = np.sort(gt_overlaps)\n    if thresholds is None:\n        step = 0.05\n        thresholds = np.arange(0.5, 0.95 + 1e-5, step)\n    recalls = np.zeros_like(thresholds)\n    # compute recall for each iou threshold\n    for i, t in enumerate(thresholds):\n        recalls[i] = (gt_overlaps >= t).sum() / float(num_pos)\n    # ar = 2 * np.trapz(recalls, thresholds)\n    ar = recalls.mean()\n    return {'ar': ar, 'recalls': recalls, 'thresholds': thresholds,\n            'gt_overlaps': gt_overlaps, 'num_pos': num_pos}", "answer": "if thresholds is None:\n        thresholds = np.arange(0.5, 0.95 + 1e-5, 0.05)\n    areas = {\n        'all': 0,\n        'small': 1,\n        'medium': 2,\n        'large': 3,\n    }\n    area_ranges = [\n        [0 ** 2, 1e5 ** 2],  # all\n        [0 ** 2, 32 ** 2],   # small\n        [32 ** 2, 96 ** 2],  # medium\n        [96 ** 2, 1e5 ** 2], # large\n    ]\n    assert area in areas, 'Unknown area range: {}'.format(area)\n    area_range = area_ranges[areas[area]]\n\n    gt_overlaps = np.zeros(0)\n    num_pos = 0\n\n    for entry in roidb:\n        gt_inds = np.where(entry['gt_classes'] > 0)[0]\n        gt_boxes = entry['boxes'][gt_inds]\n        gt_areas = entry['seg_areas'][gt_inds]\n        valid_gt_inds = np.where(\n            (gt_areas >= area_range[0]) & (gt_areas <= area_range[1])\n        )[0]\n        gt_boxes = gt_boxes[valid_gt_inds]\n        num_pos += len(gt_boxes)\n\n        if len(gt_boxes) == 0:\n            continue\n\n        non_gt_inds = np.where(entry['gt_classes'] == 0)[0]\n        boxes = entry['boxes'][non_gt_inds, :]\n\n        if limit is not None and boxes.shape[0] > limit:\n            boxes = boxes[:limit, :]\n\n        overlaps = box_utils.bbox_overlaps(boxes.astype(np.float32), gt_boxes.astype(np.float32))\n\n        _gt_overlaps = np.zeros((gt_boxes.shape[0]))\n        for j in range(min(boxes.shape[0], gt_boxes.shape[0])):\n            argmax_overlaps = overlaps.argmax(axis=0)\n            max_overlaps = overlaps.max(axis=0)\n            gt_ind = max_overlaps.argmax()\n            gt_ovr = max_overlaps.max()\n            assert gt_ovr >= 0\n            box_ind = argmax_overlaps[gt_ind]\n            _gt_overlaps[gt_ind] = overlaps[box_ind, gt_ind]\n            overlaps[box_ind, :] = -1\n            overlaps[:, gt_ind] = -1\n\n        gt_overlaps = np.hstack((gt_overlaps, _gt_overlaps))\n\n    gt_overlaps = np.sort(gt_overlaps)\n    recalls = np.zeros_like(thresholds)\n    for i, t in enumerate(thresholds):\n        recalls[i] = (gt_overlaps >= t).sum() / float(num_pos)\n    ar = recalls.mean()\n\n    return {\n        'ar': ar,\n        'recalls': recalls,\n        'thresholds': thresholds,\n        'gt_overlaps': gt_overlaps,\n        'num_pos': num_pos,\n    }", "license": "apache-2.0", "author": "lvpengyuan <lvpyuan@gmail.com", "project": "hoppaq_masktextspotter.caffe2", "signature": "def evaluate_box_proposals(", "license_type": "Permissive", "init_header": "# Copyright (c) 2017-present, Facebook, Inc.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n##############################################################################\n\n\"\"\"Functions for evaluating results computed for a json dataset.\"\"\"\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\nfrom __future__ import unicode_literals\n\nimport json\nimport logging\nimport numpy as np\nimport os\nimport uuid\n\nfrom pycocotools.cocoeval import COCOeval\n\nfrom core.config import cfg\nfrom utils.io import save_object\nimport utils.boxes as box_utils\n\nlogger = logging.getLogger(__name__)\n\n\ndef evaluate_box_proposals(\n    json_dataset, roidb, thresholds=None, area='all', limit=None\n):\n    \"\"\"Evaluate detection proposal recall metrics. This function is a much\n    faster alternative to the official COCO API recall evaluation code. However,\n    it produces slightly different results.\n    \"\"\"", "docstring": "    json_dataset, roidb, thresholds=None, area='all', limit=None\n):\n    \"\"\"Evaluate detection proposal recall metrics. This function is a much\n    faster alternative to the official COCO API recall evaluation code. However,\n    it produces slightly different results.\n    \"\"\""}, "aa521eabb1fc2feae6fc995de7e66601917fdd54": {"bleu": 0.8512335764469439, "jaccard": 0.734375, "edit_distance": 0.8716417910447761, "comment_cnt": 2, "max_sim": 0.8716417910447761, "header": "\n\"\"\"Module for constructing RNN Cells.\n\n\n@@RNNCell\n\n\n@@BasicRNNCell\n@@BasicLSTMCell\n@@GRUCell\n@@LSTMCell\n\n\n@@LSTMStateTuple\n\n\n@@MultiRNNCell\n@@DropoutWrapper\n@@EmbeddingWrapper\n@@InputProjectionWrapper\n@@OutputProjectionWrapper\n\"\"\"\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport collections\nimport math\n\nfrom tensorflow.python.framework import ops\nfrom tensorflow.python.framework import tensor_shape\nfrom tensorflow.python.ops import array_ops\nfrom tensorflow.python.ops import clip_ops\nfrom tensorflow.python.ops import embedding_ops\nfrom tensorflow.python.ops import init_ops\nfrom tensorflow.python.ops import math_ops\nfrom tensorflow.python.ops import nn_ops\nfrom tensorflow.python.ops import variable_scope as vs\n\nfrom tensorflow.python.ops.math_ops import sigmoid\nfrom tensorflow.python.ops.math_ops import tanh\n\nfrom tensorflow.python.platform import tf_logging as logging\nfrom tensorflow.python.util import nest\n\n\ndef _linear(args, output_size, bias, bias_start=0.0, scope=None):\n  \"\"\"Linear map: sum_i(args[i] * W[i]), where W[i] is a variable.\n\n  Args:\n    args: a 2D Tensor or a list of 2D, batch x n, Tensors.\n    output_size: int, second dimension of W[i].\n    bias: boolean, whether to add a bias term or not.\n    bias_start: starting value to initialize the bias; 0 by default.\n    scope: VariableScope for the created subgraph; defaults to \"Linear\".\n\n  Returns:\n    A 2D Tensor with shape [batch x output_size] equal to\n    sum_i(args[i] * W[i]), where W[i]s are newly created matrices.\n\n  Raises:\n    ValueError: if some of the arguments has unspecified or wrong shape.\n  \"\"\"", "body": "  if args is None or (nest.is_sequence(args) and not args):\n    raise ValueError(\"`args` must be specified\")\n  if not nest.is_sequence(args):\n    args = [args]\n\n  # Calculate the total size of arguments on dimension 1.\n  total_arg_size = 0\n  shapes = [a.get_shape().as_list() for a in args]\n  for shape in shapes:\n    if len(shape) != 2:\n      raise ValueError(\"Linear is expecting 2D arguments: %s\" % str(shapes))\n    if not shape[1]:\n      raise ValueError(\"Linear expects shape[1] of arguments: %s\" % str(shapes))\n    else:\n      total_arg_size += shape[1]\n\n  dtype = [a.dtype for a in args][0]\n\n  # Now the computation.\n  with vs.variable_scope(scope or \"Linear\"):\n    matrix = vs.get_variable(\n        \"Matrix\", [total_arg_size, output_size], dtype=dtype)\n    if len(args) == 1:\n      res = math_ops.matmul(args[0], matrix)\n    else:\n      res = math_ops.matmul(array_ops.concat(1, args), matrix)\n    if not bias:\n      return res\n    bias_term = vs.get_variable(\n        \"Bias\", [output_size],\n        dtype=dtype,\n        initializer=init_ops.constant_initializer(\n            bias_start, dtype=dtype))\n  return res + bias_term", "answer": "if args is None or (nest.is_sequence(args) and not args):\n    raise ValueError(\"`args` must be specified\")\n  if not nest.is_sequence(args):\n    args = [args]\n\n  # Calculate the total size of arguments on dimension 1.\n  total_arg_size = 0\n  shapes = [a.get_shape().as_list() for a in args]\n  for shape in shapes:\n    if len(shape) != 2:\n      raise ValueError(\"Linear is expecting 2D arguments: %s\" % str(shapes))\n    if not shape[1]:\n      raise ValueError(\"Linear expects shape[1] of arguments: %s\" % str(shapes))\n    total_arg_size += shape[1]\n\n  # Now the computation.\n  with vs.variable_scope(scope or \"Linear\"):\n    matrix = vs.get_variable(\"Matrix\", [total_arg_size, output_size])\n    if len(args) == 1:\n      res = math_ops.matmul(args[0], matrix)\n    else:\n      res = math_ops.matmul(array_ops.concat(args, 1), matrix)\n    if not bias:\n      return res\n    bias_term = vs.get_variable(\"Bias\", [output_size], initializer=init_ops.constant_initializer(bias_start))\n  return res + bias_term", "license": "apache-2.0", "author": "Patrick Nguyen <drpng@google.com", "project": "tensorflow_tensorflow", "signature": "def _linear(args, output_size, bias, bias_start=0.0, scope=None):", "license_type": "Permissive", "init_header": "# Copyright 2015 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\n\"\"\"Module for constructing RNN Cells.\n\n## Base interface for all RNN Cells\n\n@@RNNCell\n\n## RNN Cells for use with TensorFlow's core RNN methods\n\n@@BasicRNNCell\n@@BasicLSTMCell\n@@GRUCell\n@@LSTMCell\n\n## Classes storing split `RNNCell` state\n\n@@LSTMStateTuple\n\n## RNN Cell wrappers (RNNCells that wrap other RNNCells)\n\n@@MultiRNNCell\n@@DropoutWrapper\n@@EmbeddingWrapper\n@@InputProjectionWrapper\n@@OutputProjectionWrapper\n\"\"\"\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport collections\nimport math\n\nfrom tensorflow.python.framework import ops\nfrom tensorflow.python.framework import tensor_shape\nfrom tensorflow.python.ops import array_ops\nfrom tensorflow.python.ops import clip_ops\nfrom tensorflow.python.ops import embedding_ops\nfrom tensorflow.python.ops import init_ops\nfrom tensorflow.python.ops import math_ops\nfrom tensorflow.python.ops import nn_ops\nfrom tensorflow.python.ops import variable_scope as vs\n\nfrom tensorflow.python.ops.math_ops import sigmoid\nfrom tensorflow.python.ops.math_ops import tanh\n\nfrom tensorflow.python.platform import tf_logging as logging\nfrom tensorflow.python.util import nest\n\n\ndef _linear(args, output_size, bias, bias_start=0.0, scope=None):\n  \"\"\"Linear map: sum_i(args[i] * W[i]), where W[i] is a variable.\n\n  Args:\n    args: a 2D Tensor or a list of 2D, batch x n, Tensors.\n    output_size: int, second dimension of W[i].\n    bias: boolean, whether to add a bias term or not.\n    bias_start: starting value to initialize the bias; 0 by default.\n    scope: VariableScope for the created subgraph; defaults to \"Linear\".\n\n  Returns:\n    A 2D Tensor with shape [batch x output_size] equal to\n    sum_i(args[i] * W[i]), where W[i]s are newly created matrices.\n\n  Raises:\n    ValueError: if some of the arguments has unspecified or wrong shape.\n  \"\"\"", "docstring": "  \"\"\"Linear map: sum_i(args[i] * W[i]), where W[i] is a variable.\n\n  Args:\n    args: a 2D Tensor or a list of 2D, batch x n, Tensors.\n    output_size: int, second dimension of W[i].\n    bias: boolean, whether to add a bias term or not.\n    bias_start: starting value to initialize the bias; 0 by default.\n    scope: VariableScope for the created subgraph; defaults to \"Linear\".\n\n  Returns:\n    A 2D Tensor with shape [batch x output_size] equal to\n    sum_i(args[i] * W[i]), where W[i]s are newly created matrices.\n\n  Raises:\n    ValueError: if some of the arguments has unspecified or wrong shape.\n  \"\"\""}, "8c63b19f3703fead7ed046b942de1f82594f58fa": {"bleu": 0.7858424388201066, "jaccard": 0.88671875, "edit_distance": 0.768860353130016, "comment_cnt": 1, "max_sim": 0.88671875, "header": "\n\"\"\"Image preprocessing helpers.\"\"\"\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport cv2\nfrom scipy import ndimage\nimport tensorflow as tf\nfrom tensorflow.python.ops import control_flow_ops\n\n\ndef distort_color(image, color_ordering=0, fast_mode=True, scope=None):\n  \"\"\"Distort the color of a Tensor image.\n\n  TODO(coreylynch): add as a dependency, when slim or tensorflow/models are\n  pipfied.\n  Source:\n  https://raw.githubusercontent.com/tensorflow/models/a9d0e6e8923a4/slim/preprocessing/inception_preprocessing.py\n\n  Each color distortion is non-commutative and thus ordering of the color ops\n  matters. Ideally we would randomly permute the ordering of the color ops.\n  Rather then adding that level of complication, we select a distinct ordering\n  of color ops for each preprocessing thread.\n  Args:\n    image: 3-D Tensor containing single image in [0, 1].\n    color_ordering: Python int, a type of distortion (valid values: 0-3).\n    fast_mode: Avoids slower ops (random_hue and random_contrast)\n    scope: Optional scope for name_scope.\n  Returns:\n    3-D Tensor color-distorted image on range [0, 1]\n  Raises:\n    ValueError: if color_ordering not in [0, 3]\n  \"\"\"", "body": "  with tf.name_scope(scope, 'distort_color', [image]):\n    if fast_mode:\n      if color_ordering == 0:\n        image = tf.image.random_brightness(image, max_delta=32. / 255.)\n        image = tf.image.random_saturation(image, lower=0.5, upper=1.5)\n      else:\n        image = tf.image.random_saturation(image, lower=0.5, upper=1.5)\n        image = tf.image.random_brightness(image, max_delta=32. / 255.)\n    else:\n      if color_ordering == 0:\n        image = tf.image.random_brightness(image, max_delta=32. / 255.)\n        image = tf.image.random_saturation(image, lower=0.5, upper=1.5)\n        image = tf.image.random_hue(image, max_delta=0.2)\n        image = tf.image.random_contrast(image, lower=0.5, upper=1.5)\n      elif color_ordering == 1:\n        image = tf.image.random_saturation(image, lower=0.5, upper=1.5)\n        image = tf.image.random_brightness(image, max_delta=32. / 255.)\n        image = tf.image.random_contrast(image, lower=0.5, upper=1.5)\n        image = tf.image.random_hue(image, max_delta=0.2)\n      elif color_ordering == 2:\n        image = tf.image.random_contrast(image, lower=0.5, upper=1.5)\n        image = tf.image.random_hue(image, max_delta=0.2)\n        image = tf.image.random_brightness(image, max_delta=32. / 255.)\n        image = tf.image.random_saturation(image, lower=0.5, upper=1.5)\n      elif color_ordering == 3:\n        image = tf.image.random_hue(image, max_delta=0.2)\n        image = tf.image.random_saturation(image, lower=0.5, upper=1.5)\n        image = tf.image.random_contrast(image, lower=0.5, upper=1.5)\n        image = tf.image.random_brightness(image, max_delta=32. / 255.)\n      else:\n        raise ValueError('color_ordering must be in [0, 3]')\n\n    # The random_* ops do not necessarily clamp.\n    return tf.clip_by_value(image, 0.0, 1.0)", "answer": "with tf.name_scope(scope, 'distort_color', [image]):\n    if color_ordering == 0:\n      image = tf.image.random_brightness(image, max_delta=32. / 255.)\n      image = tf.image.random_saturation(image, lower=0.5, upper=1.5)\n      image = tf.image.random_hue(image, max_delta=0.2)\n      image = tf.image.random_contrast(image, lower=0.5, upper=1.5)\n    elif color_ordering == 1:\n      image = tf.image.random_saturation(image, lower=0.5, upper=1.5)\n      image = tf.image.random_brightness(image, max_delta=32. / 255.)\n      image = tf.image.random_contrast(image, lower=0.5, upper=1.5)\n      image = tf.image.random_hue(image, max_delta=0.2)\n    elif color_ordering == 2:\n      image = tf.image.random_contrast(image, lower=0.5, upper=1.5)\n      image = tf.image.random_hue(image, max_delta=0.2)\n      image = tf.image.random_brightness(image, max_delta=32. / 255.)\n      image = tf.image.random_saturation(image, lower=0.5, upper=1.5)\n    elif color_ordering == 3:\n      image = tf.image.random_hue(image, max_delta=0.2)\n      image = tf.image.random_saturation(image, lower=0.5, upper=1.5)\n      image = tf.image.random_contrast(image, lower=0.5, upper=1.5)\n      image = tf.image.random_brightness(image, max_delta=32. / 255.)\n    else:\n      raise ValueError('color_ordering must be in [0, 3]')\n\n    # The random_* ops do not necessarily clamp.\n    return tf.clip_by_value(image, 0.0, 1.0)", "license": "apache-2.0", "author": "Corey Lynch <coreylynch@google.com", "project": "tensorflow_models", "signature": "def distort_color(image, color_ordering=0, fast_mode=True, scope=None):", "license_type": "Permissive", "init_header": "# Copyright 2017 The TensorFlow Authors All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\n\"\"\"Image preprocessing helpers.\"\"\"\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport cv2\nfrom scipy import ndimage\nimport tensorflow as tf\nfrom tensorflow.python.ops import control_flow_ops\n\n\ndef distort_color(image, color_ordering=0, fast_mode=True, scope=None):\n  \"\"\"Distort the color of a Tensor image.\n\n  TODO(coreylynch): add as a dependency, when slim or tensorflow/models are\n  pipfied.\n  Source:\n  https://raw.githubusercontent.com/tensorflow/models/a9d0e6e8923a4/slim/preprocessing/inception_preprocessing.py\n\n  Each color distortion is non-commutative and thus ordering of the color ops\n  matters. Ideally we would randomly permute the ordering of the color ops.\n  Rather then adding that level of complication, we select a distinct ordering\n  of color ops for each preprocessing thread.\n  Args:\n    image: 3-D Tensor containing single image in [0, 1].\n    color_ordering: Python int, a type of distortion (valid values: 0-3).\n    fast_mode: Avoids slower ops (random_hue and random_contrast)\n    scope: Optional scope for name_scope.\n  Returns:\n    3-D Tensor color-distorted image on range [0, 1]\n  Raises:\n    ValueError: if color_ordering not in [0, 3]\n  \"\"\"", "docstring": "  \"\"\"Distort the color of a Tensor image.\n\n  TODO(coreylynch): add as a dependency, when slim or tensorflow/models are\n  pipfied.\n  Source:\n  https://raw.githubusercontent.com/tensorflow/models/a9d0e6e8923a4/slim/preprocessing/inception_preprocessing.py\n\n  Each color distortion is non-commutative and thus ordering of the color ops\n  matters. Ideally we would randomly permute the ordering of the color ops.\n  Rather then adding that level of complication, we select a distinct ordering\n  of color ops for each preprocessing thread.\n  Args:\n    image: 3-D Tensor containing single image in [0, 1].\n    color_ordering: Python int, a type of distortion (valid values: 0-3).\n    fast_mode: Avoids slower ops (random_hue and random_contrast)\n    scope: Optional scope for name_scope.\n  Returns:\n    3-D Tensor color-distorted image on range [0, 1]\n  Raises:\n    ValueError: if color_ordering not in [0, 3]\n  \"\"\""}, "b9b6837193dce19466fbd8d3920b7b734ca155e5": {"bleu": 0.9382783680411474, "jaccard": 0.8203125, "edit_distance": 0.9554455445544554, "comment_cnt": 2, "max_sim": 0.9554455445544554, "header": "\"\"\"PyTorch BERT model.\"\"\"\n\nfrom __future__ import absolute_import, division, print_function, unicode_literals\n\nimport copy\nimport json\nimport logging\nimport math\nimport os\nimport shutil\nimport tarfile\nimport tempfile\nimport sys\nfrom io import open\n\nimport torch\nfrom torch import nn\nfrom torch.nn import CrossEntropyLoss\n\nfrom .file_utils import cached_path, WEIGHTS_NAME, CONFIG_NAME\n\nlogger = logging.getLogger(__name__)\n\nPRETRAINED_MODEL_ARCHIVE_MAP = {\n    'bert-base-uncased': \"https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased.tar.gz\",\n    'bert-large-uncased': \"https://s3.amazonaws.com/models.huggingface.co/bert/bert-large-uncased.tar.gz\",\n    'bert-base-cased': \"https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-cased.tar.gz\",\n    'bert-large-cased': \"https://s3.amazonaws.com/models.huggingface.co/bert/bert-large-cased.tar.gz\",\n    'bert-base-multilingual-uncased': \"https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-uncased.tar.gz\",\n    'bert-base-multilingual-cased': \"https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-cased.tar.gz\",\n    'bert-base-chinese': \"https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-chinese.tar.gz\",\n}\nBERT_CONFIG_NAME = 'bert_config.json'\nTF_WEIGHTS_NAME = 'model.ckpt'\n\ndef load_tf_weights_in_bert(model, tf_checkpoint_path):\n    \"\"\" Load tf checkpoints in a pytorch model\n    \"\"\"", "body": "    try:\n        import re\n        import numpy as np\n        import tensorflow as tf\n    except ImportError:\n        print(\"Loading a TensorFlow models in PyTorch, requires TensorFlow to be installed. Please see \"\n            \"https://www.tensorflow.org/install/ for installation instructions.\")\n        raise\n    tf_path = os.path.abspath(tf_checkpoint_path)\n    print(\"Converting TensorFlow checkpoint from {}\".format(tf_path))\n    # Load weights from TF model\n    init_vars = tf.train.list_variables(tf_path)\n    names = []\n    arrays = []\n    for name, shape in init_vars:\n        print(\"Loading TF weight {} with shape {}\".format(name, shape))\n        array = tf.train.load_variable(tf_path, name)\n        names.append(name)\n        arrays.append(array)\n\n    for name, array in zip(names, arrays):\n        name = name.split('/')\n        # adam_v and adam_m are variables used in AdamWeightDecayOptimizer to calculated m and v\n        # which are not required for using pretrained model\n        if any(n in [\"adam_v\", \"adam_m\", \"global_step\"] for n in name):\n            print(\"Skipping {}\".format(\"/\".join(name)))\n            continue\n        pointer = model\n        for m_name in name:\n            if re.fullmatch(r'[A-Za-z]+_\\d+', m_name):\n                l = re.split(r'_(\\d+)', m_name)\n            else:\n                l = [m_name]\n            if l[0] == 'kernel' or l[0] == 'gamma':\n                pointer = getattr(pointer, 'weight')\n            elif l[0] == 'output_bias' or l[0] == 'beta':\n                pointer = getattr(pointer, 'bias')\n            elif l[0] == 'output_weights':\n                pointer = getattr(pointer, 'weight')\n            elif l[0] == 'squad':\n                pointer = getattr(pointer, 'classifier')\n            else:\n                try:\n                    pointer = getattr(pointer, l[0])\n                except AttributeError:\n                    print(\"Skipping {}\".format(\"/\".join(name)))\n                    continue\n            if len(l) >= 2:\n                num = int(l[1])\n                pointer = pointer[num]\n        if m_name[-11:] == '_embeddings':\n            pointer = getattr(pointer, 'weight')\n        elif m_name == 'kernel':\n            array = np.transpose(array)\n        try:\n            assert pointer.shape == array.shape\n        except AssertionError as e:\n            e.args += (pointer.shape, array.shape)\n            raise\n        print(\"Initialize PyTorch weight {}\".format(name))\n        pointer.data = torch.from_numpy(array)\n    return model", "answer": "try:\n        import re\n        import numpy as np\n        import tensorflow as tf\n    except ImportError:\n        logger.error(\"Loading a TensorFlow model in PyTorch, requires TensorFlow to be installed. Please see \"\n                     \"https://www.tensorflow.org/install/ for installation instructions.\")\n        raise\n\n    tf_path = os.path.abspath(tf_checkpoint_path)\n    logger.info(\"Converting TensorFlow checkpoint from {}\".format(tf_path))\n    # Load weights from TF model\n    init_vars = tf.train.list_variables(tf_path)\n    names = []\n    arrays = []\n    for name, shape in init_vars:\n        logger.info(\"Loading TF weight {} with shape {}\".format(name, shape))\n        array = tf.train.load_variable(tf_path, name)\n        names.append(name)\n        arrays.append(array)\n\n    for name, array in zip(names, arrays):\n        name = name.split('/')\n        # adam_v and adam_m are variables used in AdamWeightDecayOptimizer to calculate m and v\n        # which are not required for using pretrained model\n        if any(n in [\"adam_v\", \"adam_m\"] for n in name):\n            logger.info(\"Skipping {}\".format(\"/\".join(name)))\n            continue\n        pointer = model\n        for m_name in name:\n            if re.fullmatch(r'[A-Za-z]+_\\d+', m_name):\n                l = re.split(r'_(\\d+)', m_name)\n            else:\n                l = [m_name]\n            if l[0] == 'kernel' or l[0] == 'gamma':\n                pointer = getattr(pointer, 'weight')\n            elif l[0] == 'output_bias' or l[0] == 'beta':\n                pointer = getattr(pointer, 'bias')\n            elif l[0] == 'output_weights':\n                pointer = getattr(pointer, 'weight')\n            elif l[0] == 'squad':\n                pointer = getattr(pointer, 'classifier')\n            else:\n                try:\n                    pointer = getattr(pointer, l[0])\n                except AttributeError:\n                    logger.info(\"Skipping {}\".format(\"/\".join(name)))\n                    continue\n            if len(l) >= 2:\n                num = int(l[1])\n                pointer = pointer[num]\n        if m_name[-11:] == '_embeddings':\n            pointer = getattr(pointer, 'weight')\n        elif m_name == 'kernel':\n            array = np.transpose(array)\n        try:\n            assert pointer.shape == array.shape\n        except AssertionError as e:\n            e.args += (pointer.shape, array.shape)\n            raise\n        logger.info(\"Initialize PyTorch weight {}\".format(name))\n        pointer.data = torch.from_numpy(array)\n    return model", "license": "apache-2.0", "author": "Thomas Wolf <thomwolf@users.noreply.github.com", "project": "gitlab.com_armbiant_pytorch-nlp", "signature": "def load_tf_weights_in_bert(model, tf_checkpoint_path):", "license_type": "Permissive", "init_header": "# coding=utf-8\n# Copyright 2018 The Google AI Language Team Authors and The HuggingFace Inc. team.\n# Copyright (c) 2018, NVIDIA CORPORATION.  All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\"\"\"PyTorch BERT model.\"\"\"\n\nfrom __future__ import absolute_import, division, print_function, unicode_literals\n\nimport copy\nimport json\nimport logging\nimport math\nimport os\nimport shutil\nimport tarfile\nimport tempfile\nimport sys\nfrom io import open\n\nimport torch\nfrom torch import nn\nfrom torch.nn import CrossEntropyLoss\n\nfrom .file_utils import cached_path, WEIGHTS_NAME, CONFIG_NAME\n\nlogger = logging.getLogger(__name__)\n\nPRETRAINED_MODEL_ARCHIVE_MAP = {\n    'bert-base-uncased': \"https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased.tar.gz\",\n    'bert-large-uncased': \"https://s3.amazonaws.com/models.huggingface.co/bert/bert-large-uncased.tar.gz\",\n    'bert-base-cased': \"https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-cased.tar.gz\",\n    'bert-large-cased': \"https://s3.amazonaws.com/models.huggingface.co/bert/bert-large-cased.tar.gz\",\n    'bert-base-multilingual-uncased': \"https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-uncased.tar.gz\",\n    'bert-base-multilingual-cased': \"https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-cased.tar.gz\",\n    'bert-base-chinese': \"https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-chinese.tar.gz\",\n}\nBERT_CONFIG_NAME = 'bert_config.json'\nTF_WEIGHTS_NAME = 'model.ckpt'\n\ndef load_tf_weights_in_bert(model, tf_checkpoint_path):\n    \"\"\" Load tf checkpoints in a pytorch model\n    \"\"\"", "docstring": "    \"\"\" Load tf checkpoints in a pytorch model\n    \"\"\""}, "1e7d53acf8e9b8e0df690efccc444e69c8972813": {"bleu": 0.6657758844428496, "jaccard": 0.3515625, "edit_distance": 0.5426356589147288, "comment_cnt": 2, "max_sim": 0.6657758844428496, "header": "\n\"\"\"Simple file-based sample for the Google Assistant Service.\"\"\"\n\nimport json\nimport logging\nimport os\nimport os.path\nimport sys\n\nimport click\nimport google.auth.transport.grpc\nimport google.auth.transport.requests\nimport google.oauth2.credentials\n\nfrom google.assistant.embedded.v1alpha2 import (\n    embedded_assistant_pb2,\n    embedded_assistant_pb2_grpc\n)\n\n\nEND_OF_UTTERANCE = embedded_assistant_pb2.AssistResponse.END_OF_UTTERANCE\n\n\n@click.command()\n@click.option('--api-endpoint', default='embeddedassistant.googleapis.com',\n              metavar='<api endpoint>', show_default=True,\n              help='Address of Google Assistant API service.')\n@click.option('--credentials',\n              metavar='<credentials>', show_default=True,\n              default=os.path.join(click.get_app_dir('google-oauthlib-tool'),\n                                   'credentials.json'),\n              help='Path to read OAuth2 credentials.')\n@click.option('--device-model-id', required=True,\n              metavar='<device model id>',\n              help='Unique device model identifier.')\n@click.option('--device-id', required=True,\n              metavar='<device id>',\n              help='Unique registered device instance identifier.')\n@click.option('--lang', show_default=True,\n              metavar='<language code>',\n              default='en-US',\n              help='Language code of the Assistant.')\n@click.option('--verbose', '-v', is_flag=True, default=False,\n              help='Enable verbose logging.')\n@click.option('--input-audio-file', '-i', required=True,\n              metavar='<input file>', type=click.File('rb'),\n              help='Path to input audio file (format: LINEAR16 16000 Hz).')\n@click.option('--output-audio-file', '-o', required=True,\n              metavar='<output file>', type=click.File('wb'),\n              help='Path to output audio file (format: LINEAR16 16000 Hz).')\n@click.option('--block-size', default=1024,\n              metavar='<block size>', show_default=True,\n              help='Size of each input stream read in bytes.')\n@click.option('--grpc-deadline', default=300,\n              metavar='<grpc deadline>', show_default=True,\n              help='gRPC deadline in seconds')\ndef main(api_endpoint, credentials,\n         device_model_id, device_id, lang, verbose,\n         input_audio_file, output_audio_file,\n         block_size, grpc_deadline, *args, **kwargs):\n    \"\"\"File based sample for the Google Assistant API.\n\n    Examples:\n      $ python -m audiofileinput -i <input file> -o <output file>\n    \"\"\"", "body": "    # Setup logging.\n    logging.basicConfig(level=logging.DEBUG if verbose else logging.INFO)\n\n    # Load OAuth 2.0 credentials.\n    try:\n        with open(credentials, 'r') as f:\n            credentials = google.oauth2.credentials.Credentials(token=None,\n                                                                **json.load(f))\n            http_request = google.auth.transport.requests.Request()\n            credentials.refresh(http_request)\n    except Exception as e:\n        logging.error('Error loading credentials: %s', e)\n        logging.error('Run google-oauthlib-tool to initialize '\n                      'new OAuth 2.0 credentials.')\n        sys.exit(-1)\n\n    # Create an authorized gRPC channel.\n    grpc_channel = google.auth.transport.grpc.secure_authorized_channel(\n        credentials, http_request, api_endpoint)\n    logging.info('Connecting to %s', api_endpoint)\n\n    # Create gRPC stubs\n    assistant = embedded_assistant_pb2_grpc.EmbeddedAssistantStub(grpc_channel)\n\n    # Generate gRPC requests.\n    def gen_assist_requests(input_stream):\n        dialog_state_in = embedded_assistant_pb2.DialogStateIn(\n            language_code=lang,\n            conversation_state=b''\n        )\n        config = embedded_assistant_pb2.AssistConfig(\n            audio_in_config=embedded_assistant_pb2.AudioInConfig(\n                encoding='LINEAR16',\n                sample_rate_hertz=16000,\n            ),\n            audio_out_config=embedded_assistant_pb2.AudioOutConfig(\n                encoding='LINEAR16',\n                sample_rate_hertz=16000,\n                volume_percentage=100,\n            ),\n            dialog_state_in=dialog_state_in,\n            device_config=embedded_assistant_pb2.DeviceConfig(\n                device_id=device_id,\n                device_model_id=device_model_id,\n            )\n        )\n        # Send first AssistRequest message with configuration.\n        yield embedded_assistant_pb2.AssistRequest(config=config)\n        while True:\n            # Read user request from file.\n            data = input_stream.read(block_size)\n            if not data:\n                break\n            # Send following AssitRequest message with audio chunks.\n            yield embedded_assistant_pb2.AssistRequest(audio_in=data)\n\n    for resp in assistant.Assist(gen_assist_requests(input_audio_file),\n                                 grpc_deadline):\n        # Iterate on AssistResponse messages.\n        if resp.event_type == END_OF_UTTERANCE:\n            logging.info('End of audio request detected')\n        if resp.speech_results:\n            logging.info('Transcript of user request: \"%s\".',\n                         ' '.join(r.transcript\n                                  for r in resp.speech_results))\n        if len(resp.audio_out.audio_data) > 0:\n            # Write assistant response to supplied file.\n            output_audio_file.write(resp.audio_out.audio_data)\n        if resp.dialog_state_out.supplemental_display_text:\n            logging.info('Assistant display text: \"%s\"',\n                         resp.dialog_state_out.supplemental_display_text)\n        if resp.device_action.device_request_json:\n            device_request = json.loads(resp.device_action.device_request_json)\n            logging.info('Device request: %s', device_request)", "answer": "if verbose:\n        logging.basicConfig(level=logging.DEBUG)\n    else:\n        logging.basicConfig(level=logging.INFO)\n\n    # Load OAuth 2.0 credentials.\n    try:\n        with open(credentials, 'r') as f:\n            credentials_data = json.load(f)\n            credentials = google.oauth2.credentials.Credentials(token=None, **credentials_data)\n    except Exception as e:\n        logging.error('Error loading credentials: %s', e)\n        sys.exit(-1)\n\n    # Create an authorized gRPC channel.\n    http_request = google.auth.transport.requests.Request()\n    try:\n        grpc_channel = google.auth.transport.grpc.secure_authorized_channel(\n            credentials, http_request, api_endpoint)\n        logging.info('Connecting to %s', api_endpoint)\n    except Exception as e:\n        logging.error('Error creating gRPC channel: %s', e)\n        sys.exit(-1)\n\n    # Create Google Assistant API client.\n    assistant = embedded_assistant_pb2_grpc.EmbeddedAssistantStub(grpc_channel)\n\n    def gen_assist_requests():\n        \"\"\"Yields: AssistRequest messages to send to the API.\"\"\"\n        config = embedded_assistant_pb2.AssistConfig(\n            audio_in_config=embedded_assistant_pb2.AudioInConfig(\n                encoding='LINEAR16',\n                sample_rate_hertz=16000,\n            ),\n            audio_out_config=embedded_assistant_pb2.AudioOutConfig(\n                encoding='LINEAR16',\n                sample_rate_hertz=16000,\n                volume_percentage=100,\n            ),\n            dialog_state_in=embedded_assistant_pb2.DialogStateIn(\n                language_code=lang,\n            ),\n            device_config=embedded_assistant_pb2.DeviceConfig(\n                device_id=device_id,\n                device_model_id=device_model_id,\n            )\n        )\n        yield embedded_assistant_pb2.AssistRequest(config=config)\n        while True:\n            data = input_audio_file.read(block_size)\n            if not data:\n                break\n            yield embedded_assistant_pb2.AssistRequest(audio_in=data)\n\n    # Send requests and receive responses.\n    try:\n        for resp in assistant.Assist(gen_assist_requests(), grpc_deadline):\n            if resp.event_type == END_OF_UTTERANCE:\n                logging.info('End of utterance detected')\n                break\n            if resp.speech_results:\n                logging.info('Transcript of user request: \"%s\".', ' '.join(r.transcript for r in resp.speech_results))\n            if resp.audio_out.audio_data:\n                output_audio_file.write(resp.audio_out.audio_data)\n    except Exception as e:\n        logging.error('Error during API call: %s', e)\n        sys.exit(-1)\n\n    logging.info('Finished processing the audio file.')\n\nif", "license": "apache-2.0", "author": "Johan Euphrosine <proppy@google.com", "project": "googlesamples_assistant-sdk-python", "signature": "def main(api_endpoint, credentials,", "license_type": "Permissive", "init_header": "# Copyright (C) 2018 Google Inc.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\"\"\"Simple file-based sample for the Google Assistant Service.\"\"\"\n\nimport json\nimport logging\nimport os\nimport os.path\nimport sys\n\nimport click\nimport google.auth.transport.grpc\nimport google.auth.transport.requests\nimport google.oauth2.credentials\n\nfrom google.assistant.embedded.v1alpha2 import (\n    embedded_assistant_pb2,\n    embedded_assistant_pb2_grpc\n)\n\n\nEND_OF_UTTERANCE = embedded_assistant_pb2.AssistResponse.END_OF_UTTERANCE\n\n\n@click.command()\n@click.option('--api-endpoint', default='embeddedassistant.googleapis.com',\n              metavar='<api endpoint>', show_default=True,\n              help='Address of Google Assistant API service.')\n@click.option('--credentials',\n              metavar='<credentials>', show_default=True,\n              default=os.path.join(click.get_app_dir('google-oauthlib-tool'),\n                                   'credentials.json'),\n              help='Path to read OAuth2 credentials.')\n@click.option('--device-model-id', required=True,\n              metavar='<device model id>',\n              help='Unique device model identifier.')\n@click.option('--device-id', required=True,\n              metavar='<device id>',\n              help='Unique registered device instance identifier.')\n@click.option('--lang', show_default=True,\n              metavar='<language code>',\n              default='en-US',\n              help='Language code of the Assistant.')\n@click.option('--verbose', '-v', is_flag=True, default=False,\n              help='Enable verbose logging.')\n@click.option('--input-audio-file', '-i', required=True,\n              metavar='<input file>', type=click.File('rb'),\n              help='Path to input audio file (format: LINEAR16 16000 Hz).')\n@click.option('--output-audio-file', '-o', required=True,\n              metavar='<output file>', type=click.File('wb'),\n              help='Path to output audio file (format: LINEAR16 16000 Hz).')\n@click.option('--block-size', default=1024,\n              metavar='<block size>', show_default=True,\n              help='Size of each input stream read in bytes.')\n@click.option('--grpc-deadline', default=300,\n              metavar='<grpc deadline>', show_default=True,\n              help='gRPC deadline in seconds')\ndef main(api_endpoint, credentials,\n         device_model_id, device_id, lang, verbose,\n         input_audio_file, output_audio_file,\n         block_size, grpc_deadline, *args, **kwargs):\n    \"\"\"File based sample for the Google Assistant API.\n\n    Examples:\n      $ python -m audiofileinput -i <input file> -o <output file>\n    \"\"\"", "docstring": "         device_model_id, device_id, lang, verbose,\n         input_audio_file, output_audio_file,\n         block_size, grpc_deadline, *args, **kwargs):\n    \"\"\"File based sample for the Google Assistant API.\n\n    Examples:\n      $ python -m audiofileinput -i <input file> -o <output file>\n    \"\"\""}, "b174956e6041db918aa4b8f5a391bfbc60aa6bda": {"bleu": 1.0, "jaccard": 1.0, "edit_distance": 0.963963963963964, "comment_cnt": 1, "max_sim": 1.0, "header": "\n\"\"\"RNN helpers for TensorFlow models.\n\n\n@@bidirectional_dynamic_rnn\n@@dynamic_rnn\n@@raw_rnn\n@@static_rnn\n@@static_state_saving_rnn\n@@static_bidirectional_rnn\n\"\"\"\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nfrom tensorflow.python.framework import constant_op\nfrom tensorflow.python.framework import dtypes\nfrom tensorflow.python.framework import ops\nfrom tensorflow.python.framework import tensor_shape\nfrom tensorflow.python.ops import array_ops\nfrom tensorflow.python.ops import control_flow_ops\nfrom tensorflow.python.ops import math_ops\nfrom tensorflow.python.ops import rnn_cell_impl\nfrom tensorflow.python.ops import tensor_array_ops\nfrom tensorflow.python.ops import variable_scope as vs\nfrom tensorflow.python.util import nest\n\n\n_concat = rnn_cell_impl._concat\n_like_rnncell = rnn_cell_impl._like_rnncell\n\n\ndef _best_effort_input_batch_size(flat_input):\n  \"\"\"Get static input batch size if available, with fallback to the dynamic one.\n\n  Args:\n    flat_input: An iterable of time major input Tensors of shape [max_time,\n      batch_size, ...]. All inputs should have compatible batch sizes.\n\n  Returns:\n    The batch size in Python integer if available, or a scalar Tensor otherwise.\n\n  Raises:\n    ValueError: if there is any input with an invalid shape.\n  \"\"\"", "body": "  for input_ in flat_input:\n    shape = input_.shape\n    if shape.ndims is None:\n      continue\n    if shape.ndims < 2:\n      raise ValueError(\n          \"Expected input tensor %s to have rank at least 2\" % input_)\n    batch_size = shape[1].value\n    if batch_size is not None:\n      return batch_size\n  # Fallback to the dynamic batch size of the first input.\n  return array_ops.shape(flat_input[0])[1]", "answer": "for input_ in flat_input:\n    shape = input_.shape\n    if shape.ndims is None:\n      continue\n    if shape.ndims < 2:\n      raise ValueError(\"Expected input tensor %s to have rank at least 2\" % input_)\n    batch_size = shape[1].value\n    if batch_size is not None:\n      return batch_size\n\n  # Fallback to the dynamic batch size of the first input.\n  return array_ops.shape(flat_input[0])[1]", "license": "apache-2.0", "author": "A. Unique TensorFlower <gardener@tensorflow.org", "project": "tensorflow_tensorflow", "signature": "def _best_effort_input_batch_size(flat_input):", "license_type": "Permissive", "init_header": "# Copyright 2015 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\n\"\"\"RNN helpers for TensorFlow models.\n\n\n@@bidirectional_dynamic_rnn\n@@dynamic_rnn\n@@raw_rnn\n@@static_rnn\n@@static_state_saving_rnn\n@@static_bidirectional_rnn\n\"\"\"\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nfrom tensorflow.python.framework import constant_op\nfrom tensorflow.python.framework import dtypes\nfrom tensorflow.python.framework import ops\nfrom tensorflow.python.framework import tensor_shape\nfrom tensorflow.python.ops import array_ops\nfrom tensorflow.python.ops import control_flow_ops\nfrom tensorflow.python.ops import math_ops\nfrom tensorflow.python.ops import rnn_cell_impl\nfrom tensorflow.python.ops import tensor_array_ops\nfrom tensorflow.python.ops import variable_scope as vs\nfrom tensorflow.python.util import nest\n\n\n# pylint: disable=protected-access\n_concat = rnn_cell_impl._concat\n_like_rnncell = rnn_cell_impl._like_rnncell\n# pylint: enable=protected-access\n\n\ndef _best_effort_input_batch_size(flat_input):\n  \"\"\"Get static input batch size if available, with fallback to the dynamic one.\n\n  Args:\n    flat_input: An iterable of time major input Tensors of shape [max_time,\n      batch_size, ...]. All inputs should have compatible batch sizes.\n\n  Returns:\n    The batch size in Python integer if available, or a scalar Tensor otherwise.\n\n  Raises:\n    ValueError: if there is any input with an invalid shape.\n  \"\"\"", "docstring": "  \"\"\"Get static input batch size if available, with fallback to the dynamic one.\n\n  Args:\n    flat_input: An iterable of time major input Tensors of shape [max_time,\n      batch_size, ...]. All inputs should have compatible batch sizes.\n\n  Returns:\n    The batch size in Python integer if available, or a scalar Tensor otherwise.\n\n  Raises:\n    ValueError: if there is any input with an invalid shape.\n  \"\"\""}, "e68c171d9c7e33d7e932f5d5b7f15859faa2348b": {"bleu": 0.8075891982357138, "jaccard": 0.52734375, "edit_distance": 0.6799999999999999, "comment_cnt": 1, "max_sim": 0.8075891982357138, "header": "\"\"\"Utilities for preprocessing sequence data.\n\"\"\"\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport random\n\nimport numpy as np\nfrom six.moves import range  # pylint: disable=redefined-builtin\n\nfrom tensorflow.python.keras._impl.keras.utils.data_utils import Sequence\nfrom tensorflow.python.util.tf_export import tf_export\n\n\n@tf_export('keras.preprocessing.sequence.pad_sequences')\ndef pad_sequences(sequences,\n                  maxlen=None,\n                  dtype='int32',\n                  padding='pre',\n                  truncating='pre',\n                  value=0.):\n  \"\"\"Pads sequences to the same length.\n\n  This function transforms a list of\n  `num_samples` sequences (lists of integers)\n  into a 2D Numpy array of shape `(num_samples, num_timesteps)`.\n  `num_timesteps` is either the `maxlen` argument if provided,\n  or the length of the longest sequence otherwise.\n\n  Sequences that are shorter than `num_timesteps`\n  are padded with `value` at the end.\n\n  Sequences longer than `num_timesteps` are truncated\n  so that they fit the desired length.\n  The position where padding or truncation happens is determined by\n  the arguments `padding` and `truncating`, respectively.\n\n  Pre-padding is the default.\n\n  Arguments:\n      sequences: List of lists, where each element is a sequence.\n      maxlen: Int, maximum length of all sequences.\n      dtype: Type of the output sequences.\n      padding: String, 'pre' or 'post':\n          pad either before or after each sequence.\n      truncating: String, 'pre' or 'post':\n          remove values from sequences larger than\n          `maxlen`, either at the beginning or at the end of the sequences.\n      value: Float, padding value.\n\n  Returns:\n      x: Numpy array with shape `(len(sequences), maxlen)`\n\n  Raises:\n      ValueError: In case of invalid values for `truncating` or `padding`,\n          or in case of invalid shape for a `sequences` entry.\n  \"\"\"", "body": "  if not hasattr(sequences, '__len__'):\n    raise ValueError('`sequences` must be iterable.')\n  lengths = []\n  for x in sequences:\n    if not hasattr(x, '__len__'):\n      raise ValueError('`sequences` must be a list of iterables. '\n                       'Found non-iterable: ' + str(x))\n    lengths.append(len(x))\n\n  num_samples = len(sequences)\n  if maxlen is None:\n    maxlen = np.max(lengths)\n\n  # take the sample shape from the first non empty sequence\n  # checking for consistency in the main loop below.\n  sample_shape = tuple()\n  for s in sequences:\n    if len(s) > 0:  # pylint: disable=g-explicit-length-test\n      sample_shape = np.asarray(s).shape[1:]\n      break\n\n  x = (np.ones((num_samples, maxlen) + sample_shape) * value).astype(dtype)\n  for idx, s in enumerate(sequences):\n    if not len(s):  # pylint: disable=g-explicit-length-test\n      continue  # empty list/array was found\n    if truncating == 'pre':\n      trunc = s[-maxlen:]  # pylint: disable=invalid-unary-operand-type\n    elif truncating == 'post':\n      trunc = s[:maxlen]\n    else:\n      raise ValueError('Truncating type \"%s\" not understood' % truncating)\n\n    # check `trunc` has expected shape\n    trunc = np.asarray(trunc, dtype=dtype)\n    if trunc.shape[1:] != sample_shape:\n      raise ValueError('Shape of sample %s of sequence at position %s '\n                       'is different from expected shape %s' %\n                       (trunc.shape[1:], idx, sample_shape))\n\n    if padding == 'post':\n      x[idx, :len(trunc)] = trunc\n    elif padding == 'pre':\n      x[idx, -len(trunc):] = trunc\n    else:\n      raise ValueError('Padding type \"%s\" not understood' % padding)\n  return x", "answer": "if not hasattr(sequences, '__len__'):\n    raise ValueError('`sequences` must be iterable.')\n\n  num_samples = len(sequences)\n\n  lengths = []\n  for s in sequences:\n    if not hasattr(s, '__len__'):\n      raise ValueError('`sequences` must be a list of iterables. '\n                       'Found non-iterable: ' + str(s))\n    lengths.append(len(s))\n\n  if maxlen is None:\n    maxlen = np.max(lengths)\n\n  sample_shape = tuple()\n  for s in sequences:\n    if len(s) > 0:\n      sample_shape = np.asarray(s).shape[1:]\n      break\n\n  is_dtype_str = np.issubdtype(np.dtype(dtype), np.str_) or np.issubdtype(np.dtype(dtype), np.unicode_)\n\n  x = np.full((num_samples, maxlen) + sample_shape, value, dtype=dtype)\n\n  for idx, s in enumerate(sequences):\n    if not len(s):\n      continue  # empty list/array was found\n    if truncating == 'pre':\n      trunc = s[-maxlen:]\n    elif truncating == 'post':\n      trunc = s[:maxlen]\n    else:\n      raise ValueError('Truncating type \"%s\" not understood' % truncating)\n\n    trunc = np.asarray(trunc, dtype=dtype)\n    if trunc.shape[1:] != sample_shape:\n      raise ValueError('Shape of sample %s of sequence at position %s is different from expected shape %s' %\n                       (trunc.shape[1:], idx, sample_shape))\n\n    if padding == 'post':\n      x[idx, :len(trunc)] = trunc\n    elif padding == 'pre':\n      x[idx, -len(trunc):] = trunc\n    else:\n      raise ValueError('Padding type \"%s\" not understood' % padding)\n\n  return x", "license": "apache-2.0", "author": "Pavithra Vijay <psv@google.com", "project": "tensorflow_tensorflow", "signature": "def pad_sequences(sequences,", "license_type": "Permissive", "init_header": "# Copyright 2015 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\"\"\"Utilities for preprocessing sequence data.\n\"\"\"\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport random\n\nimport numpy as np\nfrom six.moves import range  # pylint: disable=redefined-builtin\n\nfrom tensorflow.python.keras._impl.keras.utils.data_utils import Sequence\nfrom tensorflow.python.util.tf_export import tf_export\n\n\n@tf_export('keras.preprocessing.sequence.pad_sequences')\ndef pad_sequences(sequences,\n                  maxlen=None,\n                  dtype='int32',\n                  padding='pre',\n                  truncating='pre',\n                  value=0.):\n  \"\"\"Pads sequences to the same length.\n\n  This function transforms a list of\n  `num_samples` sequences (lists of integers)\n  into a 2D Numpy array of shape `(num_samples, num_timesteps)`.\n  `num_timesteps` is either the `maxlen` argument if provided,\n  or the length of the longest sequence otherwise.\n\n  Sequences that are shorter than `num_timesteps`\n  are padded with `value` at the end.\n\n  Sequences longer than `num_timesteps` are truncated\n  so that they fit the desired length.\n  The position where padding or truncation happens is determined by\n  the arguments `padding` and `truncating`, respectively.\n\n  Pre-padding is the default.\n\n  Arguments:\n      sequences: List of lists, where each element is a sequence.\n      maxlen: Int, maximum length of all sequences.\n      dtype: Type of the output sequences.\n      padding: String, 'pre' or 'post':\n          pad either before or after each sequence.\n      truncating: String, 'pre' or 'post':\n          remove values from sequences larger than\n          `maxlen`, either at the beginning or at the end of the sequences.\n      value: Float, padding value.\n\n  Returns:\n      x: Numpy array with shape `(len(sequences), maxlen)`\n\n  Raises:\n      ValueError: In case of invalid values for `truncating` or `padding`,\n          or in case of invalid shape for a `sequences` entry.\n  \"\"\"", "docstring": "                  maxlen=None,\n                  dtype='int32',\n                  padding='pre',\n                  truncating='pre',\n                  value=0.):\n  \"\"\"Pads sequences to the same length.\n\n  This function transforms a list of\n  `num_samples` sequences (lists of integers)\n  into a 2D Numpy array of shape `(num_samples, num_timesteps)`.\n  `num_timesteps` is either the `maxlen` argument if provided,\n  or the length of the longest sequence otherwise.\n\n  Sequences that are shorter than `num_timesteps`\n  are padded with `value` at the end.\n\n  Sequences longer than `num_timesteps` are truncated\n  so that they fit the desired length.\n  The position where padding or truncation happens is determined by\n  the arguments `padding` and `truncating`, respectively.\n\n  Pre-padding is the default.\n\n  Arguments:\n      sequences: List of lists, where each element is a sequence.\n      maxlen: Int, maximum length of all sequences.\n      dtype: Type of the output sequences.\n      padding: String, 'pre' or 'post':\n          pad either before or after each sequence.\n      truncating: String, 'pre' or 'post':\n          remove values from sequences larger than\n          `maxlen`, either at the beginning or at the end of the sequences.\n      value: Float, padding value.\n\n  Returns:\n      x: Numpy array with shape `(len(sequences), maxlen)`\n\n  Raises:\n      ValueError: In case of invalid values for `truncating` or `padding`,\n          or in case of invalid shape for a `sequences` entry.\n  \"\"\""}, "063c525d98cd91bbd5364dba809436f8aeed8233": {"bleu": 0.7835915752859893, "jaccard": 0.6171875, "edit_distance": 0.7, "comment_cnt": 1, "max_sim": 0.7835915752859893, "header": "\"\"\"PyTorch OpenAI GPT-2 model.\"\"\"\n\nfrom __future__ import absolute_import, division, print_function, unicode_literals\n\nimport collections\nimport copy\nimport json\nimport logging\nimport math\nimport os\nimport shutil\nimport tarfile\nimport tempfile\nimport sys\nfrom io import open\n\nimport torch\nimport torch.nn as nn\nfrom torch.nn import CrossEntropyLoss\nfrom torch.nn.parameter import Parameter\n\nfrom .file_utils import cached_path, CONFIG_NAME, WEIGHTS_NAME\nfrom .modeling import BertLayerNorm as LayerNorm\n\nlogger = logging.getLogger(__name__)\n\nPRETRAINED_MODEL_ARCHIVE_MAP = {\"gpt2\": \"https://s3.amazonaws.com/models.huggingface.co/bert/gpt2-pytorch_model.bin\"}\nPRETRAINED_CONFIG_ARCHIVE_MAP = {\"gpt2\": \"https://s3.amazonaws.com/models.huggingface.co/bert/gpt2-config.json\"}\n\ndef load_tf_weights_in_gpt2(model, gpt2_checkpoint_path):\n    \"\"\" Load tf checkpoints in a pytorch model\n    \"\"\"", "body": "    try:\n        import re\n        import numpy as np\n        import tensorflow as tf\n    except ImportError:\n        print(\"Loading a TensorFlow models in PyTorch, requires TensorFlow to be installed. Please see \"\n            \"https://www.tensorflow.org/install/ for installation instructions.\")\n        raise\n    tf_path = os.path.abspath(gpt2_checkpoint_path)\n    print(\"Converting TensorFlow checkpoint from {}\".format(tf_path))\n    # Load weights from TF model\n    init_vars = tf.train.list_variables(tf_path)\n    names = []\n    arrays = []\n    for name, shape in init_vars:\n        print(\"Loading TF weight {} with shape {}\".format(name, shape))\n        array = tf.train.load_variable(tf_path, name)\n        names.append(name)\n        arrays.append(array.squeeze())\n\n    for name, array in zip(names, arrays):\n        name = name[6:]  # skip \"model/\"\n        name = name.split('/')\n        pointer = model\n        for m_name in name:\n            if re.fullmatch(r'[A-Za-z]+\\d+', m_name):\n                l = re.split(r'(\\d+)', m_name)\n            else:\n                l = [m_name]\n            if l[0] == 'w' or l[0] == 'g':\n                pointer = getattr(pointer, 'weight')\n            elif l[0] == 'b':\n                pointer = getattr(pointer, 'bias')\n            elif l[0] == 'wpe' or l[0] == 'wte':\n                pointer = getattr(pointer, l[0])\n                pointer = getattr(pointer, 'weight')\n            else:\n                pointer = getattr(pointer, l[0])\n            if len(l) >= 2:\n                num = int(l[1])\n                pointer = pointer[num]\n        try:\n            assert pointer.shape == array.shape\n        except AssertionError as e:\n            e.args += (pointer.shape, array.shape)\n            raise\n        print(\"Initialize PyTorch weight {}\".format(name))\n        pointer.data = torch.from_numpy(array)\n    return model", "answer": "try:\n        import tensorflow as tf\n    except ImportError:\n        logger.error(\"Loading a TensorFlow model in PyTorch, requires TensorFlow to be installed. Please see https://www.tensorflow.org/install/ for installation instructions.\")\n        raise\n\n    tf_path = os.path.abspath(gpt2_checkpoint_path)\n    logger.info(\"Converting TensorFlow checkpoint from {}\".format(tf_path))\n\n    # Load weights from TF model\n    init_vars = tf.train.list_variables(tf_path)\n    names = []\n    arrays = []\n    for name, shape in init_vars:\n        logger.info(\"Loading TF weight {} with shape {}\".format(name, shape))\n        array = tf.train.load_variable(tf_path, name)\n        names.append(name)\n        arrays.append(array)\n\n    for name, array in zip(names, arrays):\n        name = name.split('/')\n        if any(n in [\"adam_v\", \"adam_m\", \"global_step\"] for n in name):\n            logger.info(\"Skipping {}\".format(\"/\".join(name)))\n            continue\n        pointer = model\n        for m_name in name:\n            if re.fullmatch(r'[A-Za-z]+\\d+', m_name):\n                l = re.split(r'(\\d+)', m_name)\n            else:\n                l = [m_name]\n            if l[0] == 'w' or l[0] == 'g':\n                pointer = getattr(pointer, 'weight')\n            elif l[0] == 'b':\n                pointer = getattr(pointer, 'bias')\n            else:\n                pointer = getattr(pointer, l[0])\n            if len(l) >= 2:\n                num = int(l[1])\n                pointer = pointer[num]\n        if m_name[-11:] == '_embeddings':\n            pointer = getattr(pointer, 'weight')\n        elif m_name == 'kernel':\n            array = np.transpose(array)\n        try:\n            assert pointer.shape == array.shape\n        except AssertionError as e:\n            e.args += (pointer.shape, array.shape)\n            raise\n        logger.info(\"Initialize PyTorch weight {}\".format(name))\n        pointer.data = torch.from_numpy(array)\n    return model", "license": "apache-2.0", "author": "Thomas Wolf <thomwolf@users.noreply.github.com", "project": "gitlab.com_armbiant_pytorch-nlp", "signature": "def load_tf_weights_in_gpt2(model, gpt2_checkpoint_path):", "license_type": "Permissive", "init_header": "# coding=utf-8\n# Copyright 2018 The OpenAI Team Authors and HuggingFace Inc. team.\n# Copyright (c) 2018, NVIDIA CORPORATION.  All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\"\"\"PyTorch OpenAI GPT-2 model.\"\"\"\n\nfrom __future__ import absolute_import, division, print_function, unicode_literals\n\nimport collections\nimport copy\nimport json\nimport logging\nimport math\nimport os\nimport shutil\nimport tarfile\nimport tempfile\nimport sys\nfrom io import open\n\nimport torch\nimport torch.nn as nn\nfrom torch.nn import CrossEntropyLoss\nfrom torch.nn.parameter import Parameter\n\nfrom .file_utils import cached_path, CONFIG_NAME, WEIGHTS_NAME\nfrom .modeling import BertLayerNorm as LayerNorm\n\nlogger = logging.getLogger(__name__)\n\nPRETRAINED_MODEL_ARCHIVE_MAP = {\"gpt2\": \"https://s3.amazonaws.com/models.huggingface.co/bert/gpt2-pytorch_model.bin\"}\nPRETRAINED_CONFIG_ARCHIVE_MAP = {\"gpt2\": \"https://s3.amazonaws.com/models.huggingface.co/bert/gpt2-config.json\"}\n\ndef load_tf_weights_in_gpt2(model, gpt2_checkpoint_path):\n    \"\"\" Load tf checkpoints in a pytorch model\n    \"\"\"", "docstring": "    \"\"\" Load tf checkpoints in a pytorch model\n    \"\"\""}, "5a3a5cc225bc605dbeb09a2f7791decf0545fedd": {"bleu": 1.0, "jaccard": 1.0, "edit_distance": 0.963963963963964, "comment_cnt": 1, "max_sim": 1.0, "header": "\n\"\"\"RNN helpers for TensorFlow models.\"\"\"\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nfrom tensorflow.python.eager import context\nfrom tensorflow.python.framework import constant_op\nfrom tensorflow.python.framework import dtypes\nfrom tensorflow.python.framework import ops\nfrom tensorflow.python.framework import tensor_shape\nfrom tensorflow.python.framework import tensor_util\nfrom tensorflow.python.keras.engine import base_layer\nfrom tensorflow.python.ops import array_ops\nfrom tensorflow.python.ops import control_flow_ops\nfrom tensorflow.python.ops import control_flow_util\nfrom tensorflow.python.ops import math_ops\nfrom tensorflow.python.ops import rnn_cell_impl\nfrom tensorflow.python.ops import tensor_array_ops\nfrom tensorflow.python.ops import variable_scope as vs\nfrom tensorflow.python.util import nest\nfrom tensorflow.python.util.tf_export import tf_export\n\n\n_concat = rnn_cell_impl._concat\n\n\ndef _best_effort_input_batch_size(flat_input):\n  \"\"\"Get static input batch size if available, with fallback to the dynamic one.\n\n  Args:\n    flat_input: An iterable of time major input Tensors of shape\n      `[max_time, batch_size, ...]`.\n    All inputs should have compatible batch sizes.\n\n  Returns:\n    The batch size in Python integer if available, or a scalar Tensor otherwise.\n\n  Raises:\n    ValueError: if there is any input with an invalid shape.\n  \"\"\"", "body": "  for input_ in flat_input:\n    shape = input_.shape\n    if shape.ndims is None:\n      continue\n    if shape.ndims < 2:\n      raise ValueError(\n          \"Expected input tensor %s to have rank at least 2\" % input_)\n    batch_size = shape[1].value\n    if batch_size is not None:\n      return batch_size\n  # Fallback to the dynamic batch size of the first input.\n  return array_ops.shape(flat_input[0])[1]", "answer": "for input_ in flat_input:\n    shape = input_.shape\n    if shape.ndims is None:\n      continue\n    if shape.ndims < 2:\n      raise ValueError(\"Expected input tensor %s to have rank at least 2\" % input_)\n    batch_size = shape[1].value\n    if batch_size is not None:\n      return batch_size\n\n  # Fallback to the dynamic batch size of the first input.\n  return array_ops.shape(flat_input[0])[1]", "license": "apache-2.0", "author": "devsangwoo <devsangwoo@gmail.com", "project": "devsangwoo_tensor", "signature": "def _best_effort_input_batch_size(flat_input):", "license_type": "Permissive", "init_header": "# Copyright 2015 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\n\"\"\"RNN helpers for TensorFlow models.\"\"\"\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nfrom tensorflow.python.eager import context\nfrom tensorflow.python.framework import constant_op\nfrom tensorflow.python.framework import dtypes\nfrom tensorflow.python.framework import ops\nfrom tensorflow.python.framework import tensor_shape\nfrom tensorflow.python.framework import tensor_util\nfrom tensorflow.python.keras.engine import base_layer\nfrom tensorflow.python.ops import array_ops\nfrom tensorflow.python.ops import control_flow_ops\nfrom tensorflow.python.ops import control_flow_util\nfrom tensorflow.python.ops import math_ops\nfrom tensorflow.python.ops import rnn_cell_impl\nfrom tensorflow.python.ops import tensor_array_ops\nfrom tensorflow.python.ops import variable_scope as vs\nfrom tensorflow.python.util import nest\nfrom tensorflow.python.util.tf_export import tf_export\n\n\n# pylint: disable=protected-access\n_concat = rnn_cell_impl._concat\n# pylint: enable=protected-access\n\n\ndef _best_effort_input_batch_size(flat_input):\n  \"\"\"Get static input batch size if available, with fallback to the dynamic one.\n\n  Args:\n    flat_input: An iterable of time major input Tensors of shape\n      `[max_time, batch_size, ...]`.\n    All inputs should have compatible batch sizes.\n\n  Returns:\n    The batch size in Python integer if available, or a scalar Tensor otherwise.\n\n  Raises:\n    ValueError: if there is any input with an invalid shape.\n  \"\"\"", "docstring": "  \"\"\"Get static input batch size if available, with fallback to the dynamic one.\n\n  Args:\n    flat_input: An iterable of time major input Tensors of shape\n      `[max_time, batch_size, ...]`.\n    All inputs should have compatible batch sizes.\n\n  Returns:\n    The batch size in Python integer if available, or a scalar Tensor otherwise.\n\n  Raises:\n    ValueError: if there is any input with an invalid shape.\n  \"\"\""}, "a57913f7c63c9ad5c9d208cbfcbaffec0eb9a55c": {"bleu": 0.6639102579850357, "jaccard": 0.40625, "edit_distance": 0.7333333333333334, "comment_cnt": 2, "max_sim": 0.7333333333333334, "header": "\n\"\"\"Functions to export object detection inference graph.\"\"\"\nimport logging\nimport os\nimport tensorflow as tf\nfrom tensorflow.python import pywrap_tensorflow\nfrom tensorflow.python.client import session\nfrom tensorflow.python.framework import graph_util\nfrom tensorflow.python.framework import importer\nfrom tensorflow.python.platform import gfile\nfrom tensorflow.python.training import saver as saver_lib\nfrom object_detection.builders import model_builder\nfrom object_detection.core import standard_fields as fields\nfrom object_detection.data_decoders import tf_example_decoder\n\nslim = tf.contrib.slim\n\n\ndef freeze_graph_with_def_protos(\n    input_graph_def,\n    input_saver_def,\n    input_checkpoint,\n    output_node_names,\n    restore_op_name,\n    filename_tensor_name,\n    output_graph,\n    clear_devices,\n    initializer_nodes,\n    variable_names_blacklist=''):\n  \"\"\"Converts all variables in a graph and checkpoint into constants.\"\"\"", "body": "  del restore_op_name, filename_tensor_name  # Unused by updated loading code.\n\n  # 'input_checkpoint' may be a prefix if we're using Saver V2 format\n  if not saver_lib.checkpoint_exists(input_checkpoint):\n    logging.info('Input checkpoint \"' + input_checkpoint + '\" does not exist!')\n    return -1\n\n  if not output_node_names:\n    logging.info('You must supply the name of a node to --output_node_names.')\n    return -1\n\n  # Remove all the explicit device specifications for this node. This helps to\n  # make the graph more portable.\n  if clear_devices:\n    for node in input_graph_def.node:\n      node.device = ''\n\n  _ = importer.import_graph_def(input_graph_def, name='')\n\n  with session.Session() as sess:\n    if input_saver_def:\n      saver = saver_lib.Saver(saver_def=input_saver_def)\n      saver.restore(sess, input_checkpoint)\n    else:\n      var_list = {}\n      reader = pywrap_tensorflow.NewCheckpointReader(input_checkpoint)\n      var_to_shape_map = reader.get_variable_to_shape_map()\n      for key in var_to_shape_map:\n        try:\n          tensor = sess.graph.get_tensor_by_name(key + ':0')\n        except KeyError:\n          # This tensor doesn't exist in the graph (for example it's\n          # 'global_step' or a similar housekeeping element) so skip it.\n          continue\n        var_list[key] = tensor\n      saver = saver_lib.Saver(var_list=var_list)\n      saver.restore(sess, input_checkpoint)\n      if initializer_nodes:\n        sess.run(initializer_nodes)\n\n    variable_names_blacklist = (variable_names_blacklist.split(',') if\n                                variable_names_blacklist else None)\n    output_graph_def = graph_util.convert_variables_to_constants(\n        sess,\n        input_graph_def,\n        output_node_names.split(','),\n        variable_names_blacklist=variable_names_blacklist)\n\n  with gfile.GFile(output_graph, 'wb') as f:\n    f.write(output_graph_def.SerializeToString())\n  logging.info('%d ops in the final graph.', len(output_graph_def.node))", "answer": "if not gfile.Exists(input_checkpoint):\n    raise ValueError(\"Input checkpoint file does not exist: {}\".format(input_checkpoint))\n\n  if not output_node_names:\n    raise ValueError(\"You must supply the name of a node to --output_node_names.\")\n\n  # Remove all the devices from the graph as they are not needed for inference.\n  if clear_devices:\n    for node in input_graph_def.node:\n      node.device = \"\"\n\n  _ = importer.import_graph_def(input_graph_def, name=\"\")\n\n  with session.Session() as sess:\n    if input_saver_def:\n      saver = saver_lib.Saver(saver_def=input_saver_def)\n      saver.restore(sess, input_checkpoint)\n    else:\n      var_list = {}\n      reader = pywrap_tensorflow.NewCheckpointReader(input_checkpoint)\n      var_to_shape_map = reader.get_variable_to_shape_map()\n      for key in var_to_shape_map:\n        try:\n          tensor = sess.graph.get_tensor_by_name(key + \":0\")\n        except KeyError:\n          # This tensor doesn't exist in the graph (for example it's\n          # 'global_step' or a similar housekeeping element) so skip it.\n          continue\n        var_list[key] = tensor\n      saver = saver_lib.Saver(var_list=var_list)\n      saver.restore(sess, input_checkpoint)\n\n    if initializer_nodes:\n      sess.run(initializer_nodes)\n\n    output_graph_def = graph_util.convert_variables_to_constants(\n        sess,\n        input_graph_def,\n        output_node_names.split(\",\"),\n        variable_names_blacklist=variable_names_blacklist.split(\",\") if variable_names_blacklist else None)\n\n  with gfile.GFile(output_graph, \"wb\") as f:\n    f.write(output_graph_def.SerializeToString())\n\n  logging.info(\"%d ops in the final graph.\", len(output_graph_def.node))", "license": "apache-2.0", "author": "Derek Chow <derekjchow@google.com", "project": "tensorflow_models", "signature": "def freeze_graph_with_def_protos(", "license_type": "Permissive", "init_header": "# Copyright 2017 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\n\"\"\"Functions to export object detection inference graph.\"\"\"\nimport logging\nimport os\nimport tensorflow as tf\nfrom tensorflow.python import pywrap_tensorflow\nfrom tensorflow.python.client import session\nfrom tensorflow.python.framework import graph_util\nfrom tensorflow.python.framework import importer\nfrom tensorflow.python.platform import gfile\nfrom tensorflow.python.training import saver as saver_lib\nfrom object_detection.builders import model_builder\nfrom object_detection.core import standard_fields as fields\nfrom object_detection.data_decoders import tf_example_decoder\n\nslim = tf.contrib.slim\n\n\n# TODO: Replace with freeze_graph.freeze_graph_with_def_protos when newer\n# version of Tensorflow becomes more common.\ndef freeze_graph_with_def_protos(\n    input_graph_def,\n    input_saver_def,\n    input_checkpoint,\n    output_node_names,\n    restore_op_name,\n    filename_tensor_name,\n    output_graph,\n    clear_devices,\n    initializer_nodes,\n    variable_names_blacklist=''):\n  \"\"\"Converts all variables in a graph and checkpoint into constants.\"\"\"", "docstring": "    input_graph_def,\n    input_saver_def,\n    input_checkpoint,\n    output_node_names,\n    restore_op_name,\n    filename_tensor_name,\n    output_graph,\n    clear_devices,\n    initializer_nodes,\n    variable_names_blacklist=''):\n  \"\"\"Converts all variables in a graph and checkpoint into constants.\"\"\""}, "adda1f5e564fdcf653d73aa613120175552f417f": {"bleu": 0.8968199460834887, "jaccard": 0.69921875, "edit_distance": 0.9272727272727272, "comment_cnt": 1, "max_sim": 0.9272727272727272, "header": "\"\"\"RNN helpers for TensorFlow models.\"\"\"\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nfrom tensorflow.python.eager import context\nfrom tensorflow.python.framework import constant_op\nfrom tensorflow.python.framework import dtypes\nfrom tensorflow.python.framework import ops\nfrom tensorflow.python.framework import tensor_shape\nfrom tensorflow.python.framework import tensor_util\nfrom tensorflow.python.keras.engine import base_layer\nfrom tensorflow.python.ops import array_ops\nfrom tensorflow.python.ops import control_flow_ops\nfrom tensorflow.python.ops import control_flow_util\nfrom tensorflow.python.ops import math_ops\nfrom tensorflow.python.ops import rnn_cell_impl\nfrom tensorflow.python.ops import tensor_array_ops\nfrom tensorflow.python.ops import variable_scope as vs\nfrom tensorflow.python.util import deprecation\nfrom tensorflow.python.util import nest\nfrom tensorflow.python.util.tf_export import tf_export\n\n_concat = rnn_cell_impl._concat\n\n\ndef _best_effort_input_batch_size(flat_input):\n  \"\"\"Get static input batch size if available, with fallback to the dynamic one.\n\n  Args:\n    flat_input: An iterable of time major input Tensors of shape `[max_time,\n      batch_size, ...]`. All inputs should have compatible batch sizes.\n\n  Returns:\n    The batch size in Python integer if available, or a scalar Tensor otherwise.\n\n  Raises:\n    ValueError: if there is any input with an invalid shape.\n  \"\"\"", "body": "  for input_ in flat_input:\n    shape = input_.shape\n    if shape.rank is None:\n      continue\n    if shape.rank < 2:\n      raise ValueError(\"Expected input tensor %s to have rank at least 2\" %\n                       input_)\n    batch_size = shape.dims[1].value\n    if batch_size is not None:\n      return batch_size\n  # Fallback to the dynamic batch size of the first input.\n  return array_ops.shape(flat_input[0])[1]", "answer": "for input_ in flat_input:\n    shape = input_.shape\n    if shape.ndims is None:\n      continue\n    if shape.ndims < 2:\n      raise ValueError(\"Expected input tensor %s to have rank at least 2\" % input_)\n    batch_size = shape[1].value\n    if batch_size is not None:\n      return batch_size\n\n  # Fallback to the dynamic batch size of the first input.\n  return array_ops.shape(flat_input[0])[1]", "license": "apache-2.0", "author": "Mihai Maruseac <mihaimaruseac@google.com", "project": "tensorflow_tensorflow", "signature": "def _best_effort_input_batch_size(flat_input):", "license_type": "Permissive", "init_header": "# Copyright 2015 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\"\"\"RNN helpers for TensorFlow models.\"\"\"\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nfrom tensorflow.python.eager import context\nfrom tensorflow.python.framework import constant_op\nfrom tensorflow.python.framework import dtypes\nfrom tensorflow.python.framework import ops\nfrom tensorflow.python.framework import tensor_shape\nfrom tensorflow.python.framework import tensor_util\nfrom tensorflow.python.keras.engine import base_layer\nfrom tensorflow.python.ops import array_ops\nfrom tensorflow.python.ops import control_flow_ops\nfrom tensorflow.python.ops import control_flow_util\nfrom tensorflow.python.ops import math_ops\nfrom tensorflow.python.ops import rnn_cell_impl\nfrom tensorflow.python.ops import tensor_array_ops\nfrom tensorflow.python.ops import variable_scope as vs\nfrom tensorflow.python.util import deprecation\nfrom tensorflow.python.util import nest\nfrom tensorflow.python.util.tf_export import tf_export\n\n# pylint: disable=protected-access\n_concat = rnn_cell_impl._concat\n# pylint: enable=protected-access\n\n\ndef _best_effort_input_batch_size(flat_input):\n  \"\"\"Get static input batch size if available, with fallback to the dynamic one.\n\n  Args:\n    flat_input: An iterable of time major input Tensors of shape `[max_time,\n      batch_size, ...]`. All inputs should have compatible batch sizes.\n\n  Returns:\n    The batch size in Python integer if available, or a scalar Tensor otherwise.\n\n  Raises:\n    ValueError: if there is any input with an invalid shape.\n  \"\"\"", "docstring": "  \"\"\"Get static input batch size if available, with fallback to the dynamic one.\n\n  Args:\n    flat_input: An iterable of time major input Tensors of shape `[max_time,\n      batch_size, ...]`. All inputs should have compatible batch sizes.\n\n  Returns:\n    The batch size in Python integer if available, or a scalar Tensor otherwise.\n\n  Raises:\n    ValueError: if there is any input with an invalid shape.\n  \"\"\""}, "3b91de32b1c9f3c8f0fe9c12cace79c02f2051a6": {"bleu": 0.6371830586067595, "jaccard": 0.5703125, "edit_distance": 0.7238095238095238, "comment_cnt": 7, "max_sim": 0.7238095238095238, "header": "\"\"\"K-means clustering\"\"\"\n\n\nimport warnings\n\nimport numpy as np\nimport scipy.sparse as sp\n\nfrom ..base import BaseEstimator, ClusterMixin, TransformerMixin\nfrom ..metrics.pairwise import euclidean_distances\nfrom ..metrics.pairwise import pairwise_distances_argmin_min\nfrom ..utils.extmath import row_norms, squared_norm\nfrom ..utils.sparsefuncs_fast import assign_rows_csr\nfrom ..utils.sparsefuncs import mean_variance_axis\nfrom ..utils.fixes import astype\nfrom ..utils import check_array\nfrom ..utils import check_random_state\nfrom ..utils import as_float_array\nfrom ..utils import gen_batches\nfrom ..utils.validation import check_is_fitted\nfrom ..utils.validation import FLOAT_DTYPES\nfrom ..utils.random import choice\nfrom ..externals.joblib import Parallel\nfrom ..externals.joblib import delayed\nfrom ..externals.six import string_types\n\nfrom . import _k_means\nfrom ._k_means_elkan import k_means_elkan\n\n\n\n\ndef _k_init(X, n_clusters, x_squared_norms, random_state, n_local_trials=None):\n    \"\"\"Init n_clusters seeds according to k-means++\n\n    Parameters\n    -----------\n    X: array or sparse matrix, shape (n_samples, n_features)\n        The data to pick seeds for. To avoid memory copy, the input data\n        should be double precision (dtype=np.float64).\n\n    n_clusters: integer\n        The number of seeds to choose\n\n    x_squared_norms: array, shape (n_samples,)\n        Squared Euclidean norm of each data point.\n\n    random_state: numpy.RandomState\n        The generator used to initialize the centers.\n\n    n_local_trials: integer, optional\n        The number of seeding trials for each center (except the first),\n        of which the one reducing inertia the most is greedily chosen.\n        Set to None to make the number of trials depend logarithmically\n        on the number of seeds (2+log(k)); this is the default.\n\n    Notes\n    -----\n    Selects initial cluster centers for k-mean clustering in a smart way\n    to speed up convergence. see: Arthur, D. and Vassilvitskii, S.\n    \"k-means++: the advantages of careful seeding\". ACM-SIAM symposium\n    on Discrete algorithms. 2007\n\n    Version ported from http://www.stanford.edu/~darthur/kMeansppTest.zip,\n    which is the implementation used in the aforementioned paper.\n    \"\"\"", "body": "    n_samples, n_features = X.shape\n\n    centers = np.empty((n_clusters, n_features), dtype=X.dtype)\n\n    assert x_squared_norms is not None, 'x_squared_norms None in _k_init'\n\n    # Set the number of local seeding trials if none is given\n    if n_local_trials is None:\n        # This is what Arthur/Vassilvitskii tried, but did not report\n        # specific results for other than mentioning in the conclusion\n        # that it helped.\n        n_local_trials = 2 + int(np.log(n_clusters))\n\n    # Pick first center randomly\n    center_id = random_state.randint(n_samples)\n    if sp.issparse(X):\n        centers[0] = X[center_id].toarray()\n    else:\n        centers[0] = X[center_id]\n\n    # Initialize list of closest distances and calculate current potential\n    closest_dist_sq = euclidean_distances(\n        centers[0, np.newaxis], X, Y_norm_squared=x_squared_norms,\n        squared=True)\n    current_pot = closest_dist_sq.sum()\n\n    # Pick the remaining n_clusters-1 points\n    for c in range(1, n_clusters):\n        # Choose center candidates by sampling with probability proportional\n        # to the squared distance to the closest existing center\n        rand_vals = random_state.random_sample(n_local_trials) * current_pot\n        candidate_ids = np.searchsorted(closest_dist_sq.cumsum(), rand_vals)\n\n        # Compute distances to center candidates\n        distance_to_candidates = euclidean_distances(\n            X[candidate_ids], X, Y_norm_squared=x_squared_norms, squared=True)\n\n        # Decide which candidate is the best\n        best_candidate = None\n        best_pot = None\n        best_dist_sq = None\n        for trial in range(n_local_trials):\n            # Compute potential when including center candidate\n            new_dist_sq = np.minimum(closest_dist_sq,\n                                     distance_to_candidates[trial])\n            new_pot = new_dist_sq.sum()\n\n            # Store result if it is the best local trial so far\n            if (best_candidate is None) or (new_pot < best_pot):\n                best_candidate = candidate_ids[trial]\n                best_pot = new_pot\n                best_dist_sq = new_dist_sq\n\n        # Permanently add best center candidate found in local tries\n        if sp.issparse(X):\n            centers[c] = X[best_candidate].toarray()\n        else:\n            centers[c] = X[best_candidate]\n        current_pot = best_pot\n        closest_dist_sq = best_dist_sq\n\n    return centers", "answer": "n_samples, n_features = X.shape\n\n    centers = np.empty((n_clusters, n_features), dtype=X.dtype)\n\n    # Set the number of local seeding trials if none is given\n    if n_local_trials is None:\n        n_local_trials = 2 + int(np.log(n_clusters))\n\n    # Pick first center randomly\n    center_id = random_state.randint(n_samples)\n    centers[0] = X[center_id]\n\n    # Initialize list of closest distances and calculate current potential\n    closest_dist_sq = euclidean_distances(\n        centers[0, np.newaxis], X, Y_norm_squared=x_squared_norms, squared=True)\n    current_pot = closest_dist_sq.sum()\n\n    # Pick the remaining n_clusters-1 points\n    for c in range(1, n_clusters):\n        # Choose center candidates by sampling with probability proportional\n        # to the squared distance to the closest existing center\n        rand_vals = random_state.random_sample(n_local_trials) * current_pot\n        candidate_ids = np.searchsorted(np.cumsum(closest_dist_sq), rand_vals)\n\n        # Compute distances to center candidates\n        distance_to_candidates = euclidean_distances(\n            X[candidate_ids], X, Y_norm_squared=x_squared_norms, squared=True)\n\n        # update closest distances squared and potential for each candidate\n        best_candidate = None\n        best_pot = None\n        best_dist_sq = None\n        for trial in range(n_local_trials):\n            new_dist_sq = np.minimum(closest_dist_sq, distance_to_candidates[trial])\n            new_pot = new_dist_sq.sum()\n\n            if (best_candidate is None) or (new_pot < best_pot):\n                best_candidate = candidate_ids[trial]\n                best_pot = new_pot\n                best_dist_sq = new_dist_sq\n\n        centers[c] = X[best_candidate]\n        current_pot = best_pot\n        closest_dist_sq = best_dist_sq\n\n    return centers", "license": "bsd-3-clause", "author": "ditenberg <itenbergd@gmail.com", "project": "scikit-learn_scikit-learn", "signature": "def _k_init(X, n_clusters, x_squared_norms, random_state, n_local_trials=None):", "license_type": "Permissive", "init_header": "\"\"\"K-means clustering\"\"\"\n\n# Authors: Gael Varoquaux <gael.varoquaux@normalesup.org>\n#          Thomas Rueckstiess <ruecksti@in.tum.de>\n#          James Bergstra <james.bergstra@umontreal.ca>\n#          Jan Schlueter <scikit-learn@jan-schlueter.de>\n#          Nelle Varoquaux\n#          Peter Prettenhofer <peter.prettenhofer@gmail.com>\n#          Olivier Grisel <olivier.grisel@ensta.org>\n#          Mathieu Blondel <mathieu@mblondel.org>\n#          Robert Layton <robertlayton@gmail.com>\n# License: BSD 3 clause\n\nimport warnings\n\nimport numpy as np\nimport scipy.sparse as sp\n\nfrom ..base import BaseEstimator, ClusterMixin, TransformerMixin\nfrom ..metrics.pairwise import euclidean_distances\nfrom ..metrics.pairwise import pairwise_distances_argmin_min\nfrom ..utils.extmath import row_norms, squared_norm\nfrom ..utils.sparsefuncs_fast import assign_rows_csr\nfrom ..utils.sparsefuncs import mean_variance_axis\nfrom ..utils.fixes import astype\nfrom ..utils import check_array\nfrom ..utils import check_random_state\nfrom ..utils import as_float_array\nfrom ..utils import gen_batches\nfrom ..utils.validation import check_is_fitted\nfrom ..utils.validation import FLOAT_DTYPES\nfrom ..utils.random import choice\nfrom ..externals.joblib import Parallel\nfrom ..externals.joblib import delayed\nfrom ..externals.six import string_types\n\nfrom . import _k_means\nfrom ._k_means_elkan import k_means_elkan\n\n\n###############################################################################\n# Initialization heuristic\n\n\ndef _k_init(X, n_clusters, x_squared_norms, random_state, n_local_trials=None):\n    \"\"\"Init n_clusters seeds according to k-means++\n\n    Parameters\n    -----------\n    X: array or sparse matrix, shape (n_samples, n_features)\n        The data to pick seeds for. To avoid memory copy, the input data\n        should be double precision (dtype=np.float64).\n\n    n_clusters: integer\n        The number of seeds to choose\n\n    x_squared_norms: array, shape (n_samples,)\n        Squared Euclidean norm of each data point.\n\n    random_state: numpy.RandomState\n        The generator used to initialize the centers.\n\n    n_local_trials: integer, optional\n        The number of seeding trials for each center (except the first),\n        of which the one reducing inertia the most is greedily chosen.\n        Set to None to make the number of trials depend logarithmically\n        on the number of seeds (2+log(k)); this is the default.\n\n    Notes\n    -----\n    Selects initial cluster centers for k-mean clustering in a smart way\n    to speed up convergence. see: Arthur, D. and Vassilvitskii, S.\n    \"k-means++: the advantages of careful seeding\". ACM-SIAM symposium\n    on Discrete algorithms. 2007\n\n    Version ported from http://www.stanford.edu/~darthur/kMeansppTest.zip,\n    which is the implementation used in the aforementioned paper.\n    \"\"\"", "docstring": "    \"\"\"Init n_clusters seeds according to k-means++\n\n    Parameters\n    -----------\n    X: array or sparse matrix, shape (n_samples, n_features)\n        The data to pick seeds for. To avoid memory copy, the input data\n        should be double precision (dtype=np.float64).\n\n    n_clusters: integer\n        The number of seeds to choose\n\n    x_squared_norms: array, shape (n_samples,)\n        Squared Euclidean norm of each data point.\n\n    random_state: numpy.RandomState\n        The generator used to initialize the centers.\n\n    n_local_trials: integer, optional\n        The number of seeding trials for each center (except the first),\n        of which the one reducing inertia the most is greedily chosen.\n        Set to None to make the number of trials depend logarithmically\n        on the number of seeds (2+log(k)); this is the default.\n\n    Notes\n    -----\n    Selects initial cluster centers for k-mean clustering in a smart way\n    to speed up convergence. see: Arthur, D. and Vassilvitskii, S.\n    \"k-means++: the advantages of careful seeding\". ACM-SIAM symposium\n    on Discrete algorithms. 2007\n\n    Version ported from http://www.stanford.edu/~darthur/kMeansppTest.zip,\n    which is the implementation used in the aforementioned paper.\n    \"\"\""}, "35c00596f96319b666fa1d2a4f3dc08e6af237a7": {"bleu": 1.0, "jaccard": 1.0, "edit_distance": 1.0, "comment_cnt": 7, "max_sim": 1.0, "header": "from __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport xml.etree.ElementTree as ET\nimport os\nimport pickle\nimport numpy as np\n\ndef voc_ap(rec, prec, use_07_metric=False):\n  \"\"\" ap = voc_ap(rec, prec, [use_07_metric])\n  Compute VOC AP given precision and recall.\n  If use_07_metric is true, uses the\n  VOC 07 11 point method (default:False).\n  \"\"\"", "body": "  if use_07_metric:\n    # 11 point metric\n    ap = 0.\n    for t in np.arange(0., 1.1, 0.1):\n      if np.sum(rec >= t) == 0:\n        p = 0\n      else:\n        p = np.max(prec[rec >= t])\n      ap = ap + p / 11.\n  else:\n    # correct AP calculation\n    # first append sentinel values at the end\n    mrec = np.concatenate(([0.], rec, [1.]))\n    mpre = np.concatenate(([0.], prec, [0.]))\n\n    # compute the precision envelope\n    for i in range(mpre.size - 1, 0, -1):\n      mpre[i - 1] = np.maximum(mpre[i - 1], mpre[i])\n\n    # to calculate area under PR curve, look for points\n    # where X axis (recall) changes value\n    i = np.where(mrec[1:] != mrec[:-1])[0]\n\n    # and sum (\\Delta recall) * prec\n    ap = np.sum((mrec[i + 1] - mrec[i]) * mpre[i + 1])\n  return ap", "answer": "if use_07_metric:\n    # 11 point metric\n    ap = 0.\n    for t in np.arange(0., 1.1, 0.1):\n      if np.sum(rec >= t) == 0:\n        p = 0\n      else:\n        p = np.max(prec[rec >= t])\n      ap = ap + p / 11.\n  else:\n    # correct AP calculation\n    # first append sentinel values at the end\n    mrec = np.concatenate(([0.], rec, [1.]))\n    mpre = np.concatenate(([0.], prec, [0.]))\n\n    # compute the precision envelope\n    for i in range(mpre.size - 1, 0, -1):\n      mpre[i - 1] = np.maximum(mpre[i - 1], mpre[i])\n\n    # to calculate area under PR curve, look for points\n    # where X axis (recall) changes value\n    i = np.where(mrec[1:] != mrec[:-1])[0]\n\n    # and sum (\\Delta recall) * prec\n    ap = np.sum((mrec[i + 1] - mrec[i]) * mpre[i + 1])\n  return ap", "license": "mit", "author": "phil-bergmann <philipp.bergmann@tum.de", "project": "ruotianluo_pytorch-faster-rcnn", "signature": "def voc_ap(rec, prec, use_07_metric=False):", "license_type": "Permissive", "init_header": "# --------------------------------------------------------\n# Fast/er R-CNN\n# Licensed under The MIT License [see LICENSE for details]\n# Written by Bharath Hariharan\n# --------------------------------------------------------\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport xml.etree.ElementTree as ET\nimport os\nimport pickle\nimport numpy as np\n\ndef voc_ap(rec, prec, use_07_metric=False):\n  \"\"\" ap = voc_ap(rec, prec, [use_07_metric])\n  Compute VOC AP given precision and recall.\n  If use_07_metric is true, uses the\n  VOC 07 11 point method (default:False).\n  \"\"\"", "docstring": "  \"\"\" ap = voc_ap(rec, prec, [use_07_metric])\n  Compute VOC AP given precision and recall.\n  If use_07_metric is true, uses the\n  VOC 07 11 point method (default:False).\n  \"\"\""}, "95da2ea129401c6a5d4906d047f914994fda1176": {"bleu": 0.657791131929742, "jaccard": 0.671875, "edit_distance": 0.7519747235387046, "comment_cnt": 8, "max_sim": 0.7519747235387046, "header": "\"\"\"K-means clustering\"\"\"\n\n\nfrom __future__ import division\nimport warnings\n\nimport numpy as np\nimport scipy.sparse as sp\n\nfrom ..base import BaseEstimator, ClusterMixin, TransformerMixin\nfrom ..metrics.pairwise import euclidean_distances\nfrom ..metrics.pairwise import pairwise_distances_argmin_min\nfrom ..utils.extmath import row_norms, squared_norm, stable_cumsum\nfrom ..utils.sparsefuncs_fast import assign_rows_csr\nfrom ..utils.sparsefuncs import mean_variance_axis\nfrom ..utils.validation import _num_samples\nfrom ..utils import check_array\nfrom ..utils import gen_batches\nfrom ..utils import check_random_state\nfrom ..utils.validation import check_is_fitted\nfrom ..utils.validation import FLOAT_DTYPES\nfrom ..utils._joblib import Parallel\nfrom ..utils._joblib import delayed\nfrom ..utils._joblib import effective_n_jobs\nfrom ..externals.six import string_types\nfrom ..exceptions import ConvergenceWarning\nfrom . import _k_means\nfrom ._k_means_elkan import k_means_elkan\n\n\n\n\ndef _k_init(X, n_clusters, x_squared_norms, random_state, n_local_trials=None):\n    \"\"\"Init n_clusters seeds according to k-means++\n\n    Parameters\n    -----------\n    X : array or sparse matrix, shape (n_samples, n_features)\n        The data to pick seeds for. To avoid memory copy, the input data\n        should be double precision (dtype=np.float64).\n\n    n_clusters : integer\n        The number of seeds to choose\n\n    x_squared_norms : array, shape (n_samples,)\n        Squared Euclidean norm of each data point.\n\n    random_state : int, RandomState instance\n        The generator used to initialize the centers. Use an int to make the\n        randomness deterministic.\n        See :term:`Glossary <random_state>`.\n\n    n_local_trials : integer, optional\n        The number of seeding trials for each center (except the first),\n        of which the one reducing inertia the most is greedily chosen.\n        Set to None to make the number of trials depend logarithmically\n        on the number of seeds (2+log(k)); this is the default.\n\n    Notes\n    -----\n    Selects initial cluster centers for k-mean clustering in a smart way\n    to speed up convergence. see: Arthur, D. and Vassilvitskii, S.\n    \"k-means++: the advantages of careful seeding\". ACM-SIAM symposium\n    on Discrete algorithms. 2007\n\n    Version ported from http://www.stanford.edu/~darthur/kMeansppTest.zip,\n    which is the implementation used in the aforementioned paper.\n    \"\"\"", "body": "    n_samples, n_features = X.shape\n\n    centers = np.empty((n_clusters, n_features), dtype=X.dtype)\n\n    assert x_squared_norms is not None, 'x_squared_norms None in _k_init'\n\n    # Set the number of local seeding trials if none is given\n    if n_local_trials is None:\n        # This is what Arthur/Vassilvitskii tried, but did not report\n        # specific results for other than mentioning in the conclusion\n        # that it helped.\n        n_local_trials = 2 + int(np.log(n_clusters))\n\n    # Pick first center randomly\n    center_id = random_state.randint(n_samples)\n    if sp.issparse(X):\n        centers[0] = X[center_id].toarray()\n    else:\n        centers[0] = X[center_id]\n\n    # Initialize list of closest distances and calculate current potential\n    closest_dist_sq = euclidean_distances(\n        centers[0, np.newaxis], X, Y_norm_squared=x_squared_norms,\n        squared=True)\n    current_pot = closest_dist_sq.sum()\n\n    # Pick the remaining n_clusters-1 points\n    for c in range(1, n_clusters):\n        # Choose center candidates by sampling with probability proportional\n        # to the squared distance to the closest existing center\n        rand_vals = random_state.random_sample(n_local_trials) * current_pot\n        candidate_ids = np.searchsorted(stable_cumsum(closest_dist_sq),\n                                        rand_vals)\n\n        # Compute distances to center candidates\n        distance_to_candidates = euclidean_distances(\n            X[candidate_ids], X, Y_norm_squared=x_squared_norms, squared=True)\n\n        # Decide which candidate is the best\n        best_candidate = None\n        best_pot = None\n        best_dist_sq = None\n        for trial in range(n_local_trials):\n            # Compute potential when including center candidate\n            new_dist_sq = np.minimum(closest_dist_sq,\n                                     distance_to_candidates[trial])\n            new_pot = new_dist_sq.sum()\n\n            # Store result if it is the best local trial so far\n            if (best_candidate is None) or (new_pot < best_pot):\n                best_candidate = candidate_ids[trial]\n                best_pot = new_pot\n                best_dist_sq = new_dist_sq\n\n        # Permanently add best center candidate found in local tries\n        if sp.issparse(X):\n            centers[c] = X[best_candidate].toarray()\n        else:\n            centers[c] = X[best_candidate]\n        current_pot = best_pot\n        closest_dist_sq = best_dist_sq\n\n    return centers", "answer": "n_samples, n_features = X.shape\n\n    centers = np.empty((n_clusters, n_features), dtype=X.dtype)\n\n    # Set the number of local seeding trials if none is given\n    if n_local_trials is None:\n        n_local_trials = 2 + int(np.log(n_clusters))\n\n    # Pick first center randomly\n    center_id = random_state.randint(n_samples)\n    centers[0] = X[center_id]\n\n    # Initialize list of closest distances and calculate current potential\n    closest_dist_sq = euclidean_distances(\n        centers[0, np.newaxis], X, Y_norm_squared=x_squared_norms, squared=True)\n    current_pot = closest_dist_sq.sum()\n\n    # Pick the remaining n_clusters-1 points\n    for c in range(1, n_clusters):\n        # Choose center candidates by sampling with probability proportional\n        # to the squared distance to the closest existing center\n        rand_vals = random_state.random_sample(n_local_trials) * current_pot\n        candidate_ids = np.searchsorted(stable_cumsum(closest_dist_sq), rand_vals)\n\n        # Compute distances to center candidates\n        distance_to_candidates = euclidean_distances(\n            X[candidate_ids], X, Y_norm_squared=x_squared_norms, squared=True)\n\n        # Decide which candidate is the best\n        best_candidate = None\n        best_pot = None\n        best_dist_sq = None\n        for trial in range(n_local_trials):\n            new_dist_sq = np.minimum(closest_dist_sq, distance_to_candidates[trial])\n            new_pot = new_dist_sq.sum()\n\n            if (best_candidate is None) or (new_pot < best_pot):\n                best_candidate = candidate_ids[trial]\n                best_pot = new_pot\n                best_dist_sq = new_dist_sq\n\n        centers[c] = X[best_candidate]\n        current_pot = best_pot\n        closest_dist_sq = best_dist_sq\n\n    return centers", "license": "bsd-3-clause", "author": "Prabakaran Kumaresshan <k_prabakaran+github@hotmail.com", "project": "scikit-learn_scikit-learn", "signature": "def _k_init(X, n_clusters, x_squared_norms, random_state, n_local_trials=None):", "license_type": "Permissive", "init_header": "\"\"\"K-means clustering\"\"\"\n\n# Authors: Gael Varoquaux <gael.varoquaux@normalesup.org>\n#          Thomas Rueckstiess <ruecksti@in.tum.de>\n#          James Bergstra <james.bergstra@umontreal.ca>\n#          Jan Schlueter <scikit-learn@jan-schlueter.de>\n#          Nelle Varoquaux\n#          Peter Prettenhofer <peter.prettenhofer@gmail.com>\n#          Olivier Grisel <olivier.grisel@ensta.org>\n#          Mathieu Blondel <mathieu@mblondel.org>\n#          Robert Layton <robertlayton@gmail.com>\n# License: BSD 3 clause\n\nfrom __future__ import division\nimport warnings\n\nimport numpy as np\nimport scipy.sparse as sp\n\nfrom ..base import BaseEstimator, ClusterMixin, TransformerMixin\nfrom ..metrics.pairwise import euclidean_distances\nfrom ..metrics.pairwise import pairwise_distances_argmin_min\nfrom ..utils.extmath import row_norms, squared_norm, stable_cumsum\nfrom ..utils.sparsefuncs_fast import assign_rows_csr\nfrom ..utils.sparsefuncs import mean_variance_axis\nfrom ..utils.validation import _num_samples\nfrom ..utils import check_array\nfrom ..utils import gen_batches\nfrom ..utils import check_random_state\nfrom ..utils.validation import check_is_fitted\nfrom ..utils.validation import FLOAT_DTYPES\nfrom ..utils._joblib import Parallel\nfrom ..utils._joblib import delayed\nfrom ..utils._joblib import effective_n_jobs\nfrom ..externals.six import string_types\nfrom ..exceptions import ConvergenceWarning\nfrom . import _k_means\nfrom ._k_means_elkan import k_means_elkan\n\n\n###############################################################################\n# Initialization heuristic\n\n\ndef _k_init(X, n_clusters, x_squared_norms, random_state, n_local_trials=None):\n    \"\"\"Init n_clusters seeds according to k-means++\n\n    Parameters\n    -----------\n    X : array or sparse matrix, shape (n_samples, n_features)\n        The data to pick seeds for. To avoid memory copy, the input data\n        should be double precision (dtype=np.float64).\n\n    n_clusters : integer\n        The number of seeds to choose\n\n    x_squared_norms : array, shape (n_samples,)\n        Squared Euclidean norm of each data point.\n\n    random_state : int, RandomState instance\n        The generator used to initialize the centers. Use an int to make the\n        randomness deterministic.\n        See :term:`Glossary <random_state>`.\n\n    n_local_trials : integer, optional\n        The number of seeding trials for each center (except the first),\n        of which the one reducing inertia the most is greedily chosen.\n        Set to None to make the number of trials depend logarithmically\n        on the number of seeds (2+log(k)); this is the default.\n\n    Notes\n    -----\n    Selects initial cluster centers for k-mean clustering in a smart way\n    to speed up convergence. see: Arthur, D. and Vassilvitskii, S.\n    \"k-means++: the advantages of careful seeding\". ACM-SIAM symposium\n    on Discrete algorithms. 2007\n\n    Version ported from http://www.stanford.edu/~darthur/kMeansppTest.zip,\n    which is the implementation used in the aforementioned paper.\n    \"\"\"", "docstring": "    \"\"\"Init n_clusters seeds according to k-means++\n\n    Parameters\n    -----------\n    X : array or sparse matrix, shape (n_samples, n_features)\n        The data to pick seeds for. To avoid memory copy, the input data\n        should be double precision (dtype=np.float64).\n\n    n_clusters : integer\n        The number of seeds to choose\n\n    x_squared_norms : array, shape (n_samples,)\n        Squared Euclidean norm of each data point.\n\n    random_state : int, RandomState instance\n        The generator used to initialize the centers. Use an int to make the\n        randomness deterministic.\n        See :term:`Glossary <random_state>`.\n\n    n_local_trials : integer, optional\n        The number of seeding trials for each center (except the first),\n        of which the one reducing inertia the most is greedily chosen.\n        Set to None to make the number of trials depend logarithmically\n        on the number of seeds (2+log(k)); this is the default.\n\n    Notes\n    -----\n    Selects initial cluster centers for k-mean clustering in a smart way\n    to speed up convergence. see: Arthur, D. and Vassilvitskii, S.\n    \"k-means++: the advantages of careful seeding\". ACM-SIAM symposium\n    on Discrete algorithms. 2007\n\n    Version ported from http://www.stanford.edu/~darthur/kMeansppTest.zip,\n    which is the implementation used in the aforementioned paper.\n    \"\"\""}, "707625aaa76a12ca4c65edac7c921044b8a1bf90": {"bleu": 0.7858424388201066, "jaccard": 0.88671875, "edit_distance": 0.768860353130016, "comment_cnt": 1, "max_sim": 0.88671875, "header": "\n\"\"\"Image preprocessing helpers.\"\"\"\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport cv2\nfrom scipy import ndimage\nimport tensorflow as tf\nfrom tensorflow.python.ops import control_flow_ops\n\n\ndef distort_color(image, color_ordering=0, fast_mode=True, scope=None):\n  \"\"\"Distort the color of a Tensor image.\n\n  TODO(coreylynch): add as a dependency, when slim or tensorflow/models are\n  pipfied.\n  Source:\n  https://raw.githubusercontent.com/tensorflow/models/a9d0e6e8923a4/slim/preprocessing/inception_preprocessing.py\n\n  Each color distortion is non-commutative and thus ordering of the color ops\n  matters. Ideally we would randomly permute the ordering of the color ops.\n  Rather than adding that level of complication, we select a distinct ordering\n  of color ops for each preprocessing thread.\n  Args:\n    image: 3-D Tensor containing single image in [0, 1].\n    color_ordering: Python int, a type of distortion (valid values: 0-3).\n    fast_mode: Avoids slower ops (random_hue and random_contrast)\n    scope: Optional scope for name_scope.\n  Returns:\n    3-D Tensor color-distorted image on range [0, 1]\n  Raises:\n    ValueError: if color_ordering not in [0, 3]\n  \"\"\"", "body": "  with tf.name_scope(scope, 'distort_color', [image]):\n    if fast_mode:\n      if color_ordering == 0:\n        image = tf.image.random_brightness(image, max_delta=32. / 255.)\n        image = tf.image.random_saturation(image, lower=0.5, upper=1.5)\n      else:\n        image = tf.image.random_saturation(image, lower=0.5, upper=1.5)\n        image = tf.image.random_brightness(image, max_delta=32. / 255.)\n    else:\n      if color_ordering == 0:\n        image = tf.image.random_brightness(image, max_delta=32. / 255.)\n        image = tf.image.random_saturation(image, lower=0.5, upper=1.5)\n        image = tf.image.random_hue(image, max_delta=0.2)\n        image = tf.image.random_contrast(image, lower=0.5, upper=1.5)\n      elif color_ordering == 1:\n        image = tf.image.random_saturation(image, lower=0.5, upper=1.5)\n        image = tf.image.random_brightness(image, max_delta=32. / 255.)\n        image = tf.image.random_contrast(image, lower=0.5, upper=1.5)\n        image = tf.image.random_hue(image, max_delta=0.2)\n      elif color_ordering == 2:\n        image = tf.image.random_contrast(image, lower=0.5, upper=1.5)\n        image = tf.image.random_hue(image, max_delta=0.2)\n        image = tf.image.random_brightness(image, max_delta=32. / 255.)\n        image = tf.image.random_saturation(image, lower=0.5, upper=1.5)\n      elif color_ordering == 3:\n        image = tf.image.random_hue(image, max_delta=0.2)\n        image = tf.image.random_saturation(image, lower=0.5, upper=1.5)\n        image = tf.image.random_contrast(image, lower=0.5, upper=1.5)\n        image = tf.image.random_brightness(image, max_delta=32. / 255.)\n      else:\n        raise ValueError('color_ordering must be in [0, 3]')\n\n    # The random_* ops do not necessarily clamp.\n    return tf.clip_by_value(image, 0.0, 1.0)", "answer": "with tf.name_scope(scope, 'distort_color', [image]):\n    if color_ordering == 0:\n      image = tf.image.random_brightness(image, max_delta=32. / 255.)\n      image = tf.image.random_saturation(image, lower=0.5, upper=1.5)\n      image = tf.image.random_hue(image, max_delta=0.2)\n      image = tf.image.random_contrast(image, lower=0.5, upper=1.5)\n    elif color_ordering == 1:\n      image = tf.image.random_saturation(image, lower=0.5, upper=1.5)\n      image = tf.image.random_brightness(image, max_delta=32. / 255.)\n      image = tf.image.random_contrast(image, lower=0.5, upper=1.5)\n      image = tf.image.random_hue(image, max_delta=0.2)\n    elif color_ordering == 2:\n      image = tf.image.random_contrast(image, lower=0.5, upper=1.5)\n      image = tf.image.random_hue(image, max_delta=0.2)\n      image = tf.image.random_brightness(image, max_delta=32. / 255.)\n      image = tf.image.random_saturation(image, lower=0.5, upper=1.5)\n    elif color_ordering == 3:\n      image = tf.image.random_hue(image, max_delta=0.2)\n      image = tf.image.random_saturation(image, lower=0.5, upper=1.5)\n      image = tf.image.random_contrast(image, lower=0.5, upper=1.5)\n      image = tf.image.random_brightness(image, max_delta=32. / 255.)\n    else:\n      raise ValueError('color_ordering must be in [0, 3]')\n\n    # The random_* ops do not necessarily clamp.\n    return tf.clip_by_value(image, 0.0, 1.0)", "license": "apache-2.0", "author": "supercourage <wikkywu@163.com", "project": "tensorflow_models", "signature": "def distort_color(image, color_ordering=0, fast_mode=True, scope=None):", "license_type": "Permissive", "init_header": "# Copyright 2017 The TensorFlow Authors All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\n\"\"\"Image preprocessing helpers.\"\"\"\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport cv2\nfrom scipy import ndimage\nimport tensorflow as tf\nfrom tensorflow.python.ops import control_flow_ops\n\n\ndef distort_color(image, color_ordering=0, fast_mode=True, scope=None):\n  \"\"\"Distort the color of a Tensor image.\n\n  TODO(coreylynch): add as a dependency, when slim or tensorflow/models are\n  pipfied.\n  Source:\n  https://raw.githubusercontent.com/tensorflow/models/a9d0e6e8923a4/slim/preprocessing/inception_preprocessing.py\n\n  Each color distortion is non-commutative and thus ordering of the color ops\n  matters. Ideally we would randomly permute the ordering of the color ops.\n  Rather than adding that level of complication, we select a distinct ordering\n  of color ops for each preprocessing thread.\n  Args:\n    image: 3-D Tensor containing single image in [0, 1].\n    color_ordering: Python int, a type of distortion (valid values: 0-3).\n    fast_mode: Avoids slower ops (random_hue and random_contrast)\n    scope: Optional scope for name_scope.\n  Returns:\n    3-D Tensor color-distorted image on range [0, 1]\n  Raises:\n    ValueError: if color_ordering not in [0, 3]\n  \"\"\"", "docstring": "  \"\"\"Distort the color of a Tensor image.\n\n  TODO(coreylynch): add as a dependency, when slim or tensorflow/models are\n  pipfied.\n  Source:\n  https://raw.githubusercontent.com/tensorflow/models/a9d0e6e8923a4/slim/preprocessing/inception_preprocessing.py\n\n  Each color distortion is non-commutative and thus ordering of the color ops\n  matters. Ideally we would randomly permute the ordering of the color ops.\n  Rather than adding that level of complication, we select a distinct ordering\n  of color ops for each preprocessing thread.\n  Args:\n    image: 3-D Tensor containing single image in [0, 1].\n    color_ordering: Python int, a type of distortion (valid values: 0-3).\n    fast_mode: Avoids slower ops (random_hue and random_contrast)\n    scope: Optional scope for name_scope.\n  Returns:\n    3-D Tensor color-distorted image on range [0, 1]\n  Raises:\n    ValueError: if color_ordering not in [0, 3]\n  \"\"\""}, "fab6c443cf3d2e9783d19bf52c81b7aa62d56a38": {"bleu": 0.8761474586654413, "jaccard": 0.67578125, "edit_distance": 0.8779904306220095, "comment_cnt": 1, "max_sim": 0.8779904306220095, "header": "\n\"\"\"Provides data for the MNIST-M dataset.\n\nThe dataset scripts used to create the dataset can be found at:\ntensorflow_models/domain_adaptation_/datasets/download_and_convert_mnist_m_dataset.py\n\"\"\"\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport os\nimport tensorflow as tf\n\nfrom slim.datasets import dataset_utils\n\nslim = tf.contrib.slim\n\n_FILE_PATTERN = 'mnist_m_%s.tfrecord'\n\n_SPLITS_TO_SIZES = {'train': 58001, 'valid': 1000, 'test': 9001}\n\n_NUM_CLASSES = 10\n\n_ITEMS_TO_DESCRIPTIONS = {\n    'image': 'A [32 x 32 x 1] RGB image.',\n    'label': 'A single integer between 0 and 9',\n}\n\n\ndef get_split(split_name, dataset_dir, file_pattern=None, reader=None):\n  \"\"\"Gets a dataset tuple with instructions for reading MNIST.\n\n  Args:\n    split_name: A train/test split name.\n    dataset_dir: The base directory of the dataset sources.\n\n  Returns:\n    A `Dataset` namedtuple.\n\n  Raises:\n    ValueError: if `split_name` is not a valid train/test split.\n  \"\"\"", "body": "  if split_name not in _SPLITS_TO_SIZES:\n    raise ValueError('split name %s was not recognized.' % split_name)\n\n  if not file_pattern:\n    file_pattern = _FILE_PATTERN\n  file_pattern = os.path.join(dataset_dir, file_pattern % split_name)\n\n  # Allowing None in the signature so that dataset_factory can use the default.\n  if reader is None:\n    reader = tf.TFRecordReader\n\n  keys_to_features = {\n      'image/encoded':\n          tf.FixedLenFeature((), tf.string, default_value=''),\n      'image/format':\n          tf.FixedLenFeature((), tf.string, default_value='png'),\n      'image/class/label':\n          tf.FixedLenFeature(\n              [1], tf.int64, default_value=tf.zeros([1], dtype=tf.int64)),\n  }\n\n  items_to_handlers = {\n      'image': slim.tfexample_decoder.Image(shape=[32, 32, 3], channels=3),\n      'label': slim.tfexample_decoder.Tensor('image/class/label', shape=[]),\n  }\n\n  decoder = slim.tfexample_decoder.TFExampleDecoder(\n      keys_to_features, items_to_handlers)\n\n  labels_to_names = None\n  if dataset_utils.has_labels(dataset_dir):\n    labels_to_names = dataset_utils.read_label_file(dataset_dir)\n\n  return slim.dataset.Dataset(\n      data_sources=file_pattern,\n      reader=reader,\n      decoder=decoder,\n      num_samples=_SPLITS_TO_SIZES[split_name],\n      num_classes=_NUM_CLASSES,\n      items_to_descriptions=_ITEMS_TO_DESCRIPTIONS,\n      labels_to_names=labels_to_names)", "answer": "if split_name not in _SPLITS_TO_SIZES:\n    raise ValueError('split_name %s was not recognized.' % split_name)\n\n  if not file_pattern:\n    file_pattern = _FILE_PATTERN\n  file_pattern = os.path.join(dataset_dir, file_pattern % split_name)\n\n  # Allowing None in the signature so that dataset_factory can use the default.\n  if reader is None:\n    reader = tf.TFRecordReader\n\n  keys_to_features = {\n      'image/encoded': tf.FixedLenFeature((), tf.string, default_value=''),\n      'image/format': tf.FixedLenFeature((), tf.string, default_value='png'),\n      'image/class/label': tf.FixedLenFeature(\n          [], tf.int64, default_value=tf.zeros([], dtype=tf.int64)),\n  }\n\n  items_to_handlers = {\n      'image': slim.tfexample_decoder.Image(),\n      'label': slim.tfexample_decoder.Tensor('image/class/label'),\n  }\n\n  decoder = slim.tfexample_decoder.TFExampleDecoder(\n      keys_to_features, items_to_handlers)\n\n  labels_to_names = None\n  if dataset_utils.has_labels(dataset_dir):\n    labels_to_names = dataset_utils.read_label_file(dataset_dir)\n\n  return slim.dataset.Dataset(\n      data_sources=file_pattern,\n      reader=reader,\n      decoder=decoder,\n      num_samples=_SPLITS_TO_SIZES[split_name],\n      items_to_descriptions=_ITEMS_TO_DESCRIPTIONS,\n      num_classes=_NUM_CLASSES,\n      labels_to_names=labels_to_names)", "license": "apache-2.0", "author": "David Dohan <ddohan@google.com", "project": "tensorflow_models", "signature": "def get_split(split_name, dataset_dir, file_pattern=None, reader=None):", "license_type": "Permissive", "init_header": "# Copyright 2017 Google Inc.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\"\"\"Provides data for the MNIST-M dataset.\n\nThe dataset scripts used to create the dataset can be found at:\ntensorflow_models/domain_adaptation_/datasets/download_and_convert_mnist_m_dataset.py\n\"\"\"\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport os\n# Dependency imports\nimport tensorflow as tf\n\nfrom slim.datasets import dataset_utils\n\nslim = tf.contrib.slim\n\n_FILE_PATTERN = 'mnist_m_%s.tfrecord'\n\n_SPLITS_TO_SIZES = {'train': 58001, 'valid': 1000, 'test': 9001}\n\n_NUM_CLASSES = 10\n\n_ITEMS_TO_DESCRIPTIONS = {\n    'image': 'A [32 x 32 x 1] RGB image.',\n    'label': 'A single integer between 0 and 9',\n}\n\n\ndef get_split(split_name, dataset_dir, file_pattern=None, reader=None):\n  \"\"\"Gets a dataset tuple with instructions for reading MNIST.\n\n  Args:\n    split_name: A train/test split name.\n    dataset_dir: The base directory of the dataset sources.\n\n  Returns:\n    A `Dataset` namedtuple.\n\n  Raises:\n    ValueError: if `split_name` is not a valid train/test split.\n  \"\"\"", "docstring": "  \"\"\"Gets a dataset tuple with instructions for reading MNIST.\n\n  Args:\n    split_name: A train/test split name.\n    dataset_dir: The base directory of the dataset sources.\n\n  Returns:\n    A `Dataset` namedtuple.\n\n  Raises:\n    ValueError: if `split_name` is not a valid train/test split.\n  \"\"\""}, "933c9d021919d92ed46a48a8acf5331f439e04f2": {"bleu": 0.5917280572852962, "jaccard": 0.6328125, "edit_distance": 0.7, "comment_cnt": 8, "max_sim": 0.7, "header": "\"\"\"K-means clustering\"\"\"\n\n\nimport warnings\n\nimport numpy as np\nimport scipy.sparse as sp\n\nfrom ..base import BaseEstimator, ClusterMixin, TransformerMixin\nfrom ..metrics.pairwise import euclidean_distances\nfrom ..metrics.pairwise import pairwise_distances_argmin_min\nfrom ..utils.extmath import row_norms, squared_norm, stable_cumsum\nfrom ..utils.sparsefuncs_fast import assign_rows_csr\nfrom ..utils.sparsefuncs import mean_variance_axis\nfrom ..utils.validation import _num_samples\nfrom ..utils import check_array\nfrom ..utils import gen_batches\nfrom ..utils import check_random_state\nfrom ..utils.validation import check_is_fitted\nfrom ..utils.validation import FLOAT_DTYPES\nfrom ..utils._joblib import Parallel\nfrom ..utils._joblib import delayed\nfrom ..utils._joblib import effective_n_jobs\nfrom ..exceptions import ConvergenceWarning\nfrom . import _k_means\nfrom ._k_means_elkan import k_means_elkan\n\n\n\n\ndef _k_init(X, n_clusters, x_squared_norms, random_state, n_local_trials=None):\n    \"\"\"Init n_clusters seeds according to k-means++\n\n    Parameters\n    ----------\n    X : array or sparse matrix, shape (n_samples, n_features)\n        The data to pick seeds for. To avoid memory copy, the input data\n        should be double precision (dtype=np.float64).\n\n    n_clusters : integer\n        The number of seeds to choose\n\n    x_squared_norms : array, shape (n_samples,)\n        Squared Euclidean norm of each data point.\n\n    random_state : int, RandomState instance\n        The generator used to initialize the centers. Use an int to make the\n        randomness deterministic.\n        See :term:`Glossary <random_state>`.\n\n    n_local_trials : integer, optional\n        The number of seeding trials for each center (except the first),\n        of which the one reducing inertia the most is greedily chosen.\n        Set to None to make the number of trials depend logarithmically\n        on the number of seeds (2+log(k)); this is the default.\n\n    Notes\n    -----\n    Selects initial cluster centers for k-mean clustering in a smart way\n    to speed up convergence. see: Arthur, D. and Vassilvitskii, S.\n    \"k-means++: the advantages of careful seeding\". ACM-SIAM symposium\n    on Discrete algorithms. 2007\n\n    Version ported from http://www.stanford.edu/~darthur/kMeansppTest.zip,\n    which is the implementation used in the aforementioned paper.\n    \"\"\"", "body": "    n_samples, n_features = X.shape\n\n    centers = np.empty((n_clusters, n_features), dtype=X.dtype)\n\n    assert x_squared_norms is not None, 'x_squared_norms None in _k_init'\n\n    # Set the number of local seeding trials if none is given\n    if n_local_trials is None:\n        # This is what Arthur/Vassilvitskii tried, but did not report\n        # specific results for other than mentioning in the conclusion\n        # that it helped.\n        n_local_trials = 2 + int(np.log(n_clusters))\n\n    # Pick first center randomly\n    center_id = random_state.randint(n_samples)\n    if sp.issparse(X):\n        centers[0] = X[center_id].toarray()\n    else:\n        centers[0] = X[center_id]\n\n    # Initialize list of closest distances and calculate current potential\n    closest_dist_sq = euclidean_distances(\n        centers[0, np.newaxis], X, Y_norm_squared=x_squared_norms,\n        squared=True)\n    current_pot = closest_dist_sq.sum()\n\n    # Pick the remaining n_clusters-1 points\n    for c in range(1, n_clusters):\n        # Choose center candidates by sampling with probability proportional\n        # to the squared distance to the closest existing center\n        rand_vals = random_state.random_sample(n_local_trials) * current_pot\n        candidate_ids = np.searchsorted(stable_cumsum(closest_dist_sq),\n                                        rand_vals)\n        # XXX: numerical imprecision can result in a candidate_id out of range\n        np.clip(candidate_ids, None, closest_dist_sq.size - 1,\n                out=candidate_ids)\n\n        # Compute distances to center candidates\n        distance_to_candidates = euclidean_distances(\n            X[candidate_ids], X, Y_norm_squared=x_squared_norms, squared=True)\n\n        # Decide which candidate is the best\n        best_candidate = None\n        best_pot = None\n        best_dist_sq = None\n        for trial in range(n_local_trials):\n            # Compute potential when including center candidate\n            new_dist_sq = np.minimum(closest_dist_sq,\n                                     distance_to_candidates[trial])\n            new_pot = new_dist_sq.sum()\n\n            # Store result if it is the best local trial so far\n            if (best_candidate is None) or (new_pot < best_pot):\n                best_candidate = candidate_ids[trial]\n                best_pot = new_pot\n                best_dist_sq = new_dist_sq\n\n        # Permanently add best center candidate found in local tries\n        if sp.issparse(X):\n            centers[c] = X[best_candidate].toarray()\n        else:\n            centers[c] = X[best_candidate]\n        current_pot = best_pot\n        closest_dist_sq = best_dist_sq\n\n    return centers", "answer": "n_samples, n_features = X.shape\n\n    centers = np.empty((n_clusters, n_features), dtype=X.dtype)\n\n    # Set the number of local seeding trials if none is given\n    if n_local_trials is None:\n        n_local_trials = 2 + int(np.log(n_clusters))\n\n    # Pick first center randomly\n    center_id = random_state.randint(n_samples)\n    centers[0] = X[center_id]\n\n    # Initialize list of closest distances and calculate current potential\n    closest_dist_sq = euclidean_distances(\n        centers[0, np.newaxis], X, Y_norm_squared=x_squared_norms, squared=True)\n    current_pot = closest_dist_sq.sum()\n\n    # Pick the remaining n_clusters-1 points\n    for c in range(1, n_clusters):\n        # Choose center candidates by sampling with probability proportional\n        # to the squared distance to the closest existing center\n        rand_vals = random_state.random_sample(n_local_trials) * current_pot\n        candidate_ids = np.searchsorted(stable_cumsum(closest_dist_sq), rand_vals)\n\n        # Compute distances to center candidates\n        distance_to_candidates = euclidean_distances(\n            X[candidate_ids], X, Y_norm_squared=x_squared_norms, squared=True)\n\n        # Decide which candidate is the best\n        best_candidate = None\n        best_pot = None\n        best_dist_sq = None\n        for trial in range(n_local_trials):\n            new_dist_sq = np.minimum(closest_dist_sq, distance_to_candidates[trial])\n            new_pot = new_dist_sq.sum()\n\n            if (best_candidate is None) or (new_pot < best_pot):\n                best_candidate = candidate_ids[trial]\n                best_pot = new_pot\n                best_dist_sq = new_dist_sq\n\n        centers[c] = X[best_candidate]\n        current_pot = best_pot\n        closest_dist_sq = best_dist_sq\n\n    return centers", "license": "bsd-3-clause", "author": "Joel Nothman <joel.nothman@gmail.com", "project": "scikit-learn_scikit-learn", "signature": "def _k_init(X, n_clusters, x_squared_norms, random_state, n_local_trials=None):", "license_type": "Permissive", "init_header": "\"\"\"K-means clustering\"\"\"\n\n# Authors: Gael Varoquaux <gael.varoquaux@normalesup.org>\n#          Thomas Rueckstiess <ruecksti@in.tum.de>\n#          James Bergstra <james.bergstra@umontreal.ca>\n#          Jan Schlueter <scikit-learn@jan-schlueter.de>\n#          Nelle Varoquaux\n#          Peter Prettenhofer <peter.prettenhofer@gmail.com>\n#          Olivier Grisel <olivier.grisel@ensta.org>\n#          Mathieu Blondel <mathieu@mblondel.org>\n#          Robert Layton <robertlayton@gmail.com>\n# License: BSD 3 clause\n\nimport warnings\n\nimport numpy as np\nimport scipy.sparse as sp\n\nfrom ..base import BaseEstimator, ClusterMixin, TransformerMixin\nfrom ..metrics.pairwise import euclidean_distances\nfrom ..metrics.pairwise import pairwise_distances_argmin_min\nfrom ..utils.extmath import row_norms, squared_norm, stable_cumsum\nfrom ..utils.sparsefuncs_fast import assign_rows_csr\nfrom ..utils.sparsefuncs import mean_variance_axis\nfrom ..utils.validation import _num_samples\nfrom ..utils import check_array\nfrom ..utils import gen_batches\nfrom ..utils import check_random_state\nfrom ..utils.validation import check_is_fitted\nfrom ..utils.validation import FLOAT_DTYPES\nfrom ..utils._joblib import Parallel\nfrom ..utils._joblib import delayed\nfrom ..utils._joblib import effective_n_jobs\nfrom ..exceptions import ConvergenceWarning\nfrom . import _k_means\nfrom ._k_means_elkan import k_means_elkan\n\n\n###############################################################################\n# Initialization heuristic\n\n\ndef _k_init(X, n_clusters, x_squared_norms, random_state, n_local_trials=None):\n    \"\"\"Init n_clusters seeds according to k-means++\n\n    Parameters\n    ----------\n    X : array or sparse matrix, shape (n_samples, n_features)\n        The data to pick seeds for. To avoid memory copy, the input data\n        should be double precision (dtype=np.float64).\n\n    n_clusters : integer\n        The number of seeds to choose\n\n    x_squared_norms : array, shape (n_samples,)\n        Squared Euclidean norm of each data point.\n\n    random_state : int, RandomState instance\n        The generator used to initialize the centers. Use an int to make the\n        randomness deterministic.\n        See :term:`Glossary <random_state>`.\n\n    n_local_trials : integer, optional\n        The number of seeding trials for each center (except the first),\n        of which the one reducing inertia the most is greedily chosen.\n        Set to None to make the number of trials depend logarithmically\n        on the number of seeds (2+log(k)); this is the default.\n\n    Notes\n    -----\n    Selects initial cluster centers for k-mean clustering in a smart way\n    to speed up convergence. see: Arthur, D. and Vassilvitskii, S.\n    \"k-means++: the advantages of careful seeding\". ACM-SIAM symposium\n    on Discrete algorithms. 2007\n\n    Version ported from http://www.stanford.edu/~darthur/kMeansppTest.zip,\n    which is the implementation used in the aforementioned paper.\n    \"\"\"", "docstring": "    \"\"\"Init n_clusters seeds according to k-means++\n\n    Parameters\n    ----------\n    X : array or sparse matrix, shape (n_samples, n_features)\n        The data to pick seeds for. To avoid memory copy, the input data\n        should be double precision (dtype=np.float64).\n\n    n_clusters : integer\n        The number of seeds to choose\n\n    x_squared_norms : array, shape (n_samples,)\n        Squared Euclidean norm of each data point.\n\n    random_state : int, RandomState instance\n        The generator used to initialize the centers. Use an int to make the\n        randomness deterministic.\n        See :term:`Glossary <random_state>`.\n\n    n_local_trials : integer, optional\n        The number of seeding trials for each center (except the first),\n        of which the one reducing inertia the most is greedily chosen.\n        Set to None to make the number of trials depend logarithmically\n        on the number of seeds (2+log(k)); this is the default.\n\n    Notes\n    -----\n    Selects initial cluster centers for k-mean clustering in a smart way\n    to speed up convergence. see: Arthur, D. and Vassilvitskii, S.\n    \"k-means++: the advantages of careful seeding\". ACM-SIAM symposium\n    on Discrete algorithms. 2007\n\n    Version ported from http://www.stanford.edu/~darthur/kMeansppTest.zip,\n    which is the implementation used in the aforementioned paper.\n    \"\"\""}, "13d07a9a7e71afaea8d54c92915b62852a4c2c8b": {"bleu": 1.0, "jaccard": 1.0, "edit_distance": 1.0, "comment_cnt": 7, "max_sim": 1.0, "header": "\nimport xml.etree.ElementTree as ET\nimport os\nimport _pickle as cPickle\nimport numpy as np\n\ndef voc_ap(rec, prec, use_07_metric=False):\n    \"\"\" ap = voc_ap(rec, prec, [use_07_metric])\n    Compute VOC AP given precision and recall.\n    If use_07_metric is true, uses the\n    VOC 07 11 point method (default:False).\n    \"\"\"", "body": "    if use_07_metric:\n        # 11 point metric\n        ap = 0.\n        for t in np.arange(0., 1.1, 0.1):\n            if np.sum(rec >= t) == 0:\n                p = 0\n            else:\n                p = np.max(prec[rec >= t])\n            ap = ap + p / 11.\n    else:\n        # correct AP calculation\n        # first append sentinel values at the end\n        mrec = np.concatenate(([0.], rec, [1.]))\n        mpre = np.concatenate(([0.], prec, [0.]))\n\n        # compute the precision envelope\n        for i in range(mpre.size - 1, 0, -1):\n            mpre[i - 1] = np.maximum(mpre[i - 1], mpre[i])\n\n        # to calculate area under PR curve, look for points\n        # where X axis (recall) changes value\n        i = np.where(mrec[1:] != mrec[:-1])[0]\n\n        # and sum (\\Delta recall) * prec\n        ap = np.sum((mrec[i + 1] - mrec[i]) * mpre[i + 1])\n    return ap", "answer": "if use_07_metric:\n        # 11 point metric\n        ap = 0.\n        for t in np.arange(0., 1.1, 0.1):\n            if np.sum(rec >= t) == 0:\n                p = 0\n            else:\n                p = np.max(prec[rec >= t])\n            ap = ap + p / 11.\n    else:\n        # correct AP calculation\n        # first append sentinel values at the end\n        mrec = np.concatenate(([0.], rec, [1.]))\n        mpre = np.concatenate(([0.], prec, [0.]))\n\n        # compute the precision envelope\n        for i in range(mpre.size - 1, 0, -1):\n            mpre[i - 1] = np.maximum(mpre[i - 1], mpre[i])\n\n        # to calculate area under PR curve, look for points\n        # where X axis (recall) changes value\n        i = np.where(mrec[1:] != mrec[:-1])[0]\n\n        # and sum (\\Delta recall) * prec\n        ap = np.sum((mrec[i + 1] - mrec[i]) * mpre[i + 1])\n    return ap", "license": "mit", "author": "AlexeyAB <alexeyab84@gmail.com", "project": "pjreddie_darknet", "signature": "def voc_ap(rec, prec, use_07_metric=False):", "license_type": "Permissive", "init_header": "# --------------------------------------------------------\n# Fast/er R-CNN\n# Licensed under The MIT License [see LICENSE for details]\n# Written by Bharath Hariharan\n# --------------------------------------------------------\n\nimport xml.etree.ElementTree as ET\nimport os\n#import cPickle\nimport _pickle as cPickle\nimport numpy as np\n\ndef voc_ap(rec, prec, use_07_metric=False):\n    \"\"\" ap = voc_ap(rec, prec, [use_07_metric])\n    Compute VOC AP given precision and recall.\n    If use_07_metric is true, uses the\n    VOC 07 11 point method (default:False).\n    \"\"\"", "docstring": "    \"\"\" ap = voc_ap(rec, prec, [use_07_metric])\n    Compute VOC AP given precision and recall.\n    If use_07_metric is true, uses the\n    VOC 07 11 point method (default:False).\n    \"\"\""}, "0205340e71baf62cad4f2e281a9bfa1b328e3e1c": {"bleu": 0.758728160554545, "jaccard": 0.57421875, "edit_distance": 0.7700290577002906, "comment_cnt": 10, "max_sim": 0.7700290577002906, "header": "\"\"\"Contains the definition of the Inception Resnet V2 architecture.\n\nAs described in http://arxiv.org/abs/1602.07261.\n\n  Inception-v4, Inception-ResNet and the Impact of Residual Connections\n    on Learning\n  Christian Szegedy, Sergey Ioffe, Vincent Vanhoucke, Alex Alemi\n\"\"\"\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\n\nimport tensorflow as tf\n\nslim = tf.contrib.slim\n\n\ndef inception_resnet_v2_base(inputs,\n                             final_endpoint='Conv2d_7b_1x1',\n                             output_stride=16,\n                             align_feature_maps=False,\n                             scope=None,\n                             activation_fn=tf.nn.relu):\n  \"\"\"Inception model from  http://arxiv.org/abs/1602.07261.\n\n  Constructs an Inception Resnet v2 network from inputs to the given final\n  endpoint. This method can construct the network up to the final inception\n  block Conv2d_7b_1x1.\n\n  Args:\n    inputs: a tensor of size [batch_size, height, width, channels].\n    final_endpoint: specifies the endpoint to construct the network up to. It\n      can be one of ['Conv2d_1a_3x3', 'Conv2d_2a_3x3', 'Conv2d_2b_3x3',\n      'MaxPool_3a_3x3', 'Conv2d_3b_1x1', 'Conv2d_4a_3x3', 'MaxPool_5a_3x3',\n      'Mixed_5b', 'Mixed_6a', 'PreAuxLogits', 'Mixed_7a', 'Conv2d_7b_1x1']\n    output_stride: A scalar that specifies the requested ratio of input to\n      output spatial resolution. Only supports 8 and 16.\n    align_feature_maps: When true, changes all the VALID paddings in the network\n      to SAME padding so that the feature maps are aligned.\n    scope: Optional variable_scope.\n    activation_fn: Activation function for block scopes.\n\n  Returns:\n    tensor_out: output tensor corresponding to the final_endpoint.\n    end_points: a set of activations for external use, for example summaries or\n                losses.\n\n  Raises:\n    ValueError: if final_endpoint is not set to one of the predefined values,\n      or if the output_stride is not 8 or 16, or if the output_stride is 8 and\n      we request an end point after 'PreAuxLogits'.\n  \"\"\"", "body": "  if output_stride != 8 and output_stride != 16:\n    raise ValueError('output_stride must be 8 or 16.')\n\n  padding = 'SAME' if align_feature_maps else 'VALID'\n\n  end_points = {}\n\n  def add_and_check_final(name, net):\n    end_points[name] = net\n    return name == final_endpoint\n\n  with tf.variable_scope(scope, 'InceptionResnetV2', [inputs]):\n    with slim.arg_scope([slim.conv2d, slim.max_pool2d, slim.avg_pool2d],\n                        stride=1, padding='SAME'):\n      # 149 x 149 x 32\n      net = slim.conv2d(inputs, 32, 3, stride=2, padding=padding,\n                        scope='Conv2d_1a_3x3')\n      if add_and_check_final('Conv2d_1a_3x3', net): return net, end_points\n\n      # 147 x 147 x 32\n      net = slim.conv2d(net, 32, 3, padding=padding,\n                        scope='Conv2d_2a_3x3')\n      if add_and_check_final('Conv2d_2a_3x3', net): return net, end_points\n      # 147 x 147 x 64\n      net = slim.conv2d(net, 64, 3, scope='Conv2d_2b_3x3')\n      if add_and_check_final('Conv2d_2b_3x3', net): return net, end_points\n      # 73 x 73 x 64\n      net = slim.max_pool2d(net, 3, stride=2, padding=padding,\n                            scope='MaxPool_3a_3x3')\n      if add_and_check_final('MaxPool_3a_3x3', net): return net, end_points\n      # 73 x 73 x 80\n      net = slim.conv2d(net, 80, 1, padding=padding,\n                        scope='Conv2d_3b_1x1')\n      if add_and_check_final('Conv2d_3b_1x1', net): return net, end_points\n      # 71 x 71 x 192\n      net = slim.conv2d(net, 192, 3, padding=padding,\n                        scope='Conv2d_4a_3x3')\n      if add_and_check_final('Conv2d_4a_3x3', net): return net, end_points\n      # 35 x 35 x 192\n      net = slim.max_pool2d(net, 3, stride=2, padding=padding,\n                            scope='MaxPool_5a_3x3')\n      if add_and_check_final('MaxPool_5a_3x3', net): return net, end_points\n\n      # 35 x 35 x 320\n      with tf.variable_scope('Mixed_5b'):\n        with tf.variable_scope('Branch_0'):\n          tower_conv = slim.conv2d(net, 96, 1, scope='Conv2d_1x1')\n        with tf.variable_scope('Branch_1'):\n          tower_conv1_0 = slim.conv2d(net, 48, 1, scope='Conv2d_0a_1x1')\n          tower_conv1_1 = slim.conv2d(tower_conv1_0, 64, 5,\n                                      scope='Conv2d_0b_5x5')\n        with tf.variable_scope('Branch_2'):\n          tower_conv2_0 = slim.conv2d(net, 64, 1, scope='Conv2d_0a_1x1')\n          tower_conv2_1 = slim.conv2d(tower_conv2_0, 96, 3,\n                                      scope='Conv2d_0b_3x3')\n          tower_conv2_2 = slim.conv2d(tower_conv2_1, 96, 3,\n                                      scope='Conv2d_0c_3x3')\n        with tf.variable_scope('Branch_3'):\n          tower_pool = slim.avg_pool2d(net, 3, stride=1, padding='SAME',\n                                       scope='AvgPool_0a_3x3')\n          tower_pool_1 = slim.conv2d(tower_pool, 64, 1,\n                                     scope='Conv2d_0b_1x1')\n        net = tf.concat(\n            [tower_conv, tower_conv1_1, tower_conv2_2, tower_pool_1], 3)\n\n      if add_and_check_final('Mixed_5b', net): return net, end_points\n      # TODO(alemi): Register intermediate endpoints\n      net = slim.repeat(net, 10, block35, scale=0.17,\n                        activation_fn=activation_fn)\n\n      # 17 x 17 x 1088 if output_stride == 8,\n      # 33 x 33 x 1088 if output_stride == 16\n      use_atrous = output_stride == 8\n\n      with tf.variable_scope('Mixed_6a'):\n        with tf.variable_scope('Branch_0'):\n          tower_conv = slim.conv2d(net, 384, 3, stride=1 if use_atrous else 2,\n                                   padding=padding,\n                                   scope='Conv2d_1a_3x3')\n        with tf.variable_scope('Branch_1'):\n          tower_conv1_0 = slim.conv2d(net, 256, 1, scope='Conv2d_0a_1x1')\n          tower_conv1_1 = slim.conv2d(tower_conv1_0, 256, 3,\n                                      scope='Conv2d_0b_3x3')\n          tower_conv1_2 = slim.conv2d(tower_conv1_1, 384, 3,\n                                      stride=1 if use_atrous else 2,\n                                      padding=padding,\n                                      scope='Conv2d_1a_3x3')\n        with tf.variable_scope('Branch_2'):\n          tower_pool = slim.max_pool2d(net, 3, stride=1 if use_atrous else 2,\n                                       padding=padding,\n                                       scope='MaxPool_1a_3x3')\n        net = tf.concat([tower_conv, tower_conv1_2, tower_pool], 3)\n\n      if add_and_check_final('Mixed_6a', net): return net, end_points\n\n      # TODO(alemi): register intermediate endpoints\n      with slim.arg_scope([slim.conv2d], rate=2 if use_atrous else 1):\n        net = slim.repeat(net, 20, block17, scale=0.10,\n                          activation_fn=activation_fn)\n      if add_and_check_final('PreAuxLogits', net): return net, end_points\n\n      if output_stride == 8:\n        # TODO(gpapan): Properly support output_stride for the rest of the net.\n        raise ValueError('output_stride==8 is only supported up to the '\n                         'PreAuxlogits end_point for now.')\n\n      # 8 x 8 x 2080\n      with tf.variable_scope('Mixed_7a'):\n        with tf.variable_scope('Branch_0'):\n          tower_conv = slim.conv2d(net, 256, 1, scope='Conv2d_0a_1x1')\n          tower_conv_1 = slim.conv2d(tower_conv, 384, 3, stride=2,\n                                     padding=padding,\n                                     scope='Conv2d_1a_3x3')\n        with tf.variable_scope('Branch_1'):\n          tower_conv1 = slim.conv2d(net, 256, 1, scope='Conv2d_0a_1x1')\n          tower_conv1_1 = slim.conv2d(tower_conv1, 288, 3, stride=2,\n                                      padding=padding,\n                                      scope='Conv2d_1a_3x3')\n        with tf.variable_scope('Branch_2'):\n          tower_conv2 = slim.conv2d(net, 256, 1, scope='Conv2d_0a_1x1')\n          tower_conv2_1 = slim.conv2d(tower_conv2, 288, 3,\n                                      scope='Conv2d_0b_3x3')\n          tower_conv2_2 = slim.conv2d(tower_conv2_1, 320, 3, stride=2,\n                                      padding=padding,\n                                      scope='Conv2d_1a_3x3')\n        with tf.variable_scope('Branch_3'):\n          tower_pool = slim.max_pool2d(net, 3, stride=2,\n                                       padding=padding,\n                                       scope='MaxPool_1a_3x3')\n        net = tf.concat(\n            [tower_conv_1, tower_conv1_1, tower_conv2_2, tower_pool], 3)\n\n      if add_and_check_final('Mixed_7a', net): return net, end_points\n\n      # TODO(alemi): register intermediate endpoints\n      net = slim.repeat(net, 9, block8, scale=0.20, activation_fn=activation_fn)\n      net = block8(net, activation_fn=None)\n\n      # 8 x 8 x 1536\n      net = slim.conv2d(net, 1536, 1, scope='Conv2d_7b_1x1')\n      if add_and_check_final('Conv2d_7b_1x1', net): return net, end_points\n\n    raise ValueError('final_endpoint (%s) not recognized', final_endpoint)", "answer": "end_points = {}\n\n  def add_and_check_final(name, net):\n    end_points[name] = net\n    return name == final_endpoint\n\n  with tf.variable_scope(scope, 'InceptionResnetV2', [inputs]):\n    with slim.arg_scope([slim.conv2d, slim.max_pool2d, slim.avg_pool2d],\n                        stride=1, padding='SAME'):\n      # 149 x 149 x 32\n      net = slim.conv2d(inputs, 32, [3, 3], stride=2, padding='VALID',\n                        scope='Conv2d_1a_3x3')\n      if add_and_check_final('Conv2d_1a_3x3', net): return net, end_points\n      # 147 x 147 x 32\n      net = slim.conv2d(net, 32, [3, 3], padding='VALID',\n                        scope='Conv2d_2a_3x3')\n      if add_and_check_final('Conv2d_2a_3x3', net): return net, end_points\n      # 147 x 147 x 64\n      net = slim.conv2d(net, 64, [3, 3], scope='Conv2d_2b_3x3')\n      if add_and_check_final('Conv2d_2b_3x3', net): return net, end_points\n      # 73 x 73 x 64\n      net = slim.max_pool2d(net, [3, 3], stride=2, padding='VALID',\n                            scope='MaxPool_3a_3x3')\n      if add_and_check_final('MaxPool_3a_3x3', net): return net, end_points\n      # 73 x 73 x 80\n      net = slim.conv2d(net, 80, [1, 1], padding='VALID',\n                        scope='Conv2d_3b_1x1')\n      if add_and_check_final('Conv2d_3b_1x1', net): return net, end_points\n      # 71 x 71 x 192\n      net = slim.conv2d(net, 192, [3, 3], padding='VALID',\n                        scope='Conv2d_4a_3x3')\n      if add_and_check_final('Conv2d_4a_3x3', net): return net, end_points\n      # 35 x 35 x 192\n      net = slim.max_pool2d(net, [3, 3], stride=2, padding='VALID',\n                            scope='MaxPool_5a_3x3')\n      if add_and_check_final('MaxPool_5a_3x3', net): return net, end_points\n\n      # 35 x 35 x 320\n      with tf.variable_scope('Mixed_5b'):\n        with tf.variable_scope('Branch_0'):\n          tower_conv = slim.conv2d(net, 96, [1, 1], scope='Conv2d_1x1')\n        with tf.variable_scope('Branch_1'):\n          tower_conv1_0 = slim.conv2d(net, 48, [1, 1], scope='Conv2d_0a_1x1')\n          tower_conv1_1 = slim.conv2d(tower_conv1_0, 64, [5, 5],\n                                      scope='Conv2d_0b_5x5')\n        with tf.variable_scope('Branch_2'):\n          tower_conv2_0 = slim.conv2d(net, 64, [1, 1], scope='Conv2d_0a_1x1')\n          tower_conv2_1 = slim.conv2d(tower_conv2_0, 96, [3, 3],\n                                      scope='Conv2d_0b_3x3')\n          tower_conv2_2 = slim.conv2d(tower_conv2_1, 96, [3, 3],\n                                      scope='Conv2d_0c_3x3')\n        with tf.variable_scope('Branch_3'):\n          tower_pool = slim.avg_pool2d(net, [3, 3], scope='AvgPool_0a_3x3')\n          tower_pool_1 = slim.conv2d(tower_pool, 64, [1, 1],\n                                     scope='Conv2d_0b_1x1')\n        net = tf.concat([tower_conv, tower_conv1_1, tower_conv2_2, tower_pool_1], 3)\n\n      if add_and_check_final('Mixed_5b', net): return net, end_points\n\n      # 17 x 17 x 1088\n      with tf.variable_scope('Mixed_6a'):\n        with tf.variable_scope('Branch_0'):\n          tower_conv = slim.conv2d(net, 384, [3, 3], stride=2, padding='VALID',\n                                   scope='Conv2d_1a_3x3')\n        with tf.variable_scope('Branch_1'):\n          tower_conv1_0 = slim.conv2d(net, 256, [1, 1], scope='Conv2d_0a_1x1')\n          tower_conv1_1 = slim.conv2d(tower_conv1_0, 256, [3, 3],\n                                      scope='Conv2d_0b_3x3')\n          tower_conv1_2 = slim.conv2d(tower_conv1_1, 384, [3, 3], stride=2,\n                                      padding='VALID', scope='Conv2d_1a_3x3')\n        with tf.variable_scope('Branch_2'):\n          tower_pool = slim.max_pool2d(net, [3, 3], stride=2, padding='VALID',\n                                       scope='MaxPool_1a_3x3')\n        net = tf.concat([tower_conv, tower_conv1_2, tower_pool], 3)\n\n      if add_and_check_final('Mixed_6a', net): return net, end_points\n\n      # 8 x 8 x 2080\n      with tf.variable_scope('Mixed_7a'):\n        with tf.variable_scope('Branch_0'):\n          tower_conv = slim.conv2d(net, 256, [1, 1], scope='Conv2d_0a_1x1')\n          tower_conv_1 = slim.conv2d(tower_conv, 384, [3, 3], stride=2,\n                                     padding='VALID', scope='Conv2d_1a_3x3')\n        with tf.variable_scope('Branch_1'):\n          tower_conv1 = slim.conv2d(net, 256, [1, 1], scope='Conv2d_0a_1x1')\n          tower_conv1_1 = slim.conv2d(tower_conv1, 288, [3, 3], stride=2,\n                                      padding='VALID', scope='Conv2d_1a_3x3')\n        with tf.variable_scope('Branch_2'):\n          tower_conv2 = slim.conv2d(net, 256, [1, 1], scope='Conv2d_0a_1x1')\n          tower_conv2_1 = slim.conv2d(tower_conv2, 288, [3, 3], scope='Conv2d_0b_3x3')\n          tower_conv2_2 = slim.conv2d(tower_conv2_1, 320, [3, 3], stride=2,\n                                      padding='VALID', scope='Conv2d_1a_3x3')\n        with tf.variable_scope('Branch_3'):\n          tower_pool = slim.max_pool2d(net, [3, 3], stride=2, padding='VALID',\n                                       scope='MaxPool_1a_3x3')\n        net = tf.concat([tower_conv_1, tower_conv1_1, tower_conv2_2, tower_pool], 3)\n\n      if add_and_check_final('Mixed_7a', net): return net, end_points\n\n      # 8 x 8 x 1536\n      net = slim.conv2d(net, 1536, [1, 1], scope='Conv2d_7b_1x1')\n      if add_and_check_final('Conv2d_7b_1x1', net): return net, end_points\n\n  raise ValueError('Unknown final endpoint %s' % final_endpoint)", "license": "apache-2.0", "author": "Ao-Lee <ao-li-2008@hotmail.com", "project": "Ao-Lee_FacialVarification", "signature": "def inception_resnet_v2_base(inputs,", "license_type": "Permissive", "init_header": "# Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\"\"\"Contains the definition of the Inception Resnet V2 architecture.\n\nAs described in http://arxiv.org/abs/1602.07261.\n\n  Inception-v4, Inception-ResNet and the Impact of Residual Connections\n    on Learning\n  Christian Szegedy, Sergey Ioffe, Vincent Vanhoucke, Alex Alemi\n\"\"\"\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\n\nimport tensorflow as tf\n\nslim = tf.contrib.slim\n\n\ndef inception_resnet_v2_base(inputs,\n                             final_endpoint='Conv2d_7b_1x1',\n                             output_stride=16,\n                             align_feature_maps=False,\n                             scope=None,\n                             activation_fn=tf.nn.relu):\n  \"\"\"Inception model from  http://arxiv.org/abs/1602.07261.\n\n  Constructs an Inception Resnet v2 network from inputs to the given final\n  endpoint. This method can construct the network up to the final inception\n  block Conv2d_7b_1x1.\n\n  Args:\n    inputs: a tensor of size [batch_size, height, width, channels].\n    final_endpoint: specifies the endpoint to construct the network up to. It\n      can be one of ['Conv2d_1a_3x3', 'Conv2d_2a_3x3', 'Conv2d_2b_3x3',\n      'MaxPool_3a_3x3', 'Conv2d_3b_1x1', 'Conv2d_4a_3x3', 'MaxPool_5a_3x3',\n      'Mixed_5b', 'Mixed_6a', 'PreAuxLogits', 'Mixed_7a', 'Conv2d_7b_1x1']\n    output_stride: A scalar that specifies the requested ratio of input to\n      output spatial resolution. Only supports 8 and 16.\n    align_feature_maps: When true, changes all the VALID paddings in the network\n      to SAME padding so that the feature maps are aligned.\n    scope: Optional variable_scope.\n    activation_fn: Activation function for block scopes.\n\n  Returns:\n    tensor_out: output tensor corresponding to the final_endpoint.\n    end_points: a set of activations for external use, for example summaries or\n                losses.\n\n  Raises:\n    ValueError: if final_endpoint is not set to one of the predefined values,\n      or if the output_stride is not 8 or 16, or if the output_stride is 8 and\n      we request an end point after 'PreAuxLogits'.\n  \"\"\"", "docstring": "                             final_endpoint='Conv2d_7b_1x1',\n                             output_stride=16,\n                             align_feature_maps=False,\n                             scope=None,\n                             activation_fn=tf.nn.relu):\n  \"\"\"Inception model from  http://arxiv.org/abs/1602.07261.\n\n  Constructs an Inception Resnet v2 network from inputs to the given final\n  endpoint. This method can construct the network up to the final inception\n  block Conv2d_7b_1x1.\n\n  Args:\n    inputs: a tensor of size [batch_size, height, width, channels].\n    final_endpoint: specifies the endpoint to construct the network up to. It\n      can be one of ['Conv2d_1a_3x3', 'Conv2d_2a_3x3', 'Conv2d_2b_3x3',\n      'MaxPool_3a_3x3', 'Conv2d_3b_1x1', 'Conv2d_4a_3x3', 'MaxPool_5a_3x3',\n      'Mixed_5b', 'Mixed_6a', 'PreAuxLogits', 'Mixed_7a', 'Conv2d_7b_1x1']\n    output_stride: A scalar that specifies the requested ratio of input to\n      output spatial resolution. Only supports 8 and 16.\n    align_feature_maps: When true, changes all the VALID paddings in the network\n      to SAME padding so that the feature maps are aligned.\n    scope: Optional variable_scope.\n    activation_fn: Activation function for block scopes.\n\n  Returns:\n    tensor_out: output tensor corresponding to the final_endpoint.\n    end_points: a set of activations for external use, for example summaries or\n                losses.\n\n  Raises:\n    ValueError: if final_endpoint is not set to one of the predefined values,\n      or if the output_stride is not 8 or 16, or if the output_stride is 8 and\n      we request an end point after 'PreAuxLogits'.\n  \"\"\""}, "259c8f1c13ee3c4ef85c8c57adf935289544c088": {"bleu": 0.6583709471734647, "jaccard": 0.66015625, "edit_distance": 0.7289473684210526, "comment_cnt": 3, "max_sim": 0.7289473684210526, "header": "\"\"\"\nGenerate samples of synthetic data sets.\n\"\"\"\n\n\nimport numbers\nimport array\nimport numpy as np\nfrom scipy import linalg\nimport scipy.sparse as sp\n\nfrom ..preprocessing import MultiLabelBinarizer\nfrom ..utils import check_array, check_random_state\nfrom ..utils import shuffle as util_shuffle\nfrom ..utils.random import sample_without_replacement\nfrom ..externals import six\nmap = six.moves.map\nzip = six.moves.zip\n\n\ndef make_regression(n_samples=100, n_features=100, n_informative=10,\n                    n_targets=1, bias=0.0, effective_rank=None,\n                    tail_strength=0.5, noise=0.0, shuffle=True, coef=False,\n                    random_state=None):\n    \"\"\"Generate a random regression problem.\n\n    The input set can either be well conditioned (by default) or have a low\n    rank-fat tail singular profile. See :func:`make_low_rank_matrix` for\n    more details.\n\n    The output is generated by applying a (potentially biased) random linear\n    regression model with `n_informative` nonzero regressors to the previously\n    generated input and some gaussian centered noise with some adjustable\n    scale.\n\n    Read more in the :ref:`User Guide <sample_generators>`.\n\n    Parameters\n    ----------\n    n_samples : int, optional (default=100)\n        The number of samples.\n\n    n_features : int, optional (default=100)\n        The number of features.\n\n    n_informative : int, optional (default=10)\n        The number of informative features, i.e., the number of features used\n        to build the linear model used to generate the output.\n\n    n_targets : int, optional (default=1)\n        The number of regression targets, i.e., the dimension of the y output\n        vector associated with a sample. By default, the output is a scalar.\n\n    bias : float, optional (default=0.0)\n        The bias term in the underlying linear model.\n\n    effective_rank : int or None, optional (default=None)\n        if not None:\n            The approximate number of singular vectors required to explain most\n            of the input data by linear combinations. Using this kind of\n            singular spectrum in the input allows the generator to reproduce\n            the correlations often observed in practice.\n        if None:\n            The input set is well conditioned, centered and gaussian with\n            unit variance.\n\n    tail_strength : float between 0.0 and 1.0, optional (default=0.5)\n        The relative importance of the fat noisy tail of the singular values\n        profile if `effective_rank` is not None.\n\n    noise : float, optional (default=0.0)\n        The standard deviation of the gaussian noise applied to the output.\n\n    shuffle : boolean, optional (default=True)\n        Shuffle the samples and the features.\n\n    coef : boolean, optional (default=False)\n        If True, the coefficients of the underlying linear model are returned.\n\n    random_state : int, RandomState instance or None, optional (default=None)\n        If int, random_state is the seed used by the random number generator;\n        If RandomState instance, random_state is the random number generator;\n        If None, the random number generator is the RandomState instance used\n        by `np.random`.\n\n    Returns\n    -------\n    X : array of shape [n_samples, n_features]\n        The input samples.\n\n    y : array of shape [n_samples] or [n_samples, n_targets]\n        The output values.\n\n    coef : array of shape [n_features] or [n_features, n_targets], optional\n        The coefficient of the underlying linear model. It is returned only if\n        coef is True.\n    \"\"\"", "body": "    n_informative = min(n_features, n_informative)\n    generator = check_random_state(random_state)\n\n    if effective_rank is None:\n        # Randomly generate a well conditioned input set\n        X = generator.randn(n_samples, n_features)\n\n    else:\n        # Randomly generate a low rank, fat tail input set\n        X = make_low_rank_matrix(n_samples=n_samples,\n                                 n_features=n_features,\n                                 effective_rank=effective_rank,\n                                 tail_strength=tail_strength,\n                                 random_state=generator)\n\n    # Generate a ground truth model with only n_informative features being non\n    # zeros (the other features are not correlated to y and should be ignored\n    # by a sparsifying regularizers such as L1 or elastic net)\n    ground_truth = np.zeros((n_features, n_targets))\n    ground_truth[:n_informative, :] = 100 * generator.rand(n_informative,\n                                                           n_targets)\n\n    y = np.dot(X, ground_truth) + bias\n\n    # Add noise\n    if noise > 0.0:\n        y += generator.normal(scale=noise, size=y.shape)\n\n    # Randomly permute samples and features\n    if shuffle:\n        X, y = util_shuffle(X, y, random_state=generator)\n\n        indices = np.arange(n_features)\n        generator.shuffle(indices)\n        X[:, :] = X[:, indices]\n        ground_truth = ground_truth[indices]\n\n    y = np.squeeze(y)\n\n    if coef:\n        return X, y, np.squeeze(ground_truth)\n\n    else:\n        return X, y", "answer": "generator = check_random_state(random_state)\n\n    if effective_rank is None:\n        # Randomly generate a well conditioned input set\n        X = generator.randn(n_samples, n_features)\n    else:\n        # Randomly generate a low rank, fat tail input set\n        X = make_low_rank_matrix(n_samples=n_samples, n_features=n_features,\n                                 effective_rank=effective_rank,\n                                 tail_strength=tail_strength,\n                                 random_state=generator)\n\n    # Generate a ground truth model with only n_informative features being non-zero\n    ground_truth = np.zeros((n_features, n_targets))\n    ground_truth[:n_informative, :] = 100 * generator.rand(n_informative, n_targets)\n\n    y = np.dot(X, ground_truth) + bias\n\n    # Add noise\n    if noise > 0.0:\n        y += generator.normal(scale=noise, size=y.shape)\n\n    # Optionally shuffle the dataset\n    if shuffle:\n        X, y = util_shuffle(X, y, random_state=generator)\n\n    y = np.squeeze(y)\n\n    if coef:\n        return X, y, np.squeeze(ground_truth)\n\n    return X, y", "license": "bsd-3-clause", "author": "Joel Nothman <joel.nothman@gmail.com", "project": "scikit-learn_scikit-learn", "signature": "def make_regression(n_samples=100, n_features=100, n_informative=10,", "license_type": "Permissive", "init_header": "\"\"\"\nGenerate samples of synthetic data sets.\n\"\"\"\n\n# Authors: B. Thirion, G. Varoquaux, A. Gramfort, V. Michel, O. Grisel,\n#          G. Louppe, J. Nothman\n# License: BSD 3 clause\n\nimport numbers\nimport array\nimport numpy as np\nfrom scipy import linalg\nimport scipy.sparse as sp\n\nfrom ..preprocessing import MultiLabelBinarizer\nfrom ..utils import check_array, check_random_state\nfrom ..utils import shuffle as util_shuffle\nfrom ..utils.random import sample_without_replacement\nfrom ..externals import six\nmap = six.moves.map\nzip = six.moves.zip\n\n\ndef make_regression(n_samples=100, n_features=100, n_informative=10,\n                    n_targets=1, bias=0.0, effective_rank=None,\n                    tail_strength=0.5, noise=0.0, shuffle=True, coef=False,\n                    random_state=None):\n    \"\"\"Generate a random regression problem.\n\n    The input set can either be well conditioned (by default) or have a low\n    rank-fat tail singular profile. See :func:`make_low_rank_matrix` for\n    more details.\n\n    The output is generated by applying a (potentially biased) random linear\n    regression model with `n_informative` nonzero regressors to the previously\n    generated input and some gaussian centered noise with some adjustable\n    scale.\n\n    Read more in the :ref:`User Guide <sample_generators>`.\n\n    Parameters\n    ----------\n    n_samples : int, optional (default=100)\n        The number of samples.\n\n    n_features : int, optional (default=100)\n        The number of features.\n\n    n_informative : int, optional (default=10)\n        The number of informative features, i.e., the number of features used\n        to build the linear model used to generate the output.\n\n    n_targets : int, optional (default=1)\n        The number of regression targets, i.e., the dimension of the y output\n        vector associated with a sample. By default, the output is a scalar.\n\n    bias : float, optional (default=0.0)\n        The bias term in the underlying linear model.\n\n    effective_rank : int or None, optional (default=None)\n        if not None:\n            The approximate number of singular vectors required to explain most\n            of the input data by linear combinations. Using this kind of\n            singular spectrum in the input allows the generator to reproduce\n            the correlations often observed in practice.\n        if None:\n            The input set is well conditioned, centered and gaussian with\n            unit variance.\n\n    tail_strength : float between 0.0 and 1.0, optional (default=0.5)\n        The relative importance of the fat noisy tail of the singular values\n        profile if `effective_rank` is not None.\n\n    noise : float, optional (default=0.0)\n        The standard deviation of the gaussian noise applied to the output.\n\n    shuffle : boolean, optional (default=True)\n        Shuffle the samples and the features.\n\n    coef : boolean, optional (default=False)\n        If True, the coefficients of the underlying linear model are returned.\n\n    random_state : int, RandomState instance or None, optional (default=None)\n        If int, random_state is the seed used by the random number generator;\n        If RandomState instance, random_state is the random number generator;\n        If None, the random number generator is the RandomState instance used\n        by `np.random`.\n\n    Returns\n    -------\n    X : array of shape [n_samples, n_features]\n        The input samples.\n\n    y : array of shape [n_samples] or [n_samples, n_targets]\n        The output values.\n\n    coef : array of shape [n_features] or [n_features, n_targets], optional\n        The coefficient of the underlying linear model. It is returned only if\n        coef is True.\n    \"\"\"", "docstring": "                    n_targets=1, bias=0.0, effective_rank=None,\n                    tail_strength=0.5, noise=0.0, shuffle=True, coef=False,\n                    random_state=None):\n    \"\"\"Generate a random regression problem.\n\n    The input set can either be well conditioned (by default) or have a low\n    rank-fat tail singular profile. See :func:`make_low_rank_matrix` for\n    more details.\n\n    The output is generated by applying a (potentially biased) random linear\n    regression model with `n_informative` nonzero regressors to the previously\n    generated input and some gaussian centered noise with some adjustable\n    scale.\n\n    Read more in the :ref:`User Guide <sample_generators>`.\n\n    Parameters\n    ----------\n    n_samples : int, optional (default=100)\n        The number of samples.\n\n    n_features : int, optional (default=100)\n        The number of features.\n\n    n_informative : int, optional (default=10)\n        The number of informative features, i.e., the number of features used\n        to build the linear model used to generate the output.\n\n    n_targets : int, optional (default=1)\n        The number of regression targets, i.e., the dimension of the y output\n        vector associated with a sample. By default, the output is a scalar.\n\n    bias : float, optional (default=0.0)\n        The bias term in the underlying linear model.\n\n    effective_rank : int or None, optional (default=None)\n        if not None:\n            The approximate number of singular vectors required to explain most\n            of the input data by linear combinations. Using this kind of\n            singular spectrum in the input allows the generator to reproduce\n            the correlations often observed in practice.\n        if None:\n            The input set is well conditioned, centered and gaussian with\n            unit variance.\n\n    tail_strength : float between 0.0 and 1.0, optional (default=0.5)\n        The relative importance of the fat noisy tail of the singular values\n        profile if `effective_rank` is not None.\n\n    noise : float, optional (default=0.0)\n        The standard deviation of the gaussian noise applied to the output.\n\n    shuffle : boolean, optional (default=True)\n        Shuffle the samples and the features.\n\n    coef : boolean, optional (default=False)\n        If True, the coefficients of the underlying linear model are returned.\n\n    random_state : int, RandomState instance or None, optional (default=None)\n        If int, random_state is the seed used by the random number generator;\n        If RandomState instance, random_state is the random number generator;\n        If None, the random number generator is the RandomState instance used\n        by `np.random`.\n\n    Returns\n    -------\n    X : array of shape [n_samples, n_features]\n        The input samples.\n\n    y : array of shape [n_samples] or [n_samples, n_targets]\n        The output values.\n\n    coef : array of shape [n_features] or [n_features, n_targets], optional\n        The coefficient of the underlying linear model. It is returned only if\n        coef is True.\n    \"\"\""}, "0da0144172703b6ba05ce9902ba7aff33a5144a9": {"bleu": 0.657791131929742, "jaccard": 0.671875, "edit_distance": 0.7519747235387046, "comment_cnt": 8, "max_sim": 0.7519747235387046, "header": "\"\"\"K-means clustering\"\"\"\n\n\nimport warnings\n\nimport numpy as np\nimport scipy.sparse as sp\n\nfrom ..base import BaseEstimator, ClusterMixin, TransformerMixin\nfrom ..metrics.pairwise import euclidean_distances\nfrom ..metrics.pairwise import pairwise_distances_argmin_min\nfrom ..utils.extmath import row_norms, squared_norm, stable_cumsum\nfrom ..utils.sparsefuncs_fast import assign_rows_csr\nfrom ..utils.sparsefuncs import mean_variance_axis\nfrom ..utils import check_array\nfrom ..utils import check_random_state\nfrom ..utils import as_float_array\nfrom ..utils import gen_batches\nfrom ..utils.validation import check_is_fitted\nfrom ..utils.validation import FLOAT_DTYPES\nfrom ..externals.joblib import Parallel\nfrom ..externals.joblib import delayed\nfrom ..externals.six import string_types\n\nfrom . import _k_means\nfrom ._k_means_elkan import k_means_elkan\n\n\n\n\ndef _k_init(X, n_clusters, x_squared_norms, random_state, n_local_trials=None):\n    \"\"\"Init n_clusters seeds according to k-means++\n\n    Parameters\n    -----------\n    X : array or sparse matrix, shape (n_samples, n_features)\n        The data to pick seeds for. To avoid memory copy, the input data\n        should be double precision (dtype=np.float64).\n\n    n_clusters : integer\n        The number of seeds to choose\n\n    x_squared_norms : array, shape (n_samples,)\n        Squared Euclidean norm of each data point.\n\n    random_state : numpy.RandomState\n        The generator used to initialize the centers.\n\n    n_local_trials : integer, optional\n        The number of seeding trials for each center (except the first),\n        of which the one reducing inertia the most is greedily chosen.\n        Set to None to make the number of trials depend logarithmically\n        on the number of seeds (2+log(k)); this is the default.\n\n    Notes\n    -----\n    Selects initial cluster centers for k-mean clustering in a smart way\n    to speed up convergence. see: Arthur, D. and Vassilvitskii, S.\n    \"k-means++: the advantages of careful seeding\". ACM-SIAM symposium\n    on Discrete algorithms. 2007\n\n    Version ported from http://www.stanford.edu/~darthur/kMeansppTest.zip,\n    which is the implementation used in the aforementioned paper.\n    \"\"\"", "body": "    n_samples, n_features = X.shape\n\n    centers = np.empty((n_clusters, n_features), dtype=X.dtype)\n\n    assert x_squared_norms is not None, 'x_squared_norms None in _k_init'\n\n    # Set the number of local seeding trials if none is given\n    if n_local_trials is None:\n        # This is what Arthur/Vassilvitskii tried, but did not report\n        # specific results for other than mentioning in the conclusion\n        # that it helped.\n        n_local_trials = 2 + int(np.log(n_clusters))\n\n    # Pick first center randomly\n    center_id = random_state.randint(n_samples)\n    if sp.issparse(X):\n        centers[0] = X[center_id].toarray()\n    else:\n        centers[0] = X[center_id]\n\n    # Initialize list of closest distances and calculate current potential\n    closest_dist_sq = euclidean_distances(\n        centers[0, np.newaxis], X, Y_norm_squared=x_squared_norms,\n        squared=True)\n    current_pot = closest_dist_sq.sum()\n\n    # Pick the remaining n_clusters-1 points\n    for c in range(1, n_clusters):\n        # Choose center candidates by sampling with probability proportional\n        # to the squared distance to the closest existing center\n        rand_vals = random_state.random_sample(n_local_trials) * current_pot\n        candidate_ids = np.searchsorted(stable_cumsum(closest_dist_sq),\n                                        rand_vals)\n\n        # Compute distances to center candidates\n        distance_to_candidates = euclidean_distances(\n            X[candidate_ids], X, Y_norm_squared=x_squared_norms, squared=True)\n\n        # Decide which candidate is the best\n        best_candidate = None\n        best_pot = None\n        best_dist_sq = None\n        for trial in range(n_local_trials):\n            # Compute potential when including center candidate\n            new_dist_sq = np.minimum(closest_dist_sq,\n                                     distance_to_candidates[trial])\n            new_pot = new_dist_sq.sum()\n\n            # Store result if it is the best local trial so far\n            if (best_candidate is None) or (new_pot < best_pot):\n                best_candidate = candidate_ids[trial]\n                best_pot = new_pot\n                best_dist_sq = new_dist_sq\n\n        # Permanently add best center candidate found in local tries\n        if sp.issparse(X):\n            centers[c] = X[best_candidate].toarray()\n        else:\n            centers[c] = X[best_candidate]\n        current_pot = best_pot\n        closest_dist_sq = best_dist_sq\n\n    return centers", "answer": "n_samples, n_features = X.shape\n\n    centers = np.empty((n_clusters, n_features), dtype=X.dtype)\n\n    # Set the number of local seeding trials if none is given\n    if n_local_trials is None:\n        n_local_trials = 2 + int(np.log(n_clusters))\n\n    # Pick first center randomly\n    center_id = random_state.randint(n_samples)\n    centers[0] = X[center_id]\n\n    # Initialize list of closest distances and calculate current potential\n    closest_dist_sq = euclidean_distances(\n        centers[0, np.newaxis], X, Y_norm_squared=x_squared_norms, squared=True)\n    current_pot = closest_dist_sq.sum()\n\n    # Pick the remaining n_clusters-1 points\n    for c in range(1, n_clusters):\n        # Choose center candidates by sampling with probability proportional\n        # to the squared distance to the closest existing center\n        rand_vals = random_state.random_sample(n_local_trials) * current_pot\n        candidate_ids = np.searchsorted(stable_cumsum(closest_dist_sq), rand_vals)\n\n        # Compute distances to center candidates\n        distance_to_candidates = euclidean_distances(\n            X[candidate_ids], X, Y_norm_squared=x_squared_norms, squared=True)\n\n        # Decide which candidate is the best\n        best_candidate = None\n        best_pot = None\n        best_dist_sq = None\n        for trial in range(n_local_trials):\n            new_dist_sq = np.minimum(closest_dist_sq, distance_to_candidates[trial])\n            new_pot = new_dist_sq.sum()\n\n            if (best_candidate is None) or (new_pot < best_pot):\n                best_candidate = candidate_ids[trial]\n                best_pot = new_pot\n                best_dist_sq = new_dist_sq\n\n        centers[c] = X[best_candidate]\n        current_pot = best_pot\n        closest_dist_sq = best_dist_sq\n\n    return centers", "license": "bsd-3-clause", "author": "Vrishank Bhardwaj <vrishank1997@gmail.com", "project": "scikit-learn_scikit-learn", "signature": "def _k_init(X, n_clusters, x_squared_norms, random_state, n_local_trials=None):", "license_type": "Permissive", "init_header": "\"\"\"K-means clustering\"\"\"\n\n# Authors: Gael Varoquaux <gael.varoquaux@normalesup.org>\n#          Thomas Rueckstiess <ruecksti@in.tum.de>\n#          James Bergstra <james.bergstra@umontreal.ca>\n#          Jan Schlueter <scikit-learn@jan-schlueter.de>\n#          Nelle Varoquaux\n#          Peter Prettenhofer <peter.prettenhofer@gmail.com>\n#          Olivier Grisel <olivier.grisel@ensta.org>\n#          Mathieu Blondel <mathieu@mblondel.org>\n#          Robert Layton <robertlayton@gmail.com>\n# License: BSD 3 clause\n\nimport warnings\n\nimport numpy as np\nimport scipy.sparse as sp\n\nfrom ..base import BaseEstimator, ClusterMixin, TransformerMixin\nfrom ..metrics.pairwise import euclidean_distances\nfrom ..metrics.pairwise import pairwise_distances_argmin_min\nfrom ..utils.extmath import row_norms, squared_norm, stable_cumsum\nfrom ..utils.sparsefuncs_fast import assign_rows_csr\nfrom ..utils.sparsefuncs import mean_variance_axis\nfrom ..utils import check_array\nfrom ..utils import check_random_state\nfrom ..utils import as_float_array\nfrom ..utils import gen_batches\nfrom ..utils.validation import check_is_fitted\nfrom ..utils.validation import FLOAT_DTYPES\nfrom ..externals.joblib import Parallel\nfrom ..externals.joblib import delayed\nfrom ..externals.six import string_types\n\nfrom . import _k_means\nfrom ._k_means_elkan import k_means_elkan\n\n\n###############################################################################\n# Initialization heuristic\n\n\ndef _k_init(X, n_clusters, x_squared_norms, random_state, n_local_trials=None):\n    \"\"\"Init n_clusters seeds according to k-means++\n\n    Parameters\n    -----------\n    X : array or sparse matrix, shape (n_samples, n_features)\n        The data to pick seeds for. To avoid memory copy, the input data\n        should be double precision (dtype=np.float64).\n\n    n_clusters : integer\n        The number of seeds to choose\n\n    x_squared_norms : array, shape (n_samples,)\n        Squared Euclidean norm of each data point.\n\n    random_state : numpy.RandomState\n        The generator used to initialize the centers.\n\n    n_local_trials : integer, optional\n        The number of seeding trials for each center (except the first),\n        of which the one reducing inertia the most is greedily chosen.\n        Set to None to make the number of trials depend logarithmically\n        on the number of seeds (2+log(k)); this is the default.\n\n    Notes\n    -----\n    Selects initial cluster centers for k-mean clustering in a smart way\n    to speed up convergence. see: Arthur, D. and Vassilvitskii, S.\n    \"k-means++: the advantages of careful seeding\". ACM-SIAM symposium\n    on Discrete algorithms. 2007\n\n    Version ported from http://www.stanford.edu/~darthur/kMeansppTest.zip,\n    which is the implementation used in the aforementioned paper.\n    \"\"\"", "docstring": "    \"\"\"Init n_clusters seeds according to k-means++\n\n    Parameters\n    -----------\n    X : array or sparse matrix, shape (n_samples, n_features)\n        The data to pick seeds for. To avoid memory copy, the input data\n        should be double precision (dtype=np.float64).\n\n    n_clusters : integer\n        The number of seeds to choose\n\n    x_squared_norms : array, shape (n_samples,)\n        Squared Euclidean norm of each data point.\n\n    random_state : numpy.RandomState\n        The generator used to initialize the centers.\n\n    n_local_trials : integer, optional\n        The number of seeding trials for each center (except the first),\n        of which the one reducing inertia the most is greedily chosen.\n        Set to None to make the number of trials depend logarithmically\n        on the number of seeds (2+log(k)); this is the default.\n\n    Notes\n    -----\n    Selects initial cluster centers for k-mean clustering in a smart way\n    to speed up convergence. see: Arthur, D. and Vassilvitskii, S.\n    \"k-means++: the advantages of careful seeding\". ACM-SIAM symposium\n    on Discrete algorithms. 2007\n\n    Version ported from http://www.stanford.edu/~darthur/kMeansppTest.zip,\n    which is the implementation used in the aforementioned paper.\n    \"\"\""}, "f831a1326179a9e891fd06c62b7b756c037a4bb4": {"bleu": 0.832556520770572, "jaccard": 0.6484375, "edit_distance": 0.7847222222222222, "comment_cnt": 1, "max_sim": 0.832556520770572, "header": "\"\"\"Metrics to assess performance on regression task\n\nFunctions named as ``*_score`` return a scalar value to maximize: the higher\nthe better\n\nFunction named as ``*_error`` or ``*_loss`` return a scalar value to minimize:\nthe lower the better\n\"\"\"\n\n\nfrom __future__ import division\n\nimport numpy as np\n\nfrom ..utils.validation import check_array, check_consistent_length\nfrom ..utils.validation import column_or_1d\nfrom ..externals.six import string_types\n\n\n__ALL__ = [\n    \"mean_absolute_error\",\n    \"mean_squared_error\",\n    \"mean_squared_log_error\",\n    \"median_absolute_error\",\n    \"r2_score\",\n    \"explained_variance_score\"\n]\n\n\ndef mean_absolute_error(y_true, y_pred,\n                        sample_weight=None,\n                        multioutput='uniform_average'):\n    \"\"\"Mean absolute error regression loss\n\n    Read more in the :ref:`User Guide <mean_absolute_error>`.\n\n    Parameters\n    ----------\n    y_true : array-like of shape = (n_samples) or (n_samples, n_outputs)\n        Ground truth (correct) target values.\n\n    y_pred : array-like of shape = (n_samples) or (n_samples, n_outputs)\n        Estimated target values.\n\n    sample_weight : array-like of shape = (n_samples), optional\n        Sample weights.\n\n    multioutput : string in ['raw_values', 'uniform_average']\n        or array-like of shape (n_outputs)\n        Defines aggregating of multiple output values.\n        Array-like value defines weights used to average errors.\n\n        'raw_values' :\n            Returns a full set of errors in case of multioutput input.\n\n        'uniform_average' :\n            Errors of all outputs are averaged with uniform weight.\n\n\n    Returns\n    -------\n    loss : float or ndarray of floats\n        If multioutput is 'raw_values', then mean absolute error is returned\n        for each output separately.\n        If multioutput is 'uniform_average' or an ndarray of weights, then the\n        weighted average of all output errors is returned.\n\n        MAE output is non-negative floating point. The best value is 0.0.\n\n    Examples\n    --------\n    >>> from sklearn.metrics import mean_absolute_error\n    >>> y_true = [3, -0.5, 2, 7]\n    >>> y_pred = [2.5, 0.0, 2, 8]\n    >>> mean_absolute_error(y_true, y_pred)\n    0.5\n    >>> y_true = [[0.5, 1], [-1, 1], [7, -6]]\n    >>> y_pred = [[0, 2], [-1, 2], [8, -5]]\n    >>> mean_absolute_error(y_true, y_pred)\n    0.75\n    >>> mean_absolute_error(y_true, y_pred, multioutput='raw_values')\n    array([ 0.5,  1. ])\n    >>> mean_absolute_error(y_true, y_pred, multioutput=[0.3, 0.7])\n    ... # doctest: +ELLIPSIS\n    0.849...\n    \"\"\"", "body": "    y_type, y_true, y_pred, multioutput = _check_reg_targets(\n        y_true, y_pred, multioutput)\n    output_errors = np.average(np.abs(y_pred - y_true),\n                               weights=sample_weight, axis=0)\n    if isinstance(multioutput, string_types):\n        if multioutput == 'raw_values':\n            return output_errors\n        elif multioutput == 'uniform_average':\n            # pass None as weights to np.average: uniform mean\n            multioutput = None\n\n    return np.average(output_errors, weights=multioutput)", "answer": "y_true = check_array(y_true)\n    y_pred = check_array(y_pred)\n    check_consistent_length(y_true, y_pred, sample_weight)\n    \n    output_errors = np.average(np.abs(y_pred - y_true), weights=sample_weight, axis=0)\n    \n    if isinstance(multioutput, string_types):\n        if multioutput == 'raw_values':\n            return output_errors\n        elif multioutput == 'uniform_average':\n            # pass None as weights to np.average: uniform mean\n            multioutput = None\n    return np.average(output_errors, weights=multioutput)", "license": "bsd-3-clause", "author": "Aashil <aashil@aashilpatel.com", "project": "scikit-learn_scikit-learn", "signature": "def mean_absolute_error(y_true, y_pred,", "license_type": "Permissive", "init_header": "\"\"\"Metrics to assess performance on regression task\n\nFunctions named as ``*_score`` return a scalar value to maximize: the higher\nthe better\n\nFunction named as ``*_error`` or ``*_loss`` return a scalar value to minimize:\nthe lower the better\n\"\"\"\n\n# Authors: Alexandre Gramfort <alexandre.gramfort@inria.fr>\n#          Mathieu Blondel <mathieu@mblondel.org>\n#          Olivier Grisel <olivier.grisel@ensta.org>\n#          Arnaud Joly <a.joly@ulg.ac.be>\n#          Jochen Wersdorfer <jochen@wersdoerfer.de>\n#          Lars Buitinck\n#          Joel Nothman <joel.nothman@gmail.com>\n#          Karan Desai <karandesai281196@gmail.com>\n#          Noel Dawe <noel@dawe.me>\n#          Manoj Kumar <manojkumarsivaraj334@gmail.com>\n#          Michael Eickenberg <michael.eickenberg@gmail.com>\n#          Konstantin Shmelkov <konstantin.shmelkov@polytechnique.edu>\n# License: BSD 3 clause\n\nfrom __future__ import division\n\nimport numpy as np\n\nfrom ..utils.validation import check_array, check_consistent_length\nfrom ..utils.validation import column_or_1d\nfrom ..externals.six import string_types\n\n\n__ALL__ = [\n    \"mean_absolute_error\",\n    \"mean_squared_error\",\n    \"mean_squared_log_error\",\n    \"median_absolute_error\",\n    \"r2_score\",\n    \"explained_variance_score\"\n]\n\n\ndef mean_absolute_error(y_true, y_pred,\n                        sample_weight=None,\n                        multioutput='uniform_average'):\n    \"\"\"Mean absolute error regression loss\n\n    Read more in the :ref:`User Guide <mean_absolute_error>`.\n\n    Parameters\n    ----------\n    y_true : array-like of shape = (n_samples) or (n_samples, n_outputs)\n        Ground truth (correct) target values.\n\n    y_pred : array-like of shape = (n_samples) or (n_samples, n_outputs)\n        Estimated target values.\n\n    sample_weight : array-like of shape = (n_samples), optional\n        Sample weights.\n\n    multioutput : string in ['raw_values', 'uniform_average']\n        or array-like of shape (n_outputs)\n        Defines aggregating of multiple output values.\n        Array-like value defines weights used to average errors.\n\n        'raw_values' :\n            Returns a full set of errors in case of multioutput input.\n\n        'uniform_average' :\n            Errors of all outputs are averaged with uniform weight.\n\n\n    Returns\n    -------\n    loss : float or ndarray of floats\n        If multioutput is 'raw_values', then mean absolute error is returned\n        for each output separately.\n        If multioutput is 'uniform_average' or an ndarray of weights, then the\n        weighted average of all output errors is returned.\n\n        MAE output is non-negative floating point. The best value is 0.0.\n\n    Examples\n    --------\n    >>> from sklearn.metrics import mean_absolute_error\n    >>> y_true = [3, -0.5, 2, 7]\n    >>> y_pred = [2.5, 0.0, 2, 8]\n    >>> mean_absolute_error(y_true, y_pred)\n    0.5\n    >>> y_true = [[0.5, 1], [-1, 1], [7, -6]]\n    >>> y_pred = [[0, 2], [-1, 2], [8, -5]]\n    >>> mean_absolute_error(y_true, y_pred)\n    0.75\n    >>> mean_absolute_error(y_true, y_pred, multioutput='raw_values')\n    array([ 0.5,  1. ])\n    >>> mean_absolute_error(y_true, y_pred, multioutput=[0.3, 0.7])\n    ... # doctest: +ELLIPSIS\n    0.849...\n    \"\"\"", "docstring": "                        sample_weight=None,\n                        multioutput='uniform_average'):\n    \"\"\"Mean absolute error regression loss\n\n    Read more in the :ref:`User Guide <mean_absolute_error>`.\n\n    Parameters\n    ----------\n    y_true : array-like of shape = (n_samples) or (n_samples, n_outputs)\n        Ground truth (correct) target values.\n\n    y_pred : array-like of shape = (n_samples) or (n_samples, n_outputs)\n        Estimated target values.\n\n    sample_weight : array-like of shape = (n_samples), optional\n        Sample weights.\n\n    multioutput : string in ['raw_values', 'uniform_average']\n        or array-like of shape (n_outputs)\n        Defines aggregating of multiple output values.\n        Array-like value defines weights used to average errors.\n\n        'raw_values' :\n            Returns a full set of errors in case of multioutput input.\n\n        'uniform_average' :\n            Errors of all outputs are averaged with uniform weight.\n\n\n    Returns\n    -------\n    loss : float or ndarray of floats\n        If multioutput is 'raw_values', then mean absolute error is returned\n        for each output separately.\n        If multioutput is 'uniform_average' or an ndarray of weights, then the\n        weighted average of all output errors is returned.\n\n        MAE output is non-negative floating point. The best value is 0.0.\n\n    Examples\n    --------\n    >>> from sklearn.metrics import mean_absolute_error\n    >>> y_true = [3, -0.5, 2, 7]\n    >>> y_pred = [2.5, 0.0, 2, 8]\n    >>> mean_absolute_error(y_true, y_pred)\n    0.5\n    >>> y_true = [[0.5, 1], [-1, 1], [7, -6]]\n    >>> y_pred = [[0, 2], [-1, 2], [8, -5]]\n    >>> mean_absolute_error(y_true, y_pred)\n    0.75\n    >>> mean_absolute_error(y_true, y_pred, multioutput='raw_values')\n    array([ 0.5,  1. ])\n    >>> mean_absolute_error(y_true, y_pred, multioutput=[0.3, 0.7])\n    ... # doctest: +ELLIPSIS\n    0.849...\n    \"\"\""}, "f42eb9d83798c452212806e80fb662338d853ae1": {"bleu": 0.6583709471734647, "jaccard": 0.66015625, "edit_distance": 0.7289473684210526, "comment_cnt": 3, "max_sim": 0.7289473684210526, "header": "\"\"\"\nGenerate samples of synthetic data sets.\n\"\"\"\n\n\nimport numbers\nimport array\nfrom collections.abc import Iterable\n\nimport numpy as np\nfrom scipy import linalg\nimport scipy.sparse as sp\n\nfrom ..preprocessing import MultiLabelBinarizer\nfrom ..utils import check_array, check_random_state\nfrom ..utils import shuffle as util_shuffle\nfrom ..utils.random import sample_without_replacement\n\n\ndef make_regression(n_samples=100, n_features=100, n_informative=10,\n                    n_targets=1, bias=0.0, effective_rank=None,\n                    tail_strength=0.5, noise=0.0, shuffle=True, coef=False,\n                    random_state=None):\n    \"\"\"Generate a random regression problem.\n\n    The input set can either be well conditioned (by default) or have a low\n    rank-fat tail singular profile. See :func:`make_low_rank_matrix` for\n    more details.\n\n    The output is generated by applying a (potentially biased) random linear\n    regression model with `n_informative` nonzero regressors to the previously\n    generated input and some gaussian centered noise with some adjustable\n    scale.\n\n    Read more in the :ref:`User Guide <sample_generators>`.\n\n    Parameters\n    ----------\n    n_samples : int, optional (default=100)\n        The number of samples.\n\n    n_features : int, optional (default=100)\n        The number of features.\n\n    n_informative : int, optional (default=10)\n        The number of informative features, i.e., the number of features used\n        to build the linear model used to generate the output.\n\n    n_targets : int, optional (default=1)\n        The number of regression targets, i.e., the dimension of the y output\n        vector associated with a sample. By default, the output is a scalar.\n\n    bias : float, optional (default=0.0)\n        The bias term in the underlying linear model.\n\n    effective_rank : int or None, optional (default=None)\n        if not None:\n            The approximate number of singular vectors required to explain most\n            of the input data by linear combinations. Using this kind of\n            singular spectrum in the input allows the generator to reproduce\n            the correlations often observed in practice.\n        if None:\n            The input set is well conditioned, centered and gaussian with\n            unit variance.\n\n    tail_strength : float between 0.0 and 1.0, optional (default=0.5)\n        The relative importance of the fat noisy tail of the singular values\n        profile if `effective_rank` is not None.\n\n    noise : float, optional (default=0.0)\n        The standard deviation of the gaussian noise applied to the output.\n\n    shuffle : boolean, optional (default=True)\n        Shuffle the samples and the features.\n\n    coef : boolean, optional (default=False)\n        If True, the coefficients of the underlying linear model are returned.\n\n    random_state : int, RandomState instance or None (default)\n        Determines random number generation for dataset creation. Pass an int\n        for reproducible output across multiple function calls.\n        See :term:`Glossary <random_state>`.\n\n    Returns\n    -------\n    X : array of shape [n_samples, n_features]\n        The input samples.\n\n    y : array of shape [n_samples] or [n_samples, n_targets]\n        The output values.\n\n    coef : array of shape [n_features] or [n_features, n_targets], optional\n        The coefficient of the underlying linear model. It is returned only if\n        coef is True.\n    \"\"\"", "body": "    n_informative = min(n_features, n_informative)\n    generator = check_random_state(random_state)\n\n    if effective_rank is None:\n        # Randomly generate a well conditioned input set\n        X = generator.randn(n_samples, n_features)\n\n    else:\n        # Randomly generate a low rank, fat tail input set\n        X = make_low_rank_matrix(n_samples=n_samples,\n                                 n_features=n_features,\n                                 effective_rank=effective_rank,\n                                 tail_strength=tail_strength,\n                                 random_state=generator)\n\n    # Generate a ground truth model with only n_informative features being non\n    # zeros (the other features are not correlated to y and should be ignored\n    # by a sparsifying regularizers such as L1 or elastic net)\n    ground_truth = np.zeros((n_features, n_targets))\n    ground_truth[:n_informative, :] = 100 * generator.rand(n_informative,\n                                                           n_targets)\n\n    y = np.dot(X, ground_truth) + bias\n\n    # Add noise\n    if noise > 0.0:\n        y += generator.normal(scale=noise, size=y.shape)\n\n    # Randomly permute samples and features\n    if shuffle:\n        X, y = util_shuffle(X, y, random_state=generator)\n\n        indices = np.arange(n_features)\n        generator.shuffle(indices)\n        X[:, :] = X[:, indices]\n        ground_truth = ground_truth[indices]\n\n    y = np.squeeze(y)\n\n    if coef:\n        return X, y, np.squeeze(ground_truth)\n\n    else:\n        return X, y", "answer": "generator = check_random_state(random_state)\n\n    if effective_rank is None:\n        # Randomly generate a well conditioned input set\n        X = generator.randn(n_samples, n_features)\n    else:\n        # Randomly generate a low rank, fat tail input set\n        X = make_low_rank_matrix(n_samples=n_samples, n_features=n_features,\n                                 effective_rank=effective_rank,\n                                 tail_strength=tail_strength,\n                                 random_state=generator)\n\n    # Generate a ground truth model with only n_informative features being non-zero\n    ground_truth = np.zeros((n_features, n_targets))\n    ground_truth[:n_informative, :] = 100 * generator.rand(n_informative, n_targets)\n\n    y = np.dot(X, ground_truth) + bias\n\n    # Add noise\n    if noise > 0.0:\n        y += generator.normal(scale=noise, size=y.shape)\n\n    # Optionally shuffle the dataset\n    if shuffle:\n        X, y = util_shuffle(X, y, random_state=generator)\n\n    y = np.squeeze(y)\n\n    if coef:\n        return X, y, np.squeeze(ground_truth)\n\n    return X, y", "license": "bsd-3-clause", "author": "Roman Yurchak <rth.yurchak@gmail.com", "project": "scikit-learn_scikit-learn", "signature": "def make_regression(n_samples=100, n_features=100, n_informative=10,", "license_type": "Permissive", "init_header": "\"\"\"\nGenerate samples of synthetic data sets.\n\"\"\"\n\n# Authors: B. Thirion, G. Varoquaux, A. Gramfort, V. Michel, O. Grisel,\n#          G. Louppe, J. Nothman\n# License: BSD 3 clause\n\nimport numbers\nimport array\nfrom collections.abc import Iterable\n\nimport numpy as np\nfrom scipy import linalg\nimport scipy.sparse as sp\n\nfrom ..preprocessing import MultiLabelBinarizer\nfrom ..utils import check_array, check_random_state\nfrom ..utils import shuffle as util_shuffle\nfrom ..utils.random import sample_without_replacement\n\n\ndef make_regression(n_samples=100, n_features=100, n_informative=10,\n                    n_targets=1, bias=0.0, effective_rank=None,\n                    tail_strength=0.5, noise=0.0, shuffle=True, coef=False,\n                    random_state=None):\n    \"\"\"Generate a random regression problem.\n\n    The input set can either be well conditioned (by default) or have a low\n    rank-fat tail singular profile. See :func:`make_low_rank_matrix` for\n    more details.\n\n    The output is generated by applying a (potentially biased) random linear\n    regression model with `n_informative` nonzero regressors to the previously\n    generated input and some gaussian centered noise with some adjustable\n    scale.\n\n    Read more in the :ref:`User Guide <sample_generators>`.\n\n    Parameters\n    ----------\n    n_samples : int, optional (default=100)\n        The number of samples.\n\n    n_features : int, optional (default=100)\n        The number of features.\n\n    n_informative : int, optional (default=10)\n        The number of informative features, i.e., the number of features used\n        to build the linear model used to generate the output.\n\n    n_targets : int, optional (default=1)\n        The number of regression targets, i.e., the dimension of the y output\n        vector associated with a sample. By default, the output is a scalar.\n\n    bias : float, optional (default=0.0)\n        The bias term in the underlying linear model.\n\n    effective_rank : int or None, optional (default=None)\n        if not None:\n            The approximate number of singular vectors required to explain most\n            of the input data by linear combinations. Using this kind of\n            singular spectrum in the input allows the generator to reproduce\n            the correlations often observed in practice.\n        if None:\n            The input set is well conditioned, centered and gaussian with\n            unit variance.\n\n    tail_strength : float between 0.0 and 1.0, optional (default=0.5)\n        The relative importance of the fat noisy tail of the singular values\n        profile if `effective_rank` is not None.\n\n    noise : float, optional (default=0.0)\n        The standard deviation of the gaussian noise applied to the output.\n\n    shuffle : boolean, optional (default=True)\n        Shuffle the samples and the features.\n\n    coef : boolean, optional (default=False)\n        If True, the coefficients of the underlying linear model are returned.\n\n    random_state : int, RandomState instance or None (default)\n        Determines random number generation for dataset creation. Pass an int\n        for reproducible output across multiple function calls.\n        See :term:`Glossary <random_state>`.\n\n    Returns\n    -------\n    X : array of shape [n_samples, n_features]\n        The input samples.\n\n    y : array of shape [n_samples] or [n_samples, n_targets]\n        The output values.\n\n    coef : array of shape [n_features] or [n_features, n_targets], optional\n        The coefficient of the underlying linear model. It is returned only if\n        coef is True.\n    \"\"\"", "docstring": "                    n_targets=1, bias=0.0, effective_rank=None,\n                    tail_strength=0.5, noise=0.0, shuffle=True, coef=False,\n                    random_state=None):\n    \"\"\"Generate a random regression problem.\n\n    The input set can either be well conditioned (by default) or have a low\n    rank-fat tail singular profile. See :func:`make_low_rank_matrix` for\n    more details.\n\n    The output is generated by applying a (potentially biased) random linear\n    regression model with `n_informative` nonzero regressors to the previously\n    generated input and some gaussian centered noise with some adjustable\n    scale.\n\n    Read more in the :ref:`User Guide <sample_generators>`.\n\n    Parameters\n    ----------\n    n_samples : int, optional (default=100)\n        The number of samples.\n\n    n_features : int, optional (default=100)\n        The number of features.\n\n    n_informative : int, optional (default=10)\n        The number of informative features, i.e., the number of features used\n        to build the linear model used to generate the output.\n\n    n_targets : int, optional (default=1)\n        The number of regression targets, i.e., the dimension of the y output\n        vector associated with a sample. By default, the output is a scalar.\n\n    bias : float, optional (default=0.0)\n        The bias term in the underlying linear model.\n\n    effective_rank : int or None, optional (default=None)\n        if not None:\n            The approximate number of singular vectors required to explain most\n            of the input data by linear combinations. Using this kind of\n            singular spectrum in the input allows the generator to reproduce\n            the correlations often observed in practice.\n        if None:\n            The input set is well conditioned, centered and gaussian with\n            unit variance.\n\n    tail_strength : float between 0.0 and 1.0, optional (default=0.5)\n        The relative importance of the fat noisy tail of the singular values\n        profile if `effective_rank` is not None.\n\n    noise : float, optional (default=0.0)\n        The standard deviation of the gaussian noise applied to the output.\n\n    shuffle : boolean, optional (default=True)\n        Shuffle the samples and the features.\n\n    coef : boolean, optional (default=False)\n        If True, the coefficients of the underlying linear model are returned.\n\n    random_state : int, RandomState instance or None (default)\n        Determines random number generation for dataset creation. Pass an int\n        for reproducible output across multiple function calls.\n        See :term:`Glossary <random_state>`.\n\n    Returns\n    -------\n    X : array of shape [n_samples, n_features]\n        The input samples.\n\n    y : array of shape [n_samples] or [n_samples, n_targets]\n        The output values.\n\n    coef : array of shape [n_features] or [n_features, n_targets], optional\n        The coefficient of the underlying linear model. It is returned only if\n        coef is True.\n    \"\"\""}, "005165af96df953b1aa0b1e6db62b2074c6c51ec": {"bleu": 0.7126417548112653, "jaccard": 0.453125, "edit_distance": 0.611842105263158, "comment_cnt": 1, "max_sim": 0.7126417548112653, "header": "\"\"\"Metrics to assess performance on regression task\n\nFunctions named as ``*_score`` return a scalar value to maximize: the higher\nthe better\n\nFunction named as ``*_error`` or ``*_loss`` return a scalar value to minimize:\nthe lower the better\n\"\"\"\n\n\n\nimport numpy as np\nimport warnings\n\nfrom ..utils.validation import (check_array, check_consistent_length,\n                                _num_samples)\nfrom ..utils.validation import column_or_1d\nfrom ..exceptions import UndefinedMetricWarning\n\n\n__ALL__ = [\n    \"max_error\",\n    \"mean_absolute_error\",\n    \"mean_squared_error\",\n    \"mean_squared_log_error\",\n    \"median_absolute_error\",\n    \"r2_score\",\n    \"explained_variance_score\"\n]\n\n\ndef mean_absolute_error(y_true, y_pred,\n                        sample_weight=None,\n                        multioutput='uniform_average'):\n    \"\"\"Mean absolute error regression loss\n\n    Read more in the :ref:`User Guide <mean_absolute_error>`.\n\n    Parameters\n    ----------\n    y_true : array-like of shape = (n_samples) or (n_samples, n_outputs)\n        Ground truth (correct) target values.\n\n    y_pred : array-like of shape = (n_samples) or (n_samples, n_outputs)\n        Estimated target values.\n\n    sample_weight : array-like of shape = (n_samples), optional\n        Sample weights.\n\n    multioutput : string in ['raw_values', 'uniform_average']\n        or array-like of shape (n_outputs)\n        Defines aggregating of multiple output values.\n        Array-like value defines weights used to average errors.\n\n        'raw_values' :\n            Returns a full set of errors in case of multioutput input.\n\n        'uniform_average' :\n            Errors of all outputs are averaged with uniform weight.\n\n\n    Returns\n    -------\n    loss : float or ndarray of floats\n        If multioutput is 'raw_values', then mean absolute error is returned\n        for each output separately.\n        If multioutput is 'uniform_average' or an ndarray of weights, then the\n        weighted average of all output errors is returned.\n\n        MAE output is non-negative floating point. The best value is 0.0.\n\n    Examples\n    --------\n    >>> from sklearn.metrics import mean_absolute_error\n    >>> y_true = [3, -0.5, 2, 7]\n    >>> y_pred = [2.5, 0.0, 2, 8]\n    >>> mean_absolute_error(y_true, y_pred)\n    0.5\n    >>> y_true = [[0.5, 1], [-1, 1], [7, -6]]\n    >>> y_pred = [[0, 2], [-1, 2], [8, -5]]\n    >>> mean_absolute_error(y_true, y_pred)\n    0.75\n    >>> mean_absolute_error(y_true, y_pred, multioutput='raw_values')\n    array([0.5, 1. ])\n    >>> mean_absolute_error(y_true, y_pred, multioutput=[0.3, 0.7])\n    ... # doctest: +ELLIPSIS\n    0.85...\n    \"\"\"", "body": "    y_type, y_true, y_pred, multioutput = _check_reg_targets(\n        y_true, y_pred, multioutput)\n    check_consistent_length(y_true, y_pred, sample_weight)\n    output_errors = np.average(np.abs(y_pred - y_true),\n                               weights=sample_weight, axis=0)\n    if isinstance(multioutput, str):\n        if multioutput == 'raw_values':\n            return output_errors\n        elif multioutput == 'uniform_average':\n            # pass None as weights to np.average: uniform mean\n            multioutput = None\n\n    return np.average(output_errors, weights=multioutput)", "answer": "y_true = np.asarray(y_true)\n    y_pred = np.asarray(y_pred)\n\n    check_consistent_length(y_true, y_pred, sample_weight)\n    output_errors = np.average(np.abs(y_pred - y_true), weights=sample_weight, axis=0)\n\n    if multioutput == 'raw_values':\n        return output_errors\n    elif multioutput == 'uniform_average':\n        # pass None as weights to np.average: uniform mean\n        return np.average(output_errors, weights=None)\n    else:\n        return np.average(output_errors, weights=multioutput)", "license": "bsd-3-clause", "author": "Hanmin Qin <qinhanmin2005@sina.com", "project": "scikit-learn_scikit-learn", "signature": "def mean_absolute_error(y_true, y_pred,", "license_type": "Permissive", "init_header": "\"\"\"Metrics to assess performance on regression task\n\nFunctions named as ``*_score`` return a scalar value to maximize: the higher\nthe better\n\nFunction named as ``*_error`` or ``*_loss`` return a scalar value to minimize:\nthe lower the better\n\"\"\"\n\n# Authors: Alexandre Gramfort <alexandre.gramfort@inria.fr>\n#          Mathieu Blondel <mathieu@mblondel.org>\n#          Olivier Grisel <olivier.grisel@ensta.org>\n#          Arnaud Joly <a.joly@ulg.ac.be>\n#          Jochen Wersdorfer <jochen@wersdoerfer.de>\n#          Lars Buitinck\n#          Joel Nothman <joel.nothman@gmail.com>\n#          Karan Desai <karandesai281196@gmail.com>\n#          Noel Dawe <noel@dawe.me>\n#          Manoj Kumar <manojkumarsivaraj334@gmail.com>\n#          Michael Eickenberg <michael.eickenberg@gmail.com>\n#          Konstantin Shmelkov <konstantin.shmelkov@polytechnique.edu>\n# License: BSD 3 clause\n\n\nimport numpy as np\nimport warnings\n\nfrom ..utils.validation import (check_array, check_consistent_length,\n                                _num_samples)\nfrom ..utils.validation import column_or_1d\nfrom ..exceptions import UndefinedMetricWarning\n\n\n__ALL__ = [\n    \"max_error\",\n    \"mean_absolute_error\",\n    \"mean_squared_error\",\n    \"mean_squared_log_error\",\n    \"median_absolute_error\",\n    \"r2_score\",\n    \"explained_variance_score\"\n]\n\n\ndef mean_absolute_error(y_true, y_pred,\n                        sample_weight=None,\n                        multioutput='uniform_average'):\n    \"\"\"Mean absolute error regression loss\n\n    Read more in the :ref:`User Guide <mean_absolute_error>`.\n\n    Parameters\n    ----------\n    y_true : array-like of shape = (n_samples) or (n_samples, n_outputs)\n        Ground truth (correct) target values.\n\n    y_pred : array-like of shape = (n_samples) or (n_samples, n_outputs)\n        Estimated target values.\n\n    sample_weight : array-like of shape = (n_samples), optional\n        Sample weights.\n\n    multioutput : string in ['raw_values', 'uniform_average']\n        or array-like of shape (n_outputs)\n        Defines aggregating of multiple output values.\n        Array-like value defines weights used to average errors.\n\n        'raw_values' :\n            Returns a full set of errors in case of multioutput input.\n\n        'uniform_average' :\n            Errors of all outputs are averaged with uniform weight.\n\n\n    Returns\n    -------\n    loss : float or ndarray of floats\n        If multioutput is 'raw_values', then mean absolute error is returned\n        for each output separately.\n        If multioutput is 'uniform_average' or an ndarray of weights, then the\n        weighted average of all output errors is returned.\n\n        MAE output is non-negative floating point. The best value is 0.0.\n\n    Examples\n    --------\n    >>> from sklearn.metrics import mean_absolute_error\n    >>> y_true = [3, -0.5, 2, 7]\n    >>> y_pred = [2.5, 0.0, 2, 8]\n    >>> mean_absolute_error(y_true, y_pred)\n    0.5\n    >>> y_true = [[0.5, 1], [-1, 1], [7, -6]]\n    >>> y_pred = [[0, 2], [-1, 2], [8, -5]]\n    >>> mean_absolute_error(y_true, y_pred)\n    0.75\n    >>> mean_absolute_error(y_true, y_pred, multioutput='raw_values')\n    array([0.5, 1. ])\n    >>> mean_absolute_error(y_true, y_pred, multioutput=[0.3, 0.7])\n    ... # doctest: +ELLIPSIS\n    0.85...\n    \"\"\"", "docstring": "                        sample_weight=None,\n                        multioutput='uniform_average'):\n    \"\"\"Mean absolute error regression loss\n\n    Read more in the :ref:`User Guide <mean_absolute_error>`.\n\n    Parameters\n    ----------\n    y_true : array-like of shape = (n_samples) or (n_samples, n_outputs)\n        Ground truth (correct) target values.\n\n    y_pred : array-like of shape = (n_samples) or (n_samples, n_outputs)\n        Estimated target values.\n\n    sample_weight : array-like of shape = (n_samples), optional\n        Sample weights.\n\n    multioutput : string in ['raw_values', 'uniform_average']\n        or array-like of shape (n_outputs)\n        Defines aggregating of multiple output values.\n        Array-like value defines weights used to average errors.\n\n        'raw_values' :\n            Returns a full set of errors in case of multioutput input.\n\n        'uniform_average' :\n            Errors of all outputs are averaged with uniform weight.\n\n\n    Returns\n    -------\n    loss : float or ndarray of floats\n        If multioutput is 'raw_values', then mean absolute error is returned\n        for each output separately.\n        If multioutput is 'uniform_average' or an ndarray of weights, then the\n        weighted average of all output errors is returned.\n\n        MAE output is non-negative floating point. The best value is 0.0.\n\n    Examples\n    --------\n    >>> from sklearn.metrics import mean_absolute_error\n    >>> y_true = [3, -0.5, 2, 7]\n    >>> y_pred = [2.5, 0.0, 2, 8]\n    >>> mean_absolute_error(y_true, y_pred)\n    0.5\n    >>> y_true = [[0.5, 1], [-1, 1], [7, -6]]\n    >>> y_pred = [[0, 2], [-1, 2], [8, -5]]\n    >>> mean_absolute_error(y_true, y_pred)\n    0.75\n    >>> mean_absolute_error(y_true, y_pred, multioutput='raw_values')\n    array([0.5, 1. ])\n    >>> mean_absolute_error(y_true, y_pred, multioutput=[0.3, 0.7])\n    ... # doctest: +ELLIPSIS\n    0.85...\n    \"\"\""}, "a2e4b2e0fa3961434a47734bac07e78976cac4fd": {"bleu": 0.8661233820733749, "jaccard": 0.79296875, "edit_distance": 0.8491048593350383, "comment_cnt": 1, "max_sim": 0.8661233820733749, "header": "\"\"\"Provides data for the flowers dataset.\n\nThe dataset scripts used to create the dataset can be found at:\ntensorflow/models/research/slim/datasets/download_and_convert_flowers.py\n\"\"\"\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport os\nimport tensorflow as tf\n\nfrom datasets import dataset_utils\n\nslim = tf.contrib.slim\n\n_FILE_PATTERN = 'flowers_%s_*.tfrecord'\n\nSPLITS_TO_SIZES = {'train': 3320, 'validation': 350}\n\n_NUM_CLASSES = 5\n\n_ITEMS_TO_DESCRIPTIONS = {\n    'image': 'A color image of varying size.',\n    'label': 'A single integer between 0 and 4',\n}\n\n\ndef get_split(split_name, dataset_dir, file_pattern=None, reader=None):\n  \"\"\"Gets a dataset tuple with instructions for reading flowers.\n\n  Args:\n    split_name: A train/validation split name.\n    dataset_dir: The base directory of the dataset sources.\n    file_pattern: The file pattern to use when matching the dataset sources.\n      It is assumed that the pattern contains a '%s' string so that the split\n      name can be inserted.\n    reader: The TensorFlow reader type.\n\n  Returns:\n    A `Dataset` namedtuple.\n\n  Raises:\n    ValueError: if `split_name` is not a valid train/validation split.\n  \"\"\"", "body": "  if split_name not in SPLITS_TO_SIZES:\n    raise ValueError('split name %s was not recognized.' % split_name)\n\n  if not file_pattern:\n    file_pattern = _FILE_PATTERN\n  file_pattern = os.path.join(dataset_dir, file_pattern % split_name)\n\n  # Allowing None in the signature so that dataset_factory can use the default.\n  if reader is None:\n    reader = tf.TFRecordReader\n\n  keys_to_features = {\n      'image/encoded': tf.FixedLenFeature((), tf.string, default_value=''),\n      'image/format': tf.FixedLenFeature((), tf.string, default_value='png'),\n      'image/class/label': tf.FixedLenFeature(\n          [], tf.int64, default_value=tf.zeros([], dtype=tf.int64)),\n  }\n\n  items_to_handlers = {\n      'image': slim.tfexample_decoder.Image(),\n      'label': slim.tfexample_decoder.Tensor('image/class/label'),\n  }\n\n  decoder = slim.tfexample_decoder.TFExampleDecoder(\n      keys_to_features, items_to_handlers)\n\n  labels_to_names = None\n  if dataset_utils.has_labels(dataset_dir):\n    labels_to_names = dataset_utils.read_label_file(dataset_dir)\n\n  return slim.dataset.Dataset(\n      data_sources=file_pattern,\n      reader=reader,\n      decoder=decoder,\n      num_samples=SPLITS_TO_SIZES[split_name],\n      items_to_descriptions=_ITEMS_TO_DESCRIPTIONS,\n      num_classes=_NUM_CLASSES,\n      labels_to_names=labels_to_names)", "answer": "if split_name not in SPLITS_TO_SIZES:\n    raise ValueError('split_name %s was not recognized.' % split_name)\n\n  if not file_pattern:\n    file_pattern = _FILE_PATTERN\n  file_pattern = os.path.join(dataset_dir, file_pattern % split_name)\n\n  # Allowing None in the signature so that dataset_factory can use the default.\n  if reader is None:\n    reader = tf.TFRecordReader\n\n  keys_to_features = {\n      'image/encoded': tf.FixedLenFeature((), tf.string, default_value=''),\n      'image/format': tf.FixedLenFeature((), tf.string, default_value='jpeg'),\n      'image/class/label': tf.FixedLenFeature(\n          [], tf.int64, default_value=tf.zeros([], dtype=tf.int64)),\n  }\n\n  items_to_handlers = {\n      'image': slim.tfexample_decoder.Image(),\n      'label': slim.tfexample_decoder.Tensor('image/class/label'),\n  }\n\n  decoder = slim.tfexample_decoder.TFExampleDecoder(\n      keys_to_features, items_to_handlers)\n\n  dataset = slim.dataset.Dataset(\n      data_sources=file_pattern,\n      reader=reader,\n      decoder=decoder,\n      num_samples=SPLITS_TO_SIZES[split_name],\n      items_to_descriptions=_ITEMS_TO_DESCRIPTIONS,\n      num_classes=_NUM_CLASSES)\n\n  return dataset", "license": "apache-2.0", "author": "Ao-Lee <ao-li-2008@hotmail.com", "project": "Ao-Lee_FacialVarification", "signature": "def get_split(split_name, dataset_dir, file_pattern=None, reader=None):", "license_type": "Permissive", "init_header": "# Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\"\"\"Provides data for the flowers dataset.\n\nThe dataset scripts used to create the dataset can be found at:\ntensorflow/models/research/slim/datasets/download_and_convert_flowers.py\n\"\"\"\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport os\nimport tensorflow as tf\n\nfrom datasets import dataset_utils\n\nslim = tf.contrib.slim\n\n_FILE_PATTERN = 'flowers_%s_*.tfrecord'\n\nSPLITS_TO_SIZES = {'train': 3320, 'validation': 350}\n\n_NUM_CLASSES = 5\n\n_ITEMS_TO_DESCRIPTIONS = {\n    'image': 'A color image of varying size.',\n    'label': 'A single integer between 0 and 4',\n}\n\n\ndef get_split(split_name, dataset_dir, file_pattern=None, reader=None):\n  \"\"\"Gets a dataset tuple with instructions for reading flowers.\n\n  Args:\n    split_name: A train/validation split name.\n    dataset_dir: The base directory of the dataset sources.\n    file_pattern: The file pattern to use when matching the dataset sources.\n      It is assumed that the pattern contains a '%s' string so that the split\n      name can be inserted.\n    reader: The TensorFlow reader type.\n\n  Returns:\n    A `Dataset` namedtuple.\n\n  Raises:\n    ValueError: if `split_name` is not a valid train/validation split.\n  \"\"\"", "docstring": "  \"\"\"Gets a dataset tuple with instructions for reading flowers.\n\n  Args:\n    split_name: A train/validation split name.\n    dataset_dir: The base directory of the dataset sources.\n    file_pattern: The file pattern to use when matching the dataset sources.\n      It is assumed that the pattern contains a '%s' string so that the split\n      name can be inserted.\n    reader: The TensorFlow reader type.\n\n  Returns:\n    A `Dataset` namedtuple.\n\n  Raises:\n    ValueError: if `split_name` is not a valid train/validation split.\n  \"\"\""}, "e0d296846255ddca52141cc1c8c843f063e8fa75": {"bleu": 0.7008485330667422, "jaccard": 0.5703125, "edit_distance": 0.7412935323383085, "comment_cnt": 1, "max_sim": 0.7412935323383085, "header": "\"\"\"Provides data for the Cifar10 dataset.\n\nThe dataset scripts used to create the dataset can be found at:\ntensorflow/models/research/slim/datasets/download_and_convert_cifar10.py\n\"\"\"\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport os\nimport tensorflow as tf\n\nfrom datasets import dataset_utils\n\nslim = tf.contrib.slim\n\n_FILE_PATTERN = 'cifar10_%s.tfrecord'\n\nSPLITS_TO_SIZES = {'train': 50000, 'test': 10000}\n\n_NUM_CLASSES = 10\n\n_ITEMS_TO_DESCRIPTIONS = {\n    'image': 'A [32 x 32 x 3] color image.',\n    'label': 'A single integer between 0 and 9',\n}\n\n\ndef get_split(split_name, dataset_dir, file_pattern=None, reader=None):\n  \"\"\"Gets a dataset tuple with instructions for reading cifar10.\n\n  Args:\n    split_name: A train/test split name.\n    dataset_dir: The base directory of the dataset sources.\n    file_pattern: The file pattern to use when matching the dataset sources.\n      It is assumed that the pattern contains a '%s' string so that the split\n      name can be inserted.\n    reader: The TensorFlow reader type.\n\n  Returns:\n    A `Dataset` namedtuple.\n\n  Raises:\n    ValueError: if `split_name` is not a valid train/test split.\n  \"\"\"", "body": "  if split_name not in SPLITS_TO_SIZES:\n    raise ValueError('split name %s was not recognized.' % split_name)\n\n  if not file_pattern:\n    file_pattern = _FILE_PATTERN\n  file_pattern = os.path.join(dataset_dir, file_pattern % split_name)\n\n  # Allowing None in the signature so that dataset_factory can use the default.\n  if not reader:\n    reader = tf.TFRecordReader\n\n  keys_to_features = {\n      'image/encoded': tf.FixedLenFeature((), tf.string, default_value=''),\n      'image/format': tf.FixedLenFeature((), tf.string, default_value='png'),\n      'image/class/label': tf.FixedLenFeature(\n          [], tf.int64, default_value=tf.zeros([], dtype=tf.int64)),\n  }\n\n  items_to_handlers = {\n      'image': slim.tfexample_decoder.Image(shape=[32, 32, 3]),\n      'label': slim.tfexample_decoder.Tensor('image/class/label'),\n  }\n\n  decoder = slim.tfexample_decoder.TFExampleDecoder(\n      keys_to_features, items_to_handlers)\n\n  labels_to_names = None\n  if dataset_utils.has_labels(dataset_dir):\n    labels_to_names = dataset_utils.read_label_file(dataset_dir)\n\n  return slim.dataset.Dataset(\n      data_sources=file_pattern,\n      reader=reader,\n      decoder=decoder,\n      num_samples=SPLITS_TO_SIZES[split_name],\n      items_to_descriptions=_ITEMS_TO_DESCRIPTIONS,\n      num_classes=_NUM_CLASSES,\n      labels_to_names=labels_to_names)", "answer": "if split_name not in SPLITS_TO_SIZES:\n    raise ValueError('split_name %s was not recognized.' % split_name)\n\n  if not file_pattern:\n    file_pattern = _FILE_PATTERN\n  file_pattern = os.path.join(dataset_dir, file_pattern % split_name)\n\n  # Allowing None in the signature so that dataset_factory can use the default.\n  if reader is None:\n    reader = tf.TFRecordReader\n\n  keys_to_features = {\n      'image/encoded': tf.FixedLenFeature([], tf.string),\n      'image/format': tf.FixedLenFeature([], tf.string, default_value='png'),\n      'image/class/label': tf.FixedLenFeature([], tf.int64),\n  }\n\n  items_to_handlers = {\n      'image': slim.tfexample_decoder.Image(),\n      'label': slim.tfexample_decoder.Tensor('image/class/label'),\n  }\n\n  decoder = slim.tfexample_decoder.TFExampleDecoder(\n      keys_to_features, items_to_handlers)\n\n  dataset = slim.dataset.Dataset(\n      data_sources=file_pattern,\n      reader=reader,\n      decoder=decoder,\n      num_samples=SPLITS_TO_SIZES[split_name],\n      items_to_descriptions=_ITEMS_TO_DESCRIPTIONS,\n      num_classes=_NUM_CLASSES)\n\n  return dataset", "license": "apache-2.0", "author": "Ao-Lee <ao-li-2008@hotmail.com", "project": "Ao-Lee_FacialVarification", "signature": "def get_split(split_name, dataset_dir, file_pattern=None, reader=None):", "license_type": "Permissive", "init_header": "# Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\"\"\"Provides data for the Cifar10 dataset.\n\nThe dataset scripts used to create the dataset can be found at:\ntensorflow/models/research/slim/datasets/download_and_convert_cifar10.py\n\"\"\"\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport os\nimport tensorflow as tf\n\nfrom datasets import dataset_utils\n\nslim = tf.contrib.slim\n\n_FILE_PATTERN = 'cifar10_%s.tfrecord'\n\nSPLITS_TO_SIZES = {'train': 50000, 'test': 10000}\n\n_NUM_CLASSES = 10\n\n_ITEMS_TO_DESCRIPTIONS = {\n    'image': 'A [32 x 32 x 3] color image.',\n    'label': 'A single integer between 0 and 9',\n}\n\n\ndef get_split(split_name, dataset_dir, file_pattern=None, reader=None):\n  \"\"\"Gets a dataset tuple with instructions for reading cifar10.\n\n  Args:\n    split_name: A train/test split name.\n    dataset_dir: The base directory of the dataset sources.\n    file_pattern: The file pattern to use when matching the dataset sources.\n      It is assumed that the pattern contains a '%s' string so that the split\n      name can be inserted.\n    reader: The TensorFlow reader type.\n\n  Returns:\n    A `Dataset` namedtuple.\n\n  Raises:\n    ValueError: if `split_name` is not a valid train/test split.\n  \"\"\"", "docstring": "  \"\"\"Gets a dataset tuple with instructions for reading cifar10.\n\n  Args:\n    split_name: A train/test split name.\n    dataset_dir: The base directory of the dataset sources.\n    file_pattern: The file pattern to use when matching the dataset sources.\n      It is assumed that the pattern contains a '%s' string so that the split\n      name can be inserted.\n    reader: The TensorFlow reader type.\n\n  Returns:\n    A `Dataset` namedtuple.\n\n  Raises:\n    ValueError: if `split_name` is not a valid train/test split.\n  \"\"\""}}